@article{mlir,
  title   = {{MLIR}: A Compiler Infrastructure for the End of Moore's Law},
  author  = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  journal = {arXiv preprint arXiv:2002.11054},
  year    = {2020},
  url     = {https://arxiv.org/abs/2002.11054}
}

@inproceedings{tvm2018,
  title     = {{TVM}: An Automated {End-to-End} Optimizing Compiler for Deep Learning},
  author    = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI~'18)},
  year      = {2018},
  pages     = {578--594},
  address   = {Carlsbad, CA, USA},
  publisher = {USENIX Association},
  url       = {https://www.usenix.org/conference/osdi18/presentation/chen}
}

@article{iree22tiny,
  title   = {TinyIREE: An {ML} Execution Environment for Embedded Systems from Compilation to Deployment},
  author  = {Liu, Hsin-I Cindy and Brehler, Marius and Ravishankar, Mahesh and Vasilache, Nicolas and Vanik, Ben and Laurenzo, Stella},
  journal = {arXiv preprint arXiv:2205.14479},
  year    = {2022},
  url     = {https://arxiv.org/abs/2205.14479}
}

@techreport{rvv,
  title       = {The {RISC-V} Vector Extension Version~1.0 (V)},
  author      = {{RISC-V International}},
  institution = {RISC-V International},
  year        = {2024},
  note        = {Vector ISA specification v1.0},
  url         = {https://github.com/riscvarchive/riscv-v-spec/releases}
}

@misc{stablehlo,
  title        = {StableHLO Specification},
  author       = {{OpenXLA Project}},
  year         = {2025},
  howpublished = {\url{https://openxla.org/stablehlo/spec}},
  note         = {Operation set and MLIR-based implementation}
}

@misc{xla,
  title        = {{XLA}: Optimizing Compiler for Machine Learning},
  author       = {{OpenXLA Project}},
  year         = {2025},
  howpublished = {\url{https://openxla.org/xla}},
  note         = {Project documentation}
}

@article{glow2018,
  title   = {Glow: Graph Lowering Compiler Techniques for Neural Networks},
  author  = {Rotem, Nadav and Fix, Jordan and Abdulrasool, Saleem and Catron, Garret and Deng, Summer and Dzhabarov, Roman and Gibson, Nick and Hegeman, James and Lele, Meghan and Levenstein, Roman and Montgomery, Jack and Maher, Bert and Nadathur, Satish and Olesen, Jakob and Park, Jongsoo and Rakhov, Artem and Smelyanskiy, Misha and Wang, Man},
  journal = {arXiv preprint arXiv:1805.00907},
  year    = {2018},
  url     = {https://arxiv.org/abs/1805.00907}
}

@misc{tosa,
  title        = {Tensor Operator Set Architecture (TOSA) Specification v1.0.1},
  author       = {{Arm Ltd.} and {Linaro MLPlatform}},
  year         = {2025},
  howpublished = {\url{https://www.mlplatform.org/tosa/tosa_spec.html}},
  note         = {HTML/PDF specification}
}

@misc{onnx,
  title        = {Open Neural Network Exchange (ONNX)},
  author       = {{LF AI \& Data Foundation}},
  year         = {2025},
  howpublished = {\url{https://onnx.ai/}},
  note         = {Project homepage}
}

@article{onnxmlir2020,
  title   = {Compiling {ONNX} Neural Network Models Using {MLIR}},
  author  = {Jin, Tian and Bercea, Gheorghe-Teodor and Le, Tung D. and Chen, Tong and Su, Gong and Imai, Haruki and Negishi, Yasushi and Leu, Anh and O'Brien, Kevin and Kawachiya, Kiyokuni and Eichenberger, Alexandre E.},
  journal = {arXiv preprint arXiv:2008.08272},
  year    = {2020},
  url     = {https://arxiv.org/abs/2008.08272}
}

@misc{mlir-linalg,
  title        = {{MLIR} 'linalg' Dialect},
  author       = {{LLVM Project}},
  year         = {2025},
  howpublished = {\url{https://mlir.llvm.org/docs/Dialects/Linalg/}},
  note         = {Dialect documentation}
}

@misc{mlir-memref,
  title        = {{MLIR} 'memref' Dialect},
  author       = {{LLVM Project}},
  year         = {2025},
  howpublished = {\url{https://mlir.llvm.org/docs/Dialects/MemRef/}},
  note         = {Dialect documentation}
}


@inproceedings{jouppi2017tpu,
  title     = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  author    = {Jouppi, Norman P. and Young, Cliff and Patterson, David and others},
  booktitle = {ISCA},
  year      = {2017},
  url       = {https://arxiv.org/abs/1704.04760}
}

@inproceedings{eyeriss2016,
  title     = {Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks},
  author    = {Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
  booktitle = {ISCA},
  year      = {2016},
  url       = {https://eems.mit.edu/wp-content/uploads/2016/04/eyeriss_isca_2016.pdf}
}

@article{nvdla2019,
  title   = {Integrating NVIDIA Deep Learning Accelerator (NVDLA) with RISC-V SoC on FireSim},
  author  = {Farshchi, Farzad and Huang, Qijing and Yun, Heechul},
  journal = {arXiv:1903.06495},
  year    = {2019},
  url     = {https://arxiv.org/abs/1903.06495}
}

@article{rvx-etrij,
  title   = {SNN eXpress: Streamlining Low-Power AI-SoC Development with RISC-V},
  author  = {Jang, Hyungjin and others},
  journal = {ETRI Journal},
  year    = {2024},
  note    = {Describes RVX as an EDA tool for quickly composing RISC-V SoC platforms},
  url     = {https://onlinelibrary.wiley.com/doi/10.4218/etrij.2024-0114}
}

@techreport{rocket,
    author= {Asanović, Krste and Avizienis, Rimas and Bachrach, Jonathan and Beamer, Scott and Biancolin, David and Celio, Christopher and Cook, Henry and Dabbelt, Daniel and Hauser, John and Izraelevitz, Adam and Karandikar, Sagar and Keller, Ben and Kim, Donggyu and Koenig, John and Lee, Yunsup and Love, Eric and Maas, Martin and Magyar, Albert and Mao, Howard and Moreto, Miquel and Ou, Albert and Patterson, David A. and Richards, Brian and Schmidt, Colin and Twigg, Stephen and Vo, Huy and Waterman, Andrew},
    title= {The Rocket Chip Generator},
    year= {2016},
    month= {Apr},
    institution = {EECS Department, University of California, Berkeley},
    url= {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html},
    number= {UCB/EECS-2016-17},
    abstract= {Rocket Chip is an open-source Sysem-on-Chip design generator that emits synthesizable RTL. It leverages the Chisel hardware construction language to compose a library of sophisticated generators for cores, caches, and interconnects into an integrated SoC. Rocket Chip generates general-purpose processor cores that use the open RISC-V ISA, and provides both an in-order core generator (Rocket) and an out-of-order core generator (BOOM). For SoC designers interested in utilizing heterogeneous specialization for added efficiency gains, Rocket Chip supports the integration of custom accelerators in the form of instruction set extensions, coprocessors, or fully independent novel cores. Rocket Chip has been taped out (manufactured) eleven times, and yielded functional silicon prototypes capable of booting Linux.},
}

@inproceedings{boom,
  title={Sonicboom: The 3rd generation berkeley out-of-order machine},
  author={Zhao, Jerry and Korpan, Ben and Gonzalez, Abraham and Asanovic, Krste},
  booktitle={Fourth Workshop on Computer Architecture Research with RISC-V},
  volume={5},
  pages={1--7},
  year={2020},
  organization={International Symposium on Computer Architecture Valencia, Spain}
}

@article{cva6,
   author={F. {Zaruba} and L. {Benini}},
   journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
   title={The Cost of Application-Class Processing: Energy and Performance Analysis of a Linux-Ready 1.7-GHz 64-Bit RISC-V Core in 22-nm FDSOI Technology},
   year={2019},
   volume={27},
   number={11},
   pages={2629-2640},
   doi={10.1109/TVLSI.2019.2926114},
   ISSN={1557-9999},
   month={Nov},
}

@article{pulp,
author = {Conti, Francesco and Rossi, Davide and Pullini, Antonio and Loi, Igor and Benini, Luca},
title = {PULP: A Ultra-Low Power Parallel Accelerator for Energy-Efficient and Flexible Embedded Vision},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {84},
number = {3},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-015-1070-9},
doi = {10.1007/s11265-015-1070-9},
abstract = {Novel pervasive devices such as smart surveillance cameras and autonomous micro-UAVs could greatly benefit from the availability of a computing device supporting embedded computer vision at a very low power budget. To this end, we propose PULP (Parallel processing Ultra-Low Power platform), an architecture built on clusters of tightly-coupled OpenRISC ISA cores, with advanced techniques for fast performance and energy scalability that exploit the capabilities of the STMicroelectronics UTBB FD-SOI 28nm technology. We show that PULP performance can be scaled over a 1x-354x range, with a peak theoretical energy efficiency of 211 GOPS/W. We present performance results for several demanding kernels from the image processing and vision domain, with post-layout power modeling: a motion detection application that can run at an efficiency up to 192 GOPS/W (90 \% of the theoretical peak); a ConvNet-based detector for smart surveillance that can be switched between 0.7 and 27fps operating modes, scaling energy consumption per frame between 1.2 and 12mJ on a 320 240 image; and FAST + Lucas-Kanade optical flow on a 128 128 image at the ultra-low energy budget of 14 μJ per frame at 60fps.},
journal = {J. Signal Process. Syst.},
month = sep,
pages = {339–354},
numpages = {16},
keywords = {Convolutional Neural Network, Embedded vision, FD-SOI, Motion estimation, Multi-core, OpenRISC, Optical flow, Ultra-Low Power}
}

@ARTICLE{chipyard,
  author={Amid, Alon and Biancolin, David and Gonzalez, Abraham and Grubb, Daniel and Karandikar, Sagar and Liew, Harrison and Magyar, Albert and Mao, Howard and Ou, Albert and Pemberton, Nathan and Rigge, Paul and Schmidt, Colin and Wright, John and Zhao, Jerry and Shao, Yakun Sophia and Asanović, Krste and Nikolić, Borivoje},
  journal={IEEE Micro}, 
  title={Chipyard: Integrated Design, Simulation, and Implementation Framework for Custom SoCs}, 
  year={2020},
  volume={40},
  number={4},
  pages={10-21},
  keywords={Generators;Open source software;Computational modeling;IP networks;Hardware;Physical design;Computer architecture},
  doi={10.1109/MM.2020.2996616}}

@INPROCEEDINGS{hwacha,
  author={Lee, Yunsup and Waterman, Andrew and Avizienis, Rimas and Cook, Henry and Sun, Chen and Stojanović, Vladimir and Asanović, Krste},
  booktitle={ESSCIRC 2014 - 40th European Solid State Circuits Conference (ESSCIRC)}, 
  title={A 45nm 1.3GHz 16.7 double-precision GFLOPS/W RISC-V processor with vector accelerators}, 
  year={2014},
  volume={},
  number={},
  pages={199-202},
  keywords={Vectors;Rockets;Random access memory;Computer architecture;Field programmable gate arrays;Pipelines;Hazards},
  doi={10.1109/ESSCIRC.2014.6942056}}

@INPROCEEDINGS{gemmini,
  author={Genc, Hasan and Kim, Seah and Amid, Alon and Haj-Ali, Ameer and Iyer, Vighnesh and Prakash, Pranav and Zhao, Jerry and Grubb, Daniel and Liew, Harrison and Mao, Howard and Ou, Albert and Schmidt, Colin and Steffl, Samuel and Wright, John and Stoica, Ion and Ragan-Kelley, Jonathan and Asanovic, Krste and Nikolic, Borivoje and Shao, Yakun Sophia},
  booktitle={Proceedings of the 58th Annual Design Automation Conference (DAC)}, 
  title={Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration}, 
  year={2021},
  volume={},
  number={},
  pages={}
}



@inproceedings{finn,
author = {Umuroglu, Yaman and Fraser, Nicholas J. and Gambardella, Giulio and Blott, Michaela and Leong, Philip and Jahre, Magnus and Vissers, Kees},
title = {FINN: A Framework for Fast, Scalable Binarized Neural Network Inference},
year = {2017},
isbn = {9781450343541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3020078.3021744},
doi = {10.1145/3020078.3021744},
abstract = {Research has shown that convolutional neural networks contain significant redundancy, and high classification accuracy can be obtained even when weights and activations are reduced from floating point to binary values. In this paper, we present FINN, a framework for building fast and flexible FPGA accelerators using a flexible heterogeneous streaming architecture. By utilizing a novel set of optimizations that enable efficient mapping of binarized neural networks to hardware, we implement fully connected, convolutional and pooling layers, with per-layer compute resources being tailored to user-provided throughput requirements. On a ZC706 embedded FPGA platform drawing less than 25 W total system power, we demonstrate up to 12.3 million image classifications per second with 0.31 μs latency on the MNIST dataset with 95.8\% accuracy, and 21906 image classifications per second with 283 μs latency on the CIFAR-10 and SVHN datasets with respectively 80.1\% and 94.9\% accuracy. To the best of our knowledge, ours are the fastest classification rates reported to date on these benchmarks.},
booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {65–74},
numpages = {10},
keywords = {reconfigurable logic, neural networks, hardware acceleration, binary neural network, binarized neural network, FPGA},
location = {Monterey, California, USA},
series = {FPGA '17}
}

@article{hls4ml,
  title={hls4ml: An open-source codesign workflow to empower scientific low-power machine learning devices},
  author={Fahim, Farah and Hawks, Benjamin and Herwig, Christian and Hirschauer, James and Jindariani, Sergo and Tran, Nhan and Carloni, Luca P and Di Guglielmo, Giuseppe and Harris, Philip and Krupa, Jeffrey and others},
  journal={arXiv preprint arXiv:2103.05579},
  year={2021}
}

@article{vta,
  title={A hardware--software blueprint for flexible deep learning specialization},
  author={Moreau, Thierry and Chen, Tianqi and Vega, Luis and Roesch, Jared and Yan, Eddie and Zheng, Lianmin and Fromm, Josh and Jiang, Ziheng and Ceze, Luis and Guestrin, Carlos and others},
  journal={IEEE Micro},
  volume={39},
  number={5},
  pages={8--16},
  year={2019},
  publisher={IEEE}
}

@inproceedings{maeri2018,
author = {Kwon, Hyoukjun and Samajdar, Ananda and Krishna, Tushar},
title = {MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3173176},
doi = {10.1145/3173162.3173176},
abstract = {Deep neural networks (DNN) have demonstrated highly promising results across computer vision and speech recognition, and are becoming foundational for ubiquitous AI. The computational complexity of these algorithms and a need for high energy-efficiency has led to a surge in research on hardware accelerators. \% for this paradigm. To reduce the latency and energy costs of accessing DRAM, most DNN accelerators are spatial in nature, with hundreds of processing elements (PE) operating in parallel and communicating with each other directly. DNNs are evolving at a rapid rate, and it is common to have convolution, recurrent, pooling, and fully-connected layers with varying input and filter sizes in the most recent topologies.They may be dense or sparse. They can also be partitioned in myriad ways (within and across layers) to exploit data reuse (weights and intermediate outputs). All of the above can lead to different dataflow patterns within the accelerator substrate. Unfortunately, most DNN accelerators support only fixed dataflow patterns internally as they perform a careful co-design of the PEs and the network-on-chip (NoC). In fact, the majority of them are only optimized for traffic within a convolutional layer. This makes it challenging to map arbitrary dataflows on the fabric efficiently, and can lead to underutilization of the available compute resources. DNN accelerators need to be programmable to enable mass deployment. For them to be programmable, they need to be configurable internally to support the various dataflow patterns that could be mapped over them. To address this need, we present MAERI, which is a DNN accelerator built with a set of modular and configurable building blocks that can easily support myriad DNN partitions and mappings by appropriately configuring tiny switches. MAERI provides 8-459\% better utilization across multiple dataflow mappings over baselines with rigid NoC fabrics.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {461–475},
numpages = {15},
keywords = {convolutional neural network, deep learning accelerator, machine learning, network-on-chip, recurrent neural network, spatial architecture},
location = {Williamsburg, VA, USA},
series = {ASPLOS '18}
}

@article{eyerissv2,
  title={Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices},
  author={Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel and Sze, Vivienne},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  volume={9},
  number={2},
  pages={292--308},
  year={2019},
  publisher={IEEE}
}

@INPROCEEDINGS{timeloop,
  author={Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A. and Mukkara, Anurag and Venkatesan, Rangharajan and Khailany, Brucek and Keckler, Stephen W. and Emer, Joel},
  booktitle={2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={Timeloop: A Systematic Approach to DNN Accelerator Evaluation}, 
  year={2019},
  volume={},
  number={},
  pages={304-315},
  keywords={Hardware;Neural networks;Space exploration;Systematics;Accelerator architectures;Computational modeling;modeling;accelerator architecture;deep neural networks;neural network dataflows},
  doi={10.1109/ISPASS.2019.00042}}

@INPROCEEDINGS{accelergy,
  author={Wu, Yannan Nellie and Emer, Joel S. and Sze, Vivienne},
  booktitle={2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)}, 
  title={Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  keywords={Program processors;Electric breakdown;Neural networks;Estimation;Hardware;Energy efficiency;Compounds},
  doi={10.1109/ICCAD45719.2019.8942149}}

@ARTICLE{zigzag,
  author={Mei, Linyan and Houshmand, Pouya and Jain, Vikram and Giraldo, Sebastian and Verhelst, Marian},
  journal={IEEE Transactions on Computers}, 
  title={ZigZag: Enlarging Joint Architecture-Mapping Design Space Exploration for DNN Accelerators}, 
  year={2021},
  volume={70},
  number={8},
  pages={1160-1174},
  keywords={Hardware;Computer architecture;Analytical models;Search engines;Neural networks;Space exploration;Search problems;DNN;accelerator;design space exploration;memory hierarchy;mapping;scheduling;dataflow;analytical model},
  doi={10.1109/TC.2021.3059962}}

@INPROCEEDINGS{shidiannao2014,
  author={Du, Zidong and Fasthuber, Robert and Chen, Tianshi and Ienne, Paolo and Li, Ling and Luo, Tao and Feng, Xiaobing and Chen, Yunji and Temam, Olivier},
  booktitle={2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)}, 
  title={ShiDianNao: Shifting vision processing closer to the sensor}, 
  year={2015},
  volume={},
  number={},
  pages={92-104},
  keywords={Kernel;Neural networks;Sensors;Energy efficiency;Smart phones;Filtering;Neurons},
  doi={10.1145/2749469.2750389}}

@misc{circt,
  title        = {CIRCT: Circuit IR Compilers and Tools},
  author       = {{LLVM Project}},
  year         = {2025},
  howpublished = {\url{https://circt.llvm.org/}},
  note         = {MLIR-based hardware dialects and RTL export}
}

@misc{handshake,
  title        = {CIRCT Handshake Dialect},
  author       = {{LLVM Project}},
  year         = {2025},
  howpublished = {\url{https://circt.llvm.org/docs/Handshake/}},
  note         = {Dataflow-oriented MLIR dialect for hardware}
}

@misc{staticlogic,
  title        = {CIRCT StaticLogic Dialect},
  author       = {{LLVM Project}},
  year         = {2025},
  howpublished = {\url{https://circt.llvm.org/docs/StaticLogic/}},
  note         = {Static scheduling and control for hardware pipelines}
}

@misc{emitc,
  title        = {MLIR EmitC: Generating C/C++ from MLIR},
  author       = {{LLVM Project}},
  year         = {2025},
  howpublished = {\url{https://github.com/llvm/mlir-emitc}},
  note         = {Pathway from MLIR to portable C/C++ (incl. HLS)}
}

@inproceedings{scalehls,
  title={Scalehls: A new scalable high-level synthesis framework on multi-level intermediate representation},
  author={Ye, Hanchen and Hao, Cong and Cheng, Jianyi and Jeong, Hyunmin and Huang, Jack and Neuendorffer, Stephen and Chen, Deming},
  booktitle={2022 IEEE international symposium on high-performance computer architecture (HPCA)},
  pages={741--755},
  year={2022},
  organization={IEEE}
}

@inproceedings{dynamatic,
author = {Josipovi\'{c}, Lana and Ghosal, Radhika and Ienne, Paolo},
title = {Dynamically Scheduled High-level Synthesis},
year = {2018},
isbn = {9781450356145},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3174243.3174264},
doi = {10.1145/3174243.3174264},
abstract = {High-level synthesis (HLS) tools almost universally generate statically scheduled datapaths. Static scheduling implies that circuits out of HLS tools have a hard time exploiting parallelism in code with potential memory dependencies, with control-dependent dependencies in inner loops, or where performance is limited by long latency control decisions. The situation is essentially the same as in computer architecture between Very-Long Instruction Word (VLIW) processors and dynamically scheduled superscalar processors; the former display the best performance per cost in highly regular embedded applications, but general purpose, irregular, and control-dominated computing tasks require the runtime flexibility of dynamic scheduling. In this work, we show that high-level synthesis of dynamically scheduled circuits is perfectly feasible by describing the implementation of a prototype synthesizer which generates a particular form of latency-insensitive synchronous circuits. Compared to a commercial HLS tool, the result is a different trade-off between performance and circuit complexity, much as superscalar processors represent a different trade-off compared to VLIW processors: in demanding applications, the performance is very significantly improved at an affordable cost. We here demonstrate only the first steps towards more performant high-level synthesis tools adapted to emerging FPGA applications and the demands of computing in broader application domains.},
booktitle = {Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {127–136},
numpages = {10},
keywords = {compiler, dynamically scheduled circuits, high-level synthesis, pipelining},
location = {Monterey, CALIFORNIA, USA},
series = {FPGA '18}
}


@misc{mliraie,
  title        = {mlir-aie: MLIR-based Compiler for Xilinx/AMD AI Engine},
  author       = {{AMD/Xilinx}},
  year         = {2024},
  howpublished = {\url{https://github.com/Xilinx/mlir-aie}},
  note         = {MLIR dialects and tools for AI Engine/CGRA targets}
}

@inproceedings{calyx,
author = {Nigam, Rachit and Thomas, Samuel and Li, Zhijing and Sampson, Adrian},
title = {A compiler infrastructure for accelerator generators},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446712},
doi = {10.1145/3445814.3446712},
abstract = {We present Calyx, a new intermediate language (IL) for compiling high-level programs into hardware designs. Calyx combines a hardware-like structural language with a software-like control flow representation with loops and conditionals. This split representation enables a new class of hardware-focused optimizations that require both structural and control flow information which are crucial for high-level programming models for hardware design. The Calyx compiler lowers control flow constructs using finite-state machines and generates synthesizable hardware descriptions. We have implemented Calyx in an optimizing compiler that translates high-level programs to hardware. We demonstrate Calyx using two DSL-to-RTL compilers, a systolic array generator and one for a recent imperative accelerator language, and compare them to equivalent designs generated using high-level synthesis (HLS). The systolic arrays are 4.6\texttimes{} faster and 1.11\texttimes{} larger on average than HLS implementations, and the HLS-like imperative language compiler is within a few factors of a highly optimized commercial HLS toolchain. We also describe three optimizations implemented in the Calyx compiler.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {804–817},
numpages = {14},
keywords = {Accelerator Design, Intermediate Language},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@inproceedings{ml2tuner25,
author = {Cha, JooHyoung and Lee, Munyoung and Kwon, Jinse and Lee, Jemin and Kwon, Yongin},
title = {Multi-level Machine Learning-Guided Autotuning for Efficient Code Generation on a Deep Learning Accelerator},
year = {2025},
isbn = {9798400719219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3735452.3735538},
doi = {10.1145/3735452.3735538},
abstract = {The growing complexity of deep learning models necessitates specialized hardware and software optimizations, particularly for deep learning accelerators.   While machine learning-based autotuning methods have emerged as a promising solution to reduce manual effort, both template-based and template-free approaches suffer from prolonged tuning times due to the profiling of invalid configurations, which may result in runtime errors.   To address this issue, we propose ML2Tuner, a multi-level machine learning-guided autotuning technique designed to improve efficiency and robustness.   ML2Tuner introduces two key ideas: (1) a validity prediction model to filter out invalid configurations prior to profiling, and (2) an advanced performance prediction model that leverages hidden features extracted during the compilation process.   Experimental results on an extended VTA accelerator demonstrate that ML2Tuner achieves equivalent performance improvements using only 12.3\% of the samples required by a TVM-like approach and reduces invalid profiling attempts by an average of 60.8\%, highlighting its potential to enhance autotuning performance by filtering out invalid configurations.},
booktitle = {Proceedings of the 26th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
pages = {134–145},
numpages = {12},
keywords = {Auto-tuning, Deep learning accelerator, Hardware-aware optimization, Machine learning for systems, Performance prediction},
location = {Seoul, Republic of Korea},
series = {LCTES '25}
}

