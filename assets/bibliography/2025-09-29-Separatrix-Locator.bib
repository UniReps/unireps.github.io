
@article{golub_fixedpointfinder_2018,
	title = {{FixedPointFinder}: {A} {Tensorflow} toolbox for identifying and characterizing fixed points in recurrent neural networks},
	volume = {3},
	issn = {2475-9066},
	shorttitle = {{FixedPointFinder}},
	url = {https://joss.theoj.org/papers/10.21105/joss.01003},
	doi = {10.21105/joss.01003},
	abstract = {Golub et al., (2018). FixedPointFinder: A Tensorflow toolbox for identifying and characterizing fixed points in recurrent neural networks. Journal of Open Source Software, 3(31), 1003, https://doi.org/10.21105/joss.01003},
	language = {en},
	number = {31},
	urldate = {2025-10-10},
	journal = {Journal of Open Source Software},
	author = {Golub, Matthew D. and Sussillo, David},
	month = nov,
	year = {2018},
	pages = {1003},
}

@article{klus_data-driven_2020,
	title = {Data-driven approximation of the {Koopman} generator: {Model} reduction, system identification, and control},
	volume = {406},
	issn = {0167-2789},
	shorttitle = {Data-driven approximation of the {Koopman} generator},
	url = {https://www.sciencedirect.com/science/article/pii/S0167278919306086},
	doi = {10.1016/j.physd.2020.132416},
	abstract = {We derive a data-driven method for the approximation of the Koopman generator called gEDMD, which can be regarded as a straightforward extension of EDMD (extended dynamic mode decomposition). This approach is applicable to deterministic and stochastic dynamical systems. It can be used for computing eigenvalues, eigenfunctions, and modes of the generator and for system identification. In addition to learning the governing equations of deterministic systems, which then reduces to SINDy (sparse identification of nonlinear dynamics), it is possible to identify the drift and diffusion terms of stochastic differential equations from data. Moreover, we apply gEDMD to derive coarse-grained models of high-dimensional systems, and also to determine efficient model predictive control strategies. We highlight relationships with other methods and demonstrate the efficacy of the proposed methods using several guiding examples and prototypical molecular dynamics problems.},
	urldate = {2025-10-10},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Klus, Stefan and Nüske, Feliks and Peitz, Sebastian and Niemann, Jan-Hendrik and Clementi, Cecilia and Schütte, Christof},
	month = may,
	year = {2020},
	keywords = {Coarse graining, Control, Data-driven methods, Infinitesimal generator, Koopman operator, System identification},
	pages = {132416},
}

@article{vyas_computation_2020,
	title = {Computation {Through} {Neural} {Population} {Dynamics}},
	volume = {43},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-neuro-092619-094115},
	doi = {10.1146/annurev-neuro-092619-094115},
	abstract = {Significant experimental, computational, and theoretical work has identified rich structure within the coordinated activity of interconnected neural populations. An emerging challenge now is to uncover the nature of the associated computations, how they are implemented, and what role they play in driving behavior. We term this computation through neural population dynamics. If successful, this framework will reveal general motifs of neural population activity and quantitatively describe how neural population dynamics implement computations necessary for driving goal-directed behavior. Here, we start with a mathematical primer on dynamical systems theory and analytical tools necessary to apply this perspective to experimental data. Next, we highlight some recent discoveries resulting from successful application of dynamical systems. We focus on studies spanning motor control, timing, decision-making, and working memory. Finally, we briefly discuss promising recent lines of investigation and future directions for the computation through neural population dynamics framework.},
	language = {en},
	number = {1},
	urldate = {2025-09-07},
	journal = {Annual Review of Neuroscience},
	author = {Vyas, Saurabh and Golub, Matthew D. and Sussillo, David and Shenoy, Krishna V.},
	month = jul,
	year = {2020},
	pages = {249--275},
}

@article{klus_numerical_2016,
	title = {On the numerical approximation of the {Perron}-{Frobenius} and {Koopman} operator},
	volume = {3},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2158-2491},
	url = {https://www.aimsciences.org/en/article/doi/10.3934/jcd.2016003},
	doi = {10.3934/jcd.2016003},
	abstract = {Information about the behavior of dynamical systems can oftenbe obtained by analyzing the eigenvalues and corresponding eigenfunctions oflinear operators associated with a dynamical system. Examples of such operatorsare the Perron-Frobenius and the Koopman operator. In this paper, wewill review dierent methods that have been developed over the last decades tocompute nite-dimensional approximations of these innite-dimensional operators- in particular Ulam's method and Extended Dynamic Mode Decomposition(EDMD) - and highlight the similarities and dierences between theseapproaches. The results will be illustrated using simple stochastic dierentialequations and molecular dynamics examples.},
	language = {en},
	number = {1},
	urldate = {2025-08-22},
	journal = {Journal of Computational Dynamics},
	author = {Klus, Stefan and Koltai, Péter and Schütte, Christof},
	month = sep,
	year = {2016},
	pages = {51--79},
}

@article{linderman_dynamax_2025,
	title = {Dynamax: {A} {Python} package for probabilistic state space modeling with {JAX}},
	volume = {10},
	issn = {2475-9066},
	shorttitle = {Dynamax},
	url = {https://joss.theoj.org/papers/10.21105/joss.07069},
	doi = {10.21105/joss.07069},
	abstract = {Linderman et al., (2025). Dynamax: A Python package for probabilistic state space modeling with JAX. Journal of Open Source Software, 10(108), 7069, https://doi.org/10.21105/joss.07069},
	language = {en},
	number = {108},
	urldate = {2025-08-20},
	journal = {Journal of Open Source Software},
	author = {Linderman, Scott W. and Chang, Peter and Harper-Donnelly, Giles and Kara, Aleyna and Li, Xinglong and Duran-Martin, Gerardo and Murphy, Kevin},
	month = apr,
	year = {2025},
	pages = {7069},
}

@misc{javadzadeh_dynamic_2024,
	title = {Dynamic consensus-building between neocortical areas via long-range connections},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2024.11.27.625691v2},
	doi = {10.1101/2024.11.27.625691},
	abstract = {The neocortex is organized into functionally specialized areas. While the functions and underlying neural circuitry of individual neocortical areas are well studied, it is unclear how these regions operate collectively to form percepts and implement cognitive processes. In particular, it remains unknown how distributed, potentially conflicting computations can be reconciled. Here we show that the reciprocal excitatory connections between cortical areas orchestrate neural dynamics to facilitate the gradual emergence of a ‘consensus’ across areas. We investigated the joint neural dynamics of primary (V1) and higher-order lateromedial (LM) visual areas in mice, using simultaneous multi-area electrophysiological recordings along with focal optogenetic perturbations to causally manipulate neural activity. We combined mechanistic circuit modeling with state-of-the-art data-driven nonlinear system identification, to construct biologically-constrained latent circuit models of the data that we could further interrogate. This approach revealed that long-range, reciprocal excitatory connections between V1 and LM implement an approximate line attractor in their joint dynamics, which promotes activity patterns encoding the presence of the stimulus consistently across the two areas. Further theoretical analyses revealed that the emergence of line attractor dynamics is a signature of a more general principle governing multi-area network dynamics: reciprocal inter-area excitatory connections reshape the dynamical landscape of the network, specifically slowing down the decay of activity patterns that encode stimulus features congruently across areas, while accelerating the decay of inconsistent patterns. This selective dynamic amplification leads to the emergence of multi-dimensional consensus between cortical areas about various stimulus features. Our analytical framework further predicted the timescales of specific activity patterns across areas, which we directly verified in our data. Therefore, by linking the anatomical organization of inter-area connections to the features they reconcile across areas, our work introduces a general theory of multi-area computation.},
	language = {en},
	urldate = {2025-08-19},
	publisher = {bioRxiv},
	author = {Javadzadeh, Mitra and Schimel, Marine and Hofer, Sonja B. and Ahmadian, Yashar and Hennequin, Guillaume},
	month = dec,
	year = {2024},
	note = {Pages: 2024.11.27.625691
Section: New Results},
}

@article{schimel_when_2024,
	title = {When and why does motor preparation arise in recurrent neural network models of motor control?},
	volume = {12},
	url = {https://elifesciences.org/reviewed-preprints/89131},
	doi = {10.7554/eLife.89131.3},
	abstract = {During delayed ballistic reaches, motor areas consistently display movement-specific activity patterns prior to movement onset. It is unclear why these patterns arise: while they have been proposed to seed an initial neural state from which the movement unfolds, recent experiments have uncovered the presence and necessity of ongoing inputs during movement, which may lessen the need for careful initialization. Here, we modelled the motor cortex as an input-driven dynamical system, and we asked what the optimal way to control this system to perform fast delayed reaches is. We find that delay-period inputs consistently arise in an optimally controlled model of M1. By studying a variety of network architectures, we could dissect and predict the situations in which it is beneficial for a network to prepare. Finally, we show that optimal input-driven control of neural dynamics gives rise to multiple phases of preparation during reach sequences, providing a novel explanation for experimentally observed features of monkey M1 activity in double reaching.},
	language = {en},
	urldate = {2025-08-19},
	journal = {eLife},
	author = {Schimel, Marine and Kao, Ta-Chu and Hennequin, Guillaume},
	month = aug,
	year = {2024},
	note = {Publisher: eLife Sciences Publications Limited},
}

@misc{noauthor_cbl_nodate,
	title = {{CBL}},
	url = {https://cbl.eng.cam.ac.uk/hennequin/publications/},
	urldate = {2025-08-19},
}

@incollection{parzen_information_1998,
	address = {New York, NY},
	title = {Information {Theory} and an {Extension} of the {Maximum} {Likelihood} {Principle}},
	isbn = {978-1-4612-7248-9 978-1-4612-1694-0},
	url = {http://link.springer.com/10.1007/978-1-4612-1694-0_15},
	urldate = {2025-08-15},
	booktitle = {Selected {Papers} of {Hirotugu} {Akaike}},
	publisher = {Springer New York},
	author = {Akaike, Hirotogu},
	editor = {Parzen, Emanuel and Tanabe, Kunio and Kitagawa, Genshiro},
	year = {1998},
	doi = {10.1007/978-1-4612-1694-0_15},
	note = {Series Title: Springer Series in Statistics},
	pages = {199--213},
}

@article{schwarz_estimating_1978,
	title = {Estimating the {Dimension} of a {Model}},
	volume = {6},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/aos/1176344136.full},
	doi = {10.1214/aos/1176344136},
	abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
	number = {2},
	urldate = {2025-08-15},
	journal = {The Annals of Statistics},
	author = {Schwarz, Gideon},
	month = mar,
	year = {1978},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62F99, 62J99, Akaike information criterion, asymptotics, dimension},
	pages = {461--464},
}

@misc{noauthor_estimating_nodate,
	title = {Estimating the {Dimension} of a {Model}},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/aos/1176344136.full},
	urldate = {2025-08-15},
}

@article{ramezani_innovating_2025,
	title = {Innovating beyond electrophysiology through multimodal neural interfaces},
	volume = {2},
	copyright = {2024 Springer Nature Limited},
	issn = {2948-1201},
	url = {https://www.nature.com/articles/s44287-024-00121-x},
	doi = {10.1038/s44287-024-00121-x},
	abstract = {Neural circuits distributed across different brain regions mediate how neural information is processed and integrated, resulting in complex cognitive capabilities and behaviour. To understand dynamics and interactions of neural circuits, it is crucial to capture the complete spectrum of neural activity, ranging from the fast action potentials of individual neurons to the population dynamics driven by slow brain-wide oscillations. In this Review, we discuss how advances in electrical and optical recording technologies, coupled with the emergence of machine learning methodologies, present a unique opportunity to unravel the complex dynamics of the brain. Although great progress has been made in both electrical and optical neural recording technologies, these alone fail to provide a comprehensive picture of the neuronal activity with high spatiotemporal resolution. To address this challenge, multimodal experiments integrating the complementary advantages of different techniques hold great promise. However, they are still hindered by the absence of multimodal data analysis methods capable of providing unified and interpretable explanations of the complex neural dynamics distinctly encoded in these modalities. Combining multimodal studies with advanced data analysis methods will offer novel perspectives to address unresolved questions in basic neuroscience and to develop treatments for various neurological disorders.},
	language = {en},
	number = {1},
	urldate = {2025-08-10},
	journal = {Nature Reviews Electrical Engineering},
	author = {Ramezani, Mehrdad and Ren, Yundong and Cubukcu, Ertugrul and Kuzum, Duygu},
	month = jan,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Data processing, Extracellular recording, Fluorescence imaging},
	pages = {42--57},
}

@article{branicki_finite-time_2010,
	title = {Finite-time {Lagrangian} transport analysis: stable and unstable manifolds of hyperbolic trajectories and finite-time {Lyapunov} exponents},
	volume = {17},
	issn = {1023-5809},
	shorttitle = {Finite-time {Lagrangian} transport analysis},
	url = {https://npg.copernicus.org/articles/17/1/2010/},
	doi = {10.5194/npg-17-1-2010},
	abstract = {We consider issues associated with the Lagrangian characterisation of flow structures arising in aperiodically time-dependent vector fields that are only known on a finite time interval. A major motivation for the consideration of this problem arises from the desire to study transport and mixing problems in geophysical flows where the flow is obtained from a numerical solution, on a finite space-time grid, of an appropriate partial differential equation model for the velocity field. Of particular interest is the characterisation, location, and evolution of transport barriers in the flow, i.e. material curves and surfaces. We argue that a general theory of Lagrangian transport has to account for the effects of transient flow phenomena which are not captured by the infinite-time notions of hyperbolicity even for flows defined for all time. Notions of finite-time hyperbolic trajectories, their finite time stable and unstable manifolds, as well as finite-time Lyapunov exponent (FTLE) fields and associated Lagrangian coherent structures have been the main tools for characterising transport barriers in the time-aperiodic situation. In this paper we consider a variety of examples, some with explicit solutions, that illustrate in a concrete manner the issues and phenomena that arise in the setting of finite-time dynamical systems. Of particular significance for geophysical applications is the notion of flow transition which occurs when finite-time hyperbolicity is lost or gained. The phenomena discovered and analysed in our examples point the way to a variety of directions for rigorous mathematical research in this rapidly developing and important area of dynamical systems theory.},
	language = {English},
	number = {1},
	urldate = {2025-08-07},
	journal = {Nonlinear Processes in Geophysics},
	author = {Branicki, M. and Wiggins, S.},
	month = jan,
	year = {2010},
	note = {Publisher: Copernicus GmbH},
	pages = {1--36},
}

@article{bollt_is_2021,
	title = {Is the {Finite}-{Time} {Lyapunov} {Exponent} {Field} a {Koopman} {Eigenfunction}?},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/9/21/2731},
	doi = {10.3390/math9212731},
	abstract = {This work serves as a bridge between two approaches to analysis of dynamical systems: the local, geometric analysis, and the global operator theoretic Koopman analysis. We explicitly construct vector fields where the instantaneous Lyapunov exponent field is a Koopman eigenfunction. Restricting ourselves to polynomial vector fields to make this construction easier, we find that such vector fields do exist, and we explore whether such vector fields have a special structure, thus making a link between the geometric theory and the transfer operator theory.},
	language = {en},
	number = {21},
	urldate = {2025-08-06},
	journal = {Mathematics},
	author = {Bollt, Erik M. and Ross, Shane D.},
	month = jan,
	year = {2021},
	note = {Number: 21
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Koopman operator, Lyapunov exponent, dynamical systems, invariant manifolds, spectral analysis},
	pages = {2731},
}

@misc{noauthor_is_nodate,
	title = {Is the {Finite}-{Time} {Lyapunov} {Exponent} {Field} a {Koopman} {Eigenfunction}?},
	url = {https://www.mdpi.com/2227-7390/9/21/2731?trk=public_post-text},
	urldate = {2025-08-06},
}

@article{tanaka_separatrices_2009,
	title = {Separatrices and basins of stability from time series data: an application to biodynamics},
	volume = {58},
	issn = {1573-269X},
	shorttitle = {Separatrices and basins of stability from time series data},
	url = {https://doi.org/10.1007/s11071-008-9457-9},
	doi = {10.1007/s11071-008-9457-9},
	abstract = {An approach is presented for identifying separatrices in state space generated from noisy time series data sets which are representative of those generated from experiments. We demonstrate how these separatrices can be found using Lagrangian coherent structures (LCSs), ridges in the state space distribution of the maximum finite-time Lyapunov exponent. As opposed to the current approach which requires a vector field in the state space at each instant of time, this method can be performed using only trajectories reconstructed from time series. As such, this paper forms a bridge connecting methods for evaluating time series data with methods used to evaluate LCSs in vector fields. The methods are applied to a problem in musculoskeletal biomechanics, considered as an exemplar of a class of experimental systems that contain separatrices. In this case, the separatrix reveals a basin of stability for a balancing task, outside of which is a zone of failure. We demonstrate that LCSs calculated from only trajectory data, which samples only portions of the state space, align well with LCSs found using a known vector field. In general, we believe this method provides a fruitful approach for extracting information from noisy experimental data regarding boundaries between qualitatively different kinds of behavior.},
	language = {en},
	number = {1},
	urldate = {2025-08-06},
	journal = {Nonlinear Dynamics},
	author = {Tanaka, Martin L. and Ross, Shane D.},
	month = oct,
	year = {2009},
	keywords = {Applied Dynamical Systems, Basin of stability, Dynamical Systems, Fourier Analysis, LCS, Lagrangian coherent structures, Lyapunov exponents, Multistability, Nonlinear Dynamics and Chaos Theory, Recovery envelope, Separation Science, Separatrices, Time series analysis},
	pages = {1--21},
}

@article{tanaka_separatrices_2009-1,
	title = {Separatrices and basins of stability from time series data: an application to biodynamics},
	volume = {58},
	issn = {1573-269X},
	shorttitle = {Separatrices and basins of stability from time series data},
	url = {https://doi.org/10.1007/s11071-008-9457-9},
	doi = {10.1007/s11071-008-9457-9},
	abstract = {An approach is presented for identifying separatrices in state space generated from noisy time series data sets which are representative of those generated from experiments. We demonstrate how these separatrices can be found using Lagrangian coherent structures (LCSs), ridges in the state space distribution of the maximum finite-time Lyapunov exponent. As opposed to the current approach which requires a vector field in the state space at each instant of time, this method can be performed using only trajectories reconstructed from time series. As such, this paper forms a bridge connecting methods for evaluating time series data with methods used to evaluate LCSs in vector fields. The methods are applied to a problem in musculoskeletal biomechanics, considered as an exemplar of a class of experimental systems that contain separatrices. In this case, the separatrix reveals a basin of stability for a balancing task, outside of which is a zone of failure. We demonstrate that LCSs calculated from only trajectory data, which samples only portions of the state space, align well with LCSs found using a known vector field. In general, we believe this method provides a fruitful approach for extracting information from noisy experimental data regarding boundaries between qualitatively different kinds of behavior.},
	language = {en},
	number = {1},
	urldate = {2025-08-06},
	journal = {Nonlinear Dynamics},
	author = {Tanaka, Martin L. and Ross, Shane D.},
	month = oct,
	year = {2009},
	keywords = {Applied Dynamical Systems, Basin of stability, Dynamical Systems, Fourier Analysis, LCS, Lagrangian coherent structures, Lyapunov exponents, Multistability, Nonlinear Dynamics and Chaos Theory, Recovery envelope, Separation Science, Separatrices, Time series analysis},
	pages = {1--21},
}

@article{han_solving_2018,
	title = {Solving high-dimensional partial differential equations using deep learning},
	volume = {115},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1718942115},
	doi = {10.1073/pnas.1718942115},
	abstract = {Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the “curse of dimensionality.” This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black–Scholes equation, the Hamilton–Jacobi–Bellman equation, and the Allen–Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships.},
	number = {34},
	urldate = {2025-07-28},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
	month = aug,
	year = {2018},
	pages = {8505--8510},
}

@article{altan_estimating_2021,
	title = {Estimating the dimensionality of the manifold underlying multi-electrode neural recordings},
	volume = {17},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008591},
	doi = {10.1371/journal.pcbi.1008591},
	abstract = {It is generally accepted that the number of neurons in a given brain area far exceeds the number of neurons needed to carry any specific function controlled by that area. For example, motor areas of the human brain contain tens of millions of neurons that control the activation of tens or at most hundreds of muscles. This massive redundancy implies the covariation of many neurons, which constrains the population activity to a low-dimensional manifold within the space of all possible patterns of neural activity. To gain a conceptual understanding of the complexity of the neural activity within a manifold, it is useful to estimate its dimensionality, which quantifies the number of degrees of freedom required to describe the observed population activity without significant information loss. While there are many algorithms for dimensionality estimation, we do not know which are well suited for analyzing neural activity. The objective of this study was to evaluate the efficacy of several representative algorithms for estimating the dimensionality of linearly and nonlinearly embedded data. We generated synthetic neural recordings with known intrinsic dimensionality and used them to test the algorithms’ accuracy and robustness. We emulated some of the important challenges associated with experimental data by adding noise, altering the nature of the embedding of the low-dimensional manifold within the high-dimensional recordings, varying the dimensionality of the manifold, and limiting the amount of available data. We demonstrated that linear algorithms overestimate the dimensionality of nonlinear, noise-free data. In cases of high noise, most algorithms overestimated the dimensionality. We thus developed a denoising algorithm based on deep learning, the “Joint Autoencoder”, which significantly improved subsequent dimensionality estimation. Critically, we found that all algorithms failed when the intrinsic dimensionality was high (above 20) or when the amount of data used for estimation was low. Based on the challenges we observed, we formulated a pipeline for estimating the dimensionality of experimental neural data.},
	language = {en},
	number = {11},
	urldate = {2025-07-23},
	journal = {PLOS Computational Biology},
	author = {Altan, Ege and Solla, Sara A. and Miller, Lee E. and Perreault, Eric J.},
	month = nov,
	year = {2021},
	keywords = {Algorithms, Computational pipelines, Eigenvalues, Machine learning algorithms, Maximum likelihood estimation, Neurons, Principal component analysis, Signal to noise ratio},
	pages = {e1008591},
}

@article{stringer_spontaneous_2019,
	title = {Spontaneous behaviors drive multidimensional, brainwide activity},
	volume = {364},
	url = {https://www.science.org/doi/abs/10.1126/science.aav7893},
	doi = {10.1126/science.aav7893},
	abstract = {Neuronal populations in sensory cortex produce variable responses to sensory stimuli and exhibit intricate spontaneous activity even without external sensory input. Cortical variability and spontaneous activity have been variously proposed to represent random noise, recall of prior experience, or encoding of ongoing behavioral and cognitive variables. Recording more than 10,000 neurons in mouse visual cortex, we observed that spontaneous activity reliably encoded a high-dimensional latent state, which was partially related to the mouse’s ongoing behavior and was represented not just in visual cortex but also across the forebrain. Sensory inputs did not interrupt this ongoing signal but added onto it a representation of external stimuli in orthogonal dimensions. Thus, visual cortical population activity, despite its apparently noisy structure, reliably encodes an orthogonal fusion of sensory and multidimensional behavioral information.},
	number = {6437},
	urldate = {2025-07-23},
	journal = {Science},
	author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Reddy, Charu Bai and Carandini, Matteo and Harris, Kenneth D.},
	month = apr,
	year = {2019},
	pages = {eaav7893},
}

@misc{noauthor_spontaneous_nodate,
	title = {Spontaneous behaviors drive multidimensional, brainwide activity {\textbar} {Science}},
	url = {https://www.science.org/doi/abs/10.1126/science.aav7893},
	urldate = {2025-07-23},
}

@article{sohn_bayesian_2019,
	title = {Bayesian {Computation} through {Cortical} {Latent} {Dynamics}},
	volume = {103},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627319305628},
	doi = {10.1016/j.neuron.2019.06.012},
	abstract = {Statistical regularities in the environment create prior beliefs that we rely on to optimize our behavior when sensory information is uncertain. Bayesian theory formalizes how prior beliefs can be leveraged and has had a major impact on models of perception, sensorimotor function, and cognition. However, it is not known how recurrent interactions among neurons mediate Bayesian integration. By using a time-interval reproduction task in monkeys, we found that prior statistics warp neural representations in the frontal cortex, allowing the mapping of sensory inputs to motor outputs to incorporate prior statistics in accordance with Bayesian inference. Analysis of recurrent neural network models performing the task revealed that this warping was enabled by a low-dimensional curved manifold and allowed us to further probe the potential causal underpinnings of this computational strategy. These results uncover a simple and general principle whereby prior beliefs exert their influence on behavior by sculpting cortical latent dynamics.},
	number = {5},
	urldate = {2025-07-21},
	journal = {Neuron},
	author = {Sohn, Hansem and Narain, Devika and Meirhaeghe, Nicolas and Jazayeri, Mehrdad},
	month = sep,
	year = {2019},
	keywords = {Bayesian inference, Bayesian integration, frontal cortex, neural manifold, neural trajectories, recurrent neural networks},
	pages = {934--947.e5},
}

@article{chowdhury_area_2020,
	title = {Area 2 of primary somatosensory cortex encodes kinematics of the whole arm},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.48198},
	doi = {10.7554/eLife.48198},
	abstract = {Proprioception, the sense of body position, movement, and associated forces, remains poorly understood, despite its critical role in movement. Most studies of area 2, a proprioceptive area of somatosensory cortex, have simply compared neurons’ activities to the movement of the hand through space. Using motion tracking, we sought to elaborate this relationship by characterizing how area 2 activity relates to whole arm movements. We found that a whole-arm model, unlike classic models, successfully predicted how features of neural activity changed as monkeys reached to targets in two workspaces. However, when we then evaluated this whole-arm model across active and passive movements, we found that many neurons did not consistently represent the whole arm over both conditions. These results suggest that 1) neural activity in area 2 includes representation of the whole arm during reaching and 2) many of these neurons represented limb state differently during active and passive movements.},
	urldate = {2025-07-21},
	journal = {eLife},
	author = {Chowdhury, Raeed H and Glaser, Joshua I and Miller, Lee E},
	editor = {Makin, Tamar R and Gold, Joshua I and Makin, Tamar R},
	month = jan,
	year = {2020},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {limb perturbation, microelectrode array, monkey, motion tracking, multiple neurons, proprioception, reaching},
	pages = {e48198},
}

@misc{odoherty_nonhuman_2018,
	title = {Nonhuman {Primate} {Reaching} with {Multichannel} {Sensorimotor} {Cortex} {Electrophysiology}: broadband for indy\_20160927\_06},
	shorttitle = {Nonhuman {Primate} {Reaching} with {Multichannel} {Sensorimotor} {Cortex} {Electrophysiology}},
	url = {https://zenodo.org/records/1432819},
	abstract = {This dataset supplements https://doi.org/10.5281/zenodo.583331 .

General description. These data consist of extracellular neural recordings ("broadband") from primate subject "Indy", session identifier "indy\_20160927\_06".


Filtering. The data are sampled at 24414.0625 Hz and are unfiltered, except for an anti-aliasing filter built-in to the recording amplifier: a 4th order low-pass with a roll-off of 24 dB per octave at 7.5 kHz, operating at the sampling rate.


File format. The data are contained in an HDF5 formatted file, organized according to the Neurodata Without Borders (NWB) version 1.0.6 specification.


Datasets. A few of the relevant dataset paths are listed here for convenience. In the below, n refers to the number of recording channels and k refers to the number of samples.



	
"/acquisition/timeseries/broadband/data" - k x n
	

		
The broadband neural recordings.
	
	
	
"/acquisition/timeseries/broadband/data/conversion" (scalar attribute)
	

		
When multiplied by each sample converts the data into units of volts.
	
	
	
"/acquisition/timeseries/broadband/timestamps" - k x 1
	

		
Timestamps for each sample, seconds.
	
	
	
"/general/extracellular\_ephys/electrode\_map" - n x 3
	

		
The relative coordinates of each electrode contact (x, y, z), meters.
	
	



Please refer to the master dataset for further information.},
	urldate = {2025-07-21},
	publisher = {Zenodo},
	author = {O'Doherty, Joseph E. and Cardoso, Mariana M. B. and Makin, Joseph G. and Sabes, Philip N.},
	month = sep,
	year = {2018},
	keywords = {Utah array, brain-computer interface, brain-machine interface, electrophysiology, macaque, motor cortex, neuroscience, reaching},
}

@misc{harris_nonsense_2021,
	title = {Nonsense correlations in neuroscience},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.11.29.402719v3},
	doi = {10.1101/2020.11.29.402719},
	abstract = {Many neurophysiological signals exhibit slow continuous trends over time. Because standard correlation analyses assume that all samples are independent, they can yield apparently significant “nonsense correlations” even for signals that are completely unrelated. Here we compare the performance of several methods for assessing correlations between timeseries, using simulated slowly drifting signals with and without genuine correlations. The best performance was obtained from a “pseudosession method”, which relies on one of the signals being randomly generated by the experimenter, or a “session perturbation” method which requires multiple recordings under the same conditions. If neither of these is applicable, a “linear shift” method can be used when one of the signals is stationary. Methods based on cross-validation, circular shifting, phase randomization, or detrending gave up to 100\% false positive rates in our simulations. We conclude that analysis of neural timeseries is best performed when stationarity and randomization is built into the experimental design.},
	language = {en},
	urldate = {2025-07-19},
	publisher = {bioRxiv},
	author = {Harris, Kenneth D.},
	month = jun,
	year = {2021},
	note = {Pages: 2020.11.29.402719
Section: New Results},
}

@article{nakkiran_deep_2021,
	title = {Deep double descent: where bigger models and more data hurt*},
	volume = {2021},
	issn = {1742-5468},
	shorttitle = {Deep double descent},
	url = {https://dx.doi.org/10.1088/1742-5468/ac3a74},
	doi = {10.1088/1742-5468/ac3a74},
	abstract = {We show that a variety of modern deep learning tasks exhibit a ‘double-descent’ phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	language = {en},
	number = {12},
	urldate = {2025-07-16},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = dec,
	year = {2021},
	note = {Publisher: IOP Publishing and SISSA},
	pages = {124003},
}

@article{belkin_reconciling_2019,
	title = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
	volume = {116},
	url = {https://www.pnas.org/doi/10.1073/pnas.1903070116},
	doi = {10.1073/pnas.1903070116},
	abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
	number = {32},
	urldate = {2025-07-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	month = aug,
	year = {2019},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {15849--15854},
}

@inproceedings{zhu_unpaired_2017,
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8237506},
	doi = {10.1109/ICCV.2017.244},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	urldate = {2025-07-13},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = oct,
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {Extraterrestrial measurements, Graphics, Painting, Semantics, Training, Training data},
	pages = {2242--2251},
}

@misc{bauwens_stochastic_2005,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {The {Stochastic} {Conditional} {Duration} {Model}: {A} {Latent} {Factor} {Model} for the {Analysis} of {Financial} {Durations}},
	shorttitle = {The {Stochastic} {Conditional} {Duration} {Model}},
	url = {https://papers.ssrn.com/abstract=685421},
	abstract = {We introduce a class of models for the analysis of durations, which we call stochastic conditional duration (SCD) models. These models are based on the assumption that the durations are generated by a dynamic stochastic latent variable. The model yields a wide range of shapes of hazard functions. The estimation of the parameters is performed by quasi-maximum likelihood and using the Kalman filter. The model is applied to trade, price and volume durations of stocks traded at NYSE. We also investigate the relation between price durations, spread, trade intensity and volume.},
	language = {en},
	urldate = {2025-07-13},
	publisher = {Social Science Research Network},
	author = {Bauwens, Luc and Veredas, David},
	month = apr,
	year = {2005},
	keywords = {Duration, Hazard function, Latent variable model, Market microstructure},
}

@article{vinuesa_enhancing_2022,
	title = {Enhancing computational fluid dynamics with machine learning},
	volume = {2},
	copyright = {2022 Springer Nature America, Inc.},
	issn = {2662-8457},
	url = {https://www.nature.com/articles/s43588-022-00264-7},
	doi = {10.1038/s43588-022-00264-7},
	abstract = {Machine learning is rapidly becoming a core technology for scientific computing, with numerous opportunities to advance the field of computational fluid dynamics. Here we highlight some of the areas of highest potential impact, including to accelerate direct numerical simulations, to improve turbulence closure modeling and to develop enhanced reduced-order models. We also discuss emerging areas of machine learning that are promising for computational fluid dynamics, as well as some potential limitations that should be taken into account.},
	language = {en},
	number = {6},
	urldate = {2025-07-13},
	journal = {Nature Computational Science},
	author = {Vinuesa, Ricardo and Brunton, Steven L.},
	month = jun,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Applied mathematics, Computational science, Fluid dynamics, Mechanical engineering},
	pages = {358--366},
}

@misc{barber_bayesian_2012,
	title = {Bayesian {Reasoning} and {Machine} {Learning}},
	url = {https://www.cambridge.org/highereducation/books/bayesian-reasoning-and-machine-learning/37DAFA214EEE41064543384033D2ECF0},
	abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
	language = {en},
	urldate = {2025-07-13},
	journal = {Higher Education from Cambridge University Press},
	author = {Barber, David},
	month = feb,
	year = {2012},
	doi = {10.1017/CBO9780511804779},
	note = {ISBN: 9780511804779
Publisher: Cambridge University Press},
}

@inproceedings{pei_neural_2021,
	title = {Neural {Latents} {Benchmark} ‘21: {Evaluating} latent variable models of neural population activity},
	shorttitle = {Neural {Latents} {Benchmark} ‘21},
	url = {https://openreview.net/forum?id=KVMS3fl4Rsv},
	abstract = {Advances in neural recording present increasing opportunities to study neural activity in unprecedented detail. Latent variable models (LVMs) are promising tools for analyzing this rich activity across diverse neural systems and behaviors, as LVMs do not depend on known relationships between the activity and external experimental variables. However, progress with LVMs for neuronal population activity is currently impeded by a lack of standardization, resulting in methods being developed and compared in an ad hoc manner. To coordinate these modeling efforts, we introduce a benchmark suite for latent variable modeling of neural population activity. We curate four datasets of neural spiking activity from cognitive, sensory, and motor areas to promote models that apply to the wide variety of activity seen across these areas. We identify unsupervised evaluation as a common framework for evaluating models across datasets, and apply several baselines that demonstrate the variety of the benchmarked datasets. We release this benchmark through EvalAI. http://neurallatents.github.io},
	language = {en},
	urldate = {2025-07-13},
	author = {Pei, Felix C. and Ye, Joel and Zoltowski, David M. and Wu, Anqi and Chowdhury, Raeed Hasan and Sohn, Hansem and O'Doherty, Joseph E. and Shenoy, Krishna V. and Kaufman, Matthew and Churchland, Mark M. and Jazayeri, Mehrdad and Miller, Lee E. and Pillow, Jonathan W. and Park, Il Memming and Dyer, Eva L. and Pandarinath, Chethan},
	month = aug,
	year = {2021},
}

@inproceedings{nguyen_transformers_2019,
	address = {Hong Kong},
	title = {Transformers without {Tears}: {Improving} the {Normalization} of {Self}-{Attention}},
	shorttitle = {Transformers without {Tears}},
	url = {https://aclanthology.org/2019.iwslt-1.17/},
	abstract = {We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PRENORM) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose l2 normalization with a single scale parameter (SCALENORM) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FIXNORM). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT `15 English-Vietnamese. We ob- serve sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT `14 English-German), SCALENORM and FIXNORM remain competitive but PRENORM degrades performance.},
	urldate = {2025-07-13},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Spoken} {Language} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Nguyen, Toan Q. and Salazar, Julian},
	editor = {Niehues, Jan and Cattoni, Rolando and Stüker, Sebastian and Negri, Matteo and Turchi, Marco and Ha, Thanh-Le and Salesky, Elizabeth and Sanabria, Ramon and Barrault, Loic and Specia, Lucia and Federico, Marcello},
	month = nov,
	year = {2019},
}

@inproceedings{huang_improving_2020,
	title = {Improving {Transformer} {Optimization} {Through} {Better} {Initialization}},
	url = {https://proceedings.mlr.press/v119/huang20f.html},
	abstract = {The Transformer architecture has achieved considerable success recently; the key component of the Transformer is the attention layer that enables the model to focus on important regions within an input sequence. Gradient optimization with attention layers can be notoriously difficult requiring tricks such as learning rate warmup to prevent divergence. As Transformer models are becoming larger and more expensive to train, recent research has focused on understanding and improving optimization in these architectures. In this work our contributions are two-fold: we first investigate and empirically validate the source of optimization problems in the encoder-decoder Transformer architecture; we then propose a new weight initialization scheme with theoretical justification, that enables training without warmup or layer normalization. Empirical results on public machine translation benchmarks show that our approach achieves leading accuracy, allowing to train deep Transformer models with 200 layers in both encoder and decoder (over 1000 attention/MLP blocks) without difficulty. Code for this work is available here: {\textbackslash}url\{https://github.com/layer6ai-labs/T-Fixup\}.},
	language = {en},
	urldate = {2025-07-13},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {4475--4483},
}

@article{churchland_cortical_2010,
	title = {Cortical {Preparatory} {Activity}: {Representation} of {Movement} or {First} {Cog} in a {Dynamical} {Machine}?},
	volume = {68},
	issn = {0896-6273},
	shorttitle = {Cortical {Preparatory} {Activity}},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627310007579},
	doi = {10.1016/j.neuron.2010.09.015},
	abstract = {The motor cortices are active during both movement and movement preparation. A common assumption is that preparatory activity constitutes a subthreshold form of movement activity: a neuron active during rightward movements becomes modestly active during preparation of a rightward movement. We asked whether this pattern of activity is, in fact, observed. We found that it was not: at the level of a single neuron, preparatory tuning was weakly correlated with movement-period tuning. Yet, somewhat paradoxically, preparatory tuning could be captured by a preferred direction in an abstract “space” that described the population-level pattern of movement activity. In fact, this relationship accounted for preparatory responses better than did traditional tuning models. These results are expected if preparatory activity provides the initial state of a dynamical system whose evolution produces movement activity. Our results thus suggest that preparatory activity may not represent specific factors, and may instead play a more mechanistic role.},
	number = {3},
	urldate = {2025-07-12},
	journal = {Neuron},
	author = {Churchland, Mark M. and Cunningham, John P. and Kaufman, Matthew T. and Ryu, Stephen I. and Shenoy, Krishna V.},
	month = nov,
	year = {2010},
	pages = {387--400},
}

@misc{sedler_lfads-torch_2023,
	title = {lfads-torch: {A} modular and extensible implementation of latent factor analysis via dynamical systems},
	shorttitle = {lfads-torch},
	url = {http://arxiv.org/abs/2309.01230},
	doi = {10.48550/arXiv.2309.01230},
	abstract = {Latent factor analysis via dynamical systems (LFADS) is an RNN-based variational sequential autoencoder that achieves state-of-the-art performance in denoising high-dimensional neural activity for downstream applications in science and engineering. Recently introduced variants and extensions continue to demonstrate the applicability of the architecture to a wide variety of problems in neuroscience. Since the development of the original implementation of LFADS, new technologies have emerged that use dynamic computation graphs, minimize boilerplate code, compose model configuration files, and simplify large-scale training. Building on these modern Python libraries, we introduce lfads-torch -- a new open-source implementation of LFADS that unifies existing variants and is designed to be easier to understand, configure, and extend. Documentation, source code, and issue tracking are available at https://github.com/arsedler9/lfads-torch .},
	urldate = {2025-07-12},
	publisher = {arXiv},
	author = {Sedler, Andrew R. and Pandarinath, Chethan},
	month = sep,
	year = {2023},
	note = {arXiv:2309.01230 [cs]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	url = {https://ieeexplore.ieee.org/document/5206848},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	urldate = {2025-07-12},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
}

@inproceedings{yu_gaussian-process_2008,
	title = {Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity},
	volume = {21},
	url = {https://papers.nips.cc/paper_files/paper/2008/hash/ad972f10e0800b49d76fed33a21f6698-Abstract.html},
	abstract = {We consider the problem of extracting smooth low-dimensional neural trajectories'' that summarize the activity recorded simultaneously from tens to hundreds of neurons on individual experimental trials. Beyond the benefit of visualizing the high-dimensional noisy spiking activity in a compact denoised form, such trajectories can offer insight into the dynamics of the neural circuitry underlying the recorded activity. Current methods for extracting neural trajectories involve a two-stage process: the data are firstdenoised'' by smoothing over time, then a static dimensionality reduction technique is applied. We first describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way, and account for spiking variability that may vary both across neurons and across time. We then present a novel method for extracting neural trajectories, Gaussian-process factor analysis (GPFA), which unifies the smoothing and dimensionality reduction operations in a common probabilistic framework. We applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution. By adopting a goodness-of-fit metric that measures how well the activity of each neuron can be predicted by all other recorded neurons, we found that GPFA provided a better characterization of the population activity than the two-stage methods. From the extracted single-trial neural trajectories, we directly observed a convergence in neural state during motor planning, an effect suggestive of attractor dynamics that was shown indirectly by previous studies.},
	urldate = {2025-07-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yu, Byron M and Cunningham, John P and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna V and Sahani, Maneesh},
	year = {2008},
}

@article{perkins_simple_2023,
	title = {Simple decoding of behavior from a complicated neural manifold},
	volume = {12},
	url = {https://elifesciences.org/reviewed-preprints/89421},
	doi = {10.7554/eLife.89421.1},
	abstract = {Decoders for brain-computer interfaces (BCIs) assume constraints on neural activity, chosen to reflect scientific beliefs while yielding tractable computations. We document how low tangling – a typical property of motor-cortex neural trajectories – yields unusual neural geometries. We designed a decoder, MINT, to embrace statistical constraints that are appropriate for these geometries. MINT takes a trajectory-centric approach: a library of neural trajectories (rather than a set of neural dimensions) provides a scaffold approximating the neural manifold. Each neural trajectory has a corresponding behavioral trajectory, allowing straightforward but highly nonlinear decoding. MINT consistently outperformed other interpretable methods, and outperformed expressive machine learning methods in 37 of 42 comparisons. Yet unlike these expressive methods, MINT’s constraints are known rather than the implicit result of optimizing decoder output. MINT performed well across tasks, suggesting its assumptions are generally well-matched to the statistics of neural data. Despite embracing highly nonlinear relationships between behavior and potentially complex neural trajectories, MINT’s computations are simple, scalable, and provide interpretable quantities such as data likelihoods. MINT’s performance and simplicity suggest it may be an excellent candidate for clinical BCI applications.},
	language = {en},
	urldate = {2025-07-12},
	journal = {eLife},
	author = {Perkins, Sean M. and Cunningham, John P. and Wang, Qi and Churchland, Mark M.},
	month = oct,
	year = {2023},
	note = {Publisher: eLife Sciences Publications Limited},
}

@article{ye_representation_2021,
	title = {Representation learning for neural population activity with {Neural} {Data} {Transformers}},
	volume = {5},
	url = {https://nbdt.scholasticahq.com/article/27358-representation-learning-for-neural-population-activity-with-neural-data-transformers, https://nbdt.scholasticahq.com/article/27358-representation-learning-for-neural-population-activity-with-neural-data-transformers},
	doi = {10.51628/001c.27358},
	abstract = {Neural population activity is theorized to reflect an underlying dynamical structure. This structure can be accurately captured using state space models with explicit dynamics, such as those based on recurrent neural networks (RNNs). However, using recurrence to explicitly model dynamics necessitates sequential processing of data, slowing real-time applications such as brain-computer interfaces. Here we introduce the Neural Data Transformer (NDT), a non-recurrent alternative. We test the NDT's ability to capture autonomous dynamical systems by applying it to synthetic datasets with known dynamics and data from monkey motor cortex during a reaching task well-modeled by RNNs. The NDT models these datasets as well as state-of-the-art recurrent models. Further, its non-recurrence enables 3.9ms inference, well within the loop time of real-time applications and more than 6 times faster than recurrent baselines on the monkey reaching dataset. These results suggest that an explicit dynamics model is not necessary to model autonomous neural population dynamics. Code: https://github.com/snel-repo/neural-data-transformers},
	language = {en},
	number = {3},
	urldate = {2025-07-12},
	journal = {Neurons, Behavior, Data analysis, and Theory},
	author = {Ye, Joel and Pandarinath, Chethan},
	month = aug,
	year = {2021},
	note = {Publisher: The neurons, behavior, data analysis and theory collective},
	pages = {1--18},
}

@article{le_stndt_2022,
	title = {{STNDT}: {Modeling} {Neural} {Population} {Activity} with {Spatiotemporal} {Transformers}},
	volume = {35},
	shorttitle = {{STNDT}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/72163d1c3c1726f1c29157d06e9e93c1-Abstract-Conference.html},
	language = {en},
	urldate = {2025-07-12},
	journal = {Advances in Neural Information Processing Systems},
	author = {Le, Trung and Shlizerman, Eli},
	month = dec,
	year = {2022},
	pages = {17926--17939},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2025-07-12},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2025-07-12},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gokcen_disentangling_2022,
	title = {Disentangling the flow of signals between populations of neurons},
	volume = {2},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {2662-8457},
	url = {https://www.nature.com/articles/s43588-022-00282-5},
	doi = {10.1038/s43588-022-00282-5},
	abstract = {Technological advances now allow us to record from large populations of neurons across multiple brain areas. These recordings may illuminate how communication between areas contributes to brain function, yet a substantial barrier remains: how do we disentangle the concurrent, bidirectional flow of signals between populations of neurons? We propose here a dimensionality reduction framework, delayed latents across groups (DLAG), that disentangles signals relayed in each direction, identifies how these signals are represented by each population and characterizes how they evolve within and across trials. We demonstrate that DLAG performs well on synthetic datasets similar in scale to current neurophysiological recordings. Then we study simultaneously recorded populations in primate visual areas V1 and V2, where DLAG reveals signatures of bidirectional yet selective communication. Our framework lays a foundation for dissecting the intricate flow of signals across populations of neurons, and how this signalling contributes to cortical computation.},
	language = {en},
	number = {8},
	urldate = {2025-07-12},
	journal = {Nature Computational Science},
	author = {Gokcen, Evren and Jasper, Anna I. and Semedo, João D. and Zandvakili, Amin and Kohn, Adam and Machens, Christian K. and Yu, Byron M.},
	month = aug,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational models, Computational neuroscience, Visual system},
	pages = {512--525},
}

@inproceedings{linderman_bayesian_2017,
	title = {Bayesian {Learning} and {Inference} in {Recurrent} {Switching} {Linear} {Dynamical} {Systems}},
	url = {https://proceedings.mlr.press/v54/linderman17a.html},
	abstract = {Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics.  We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. Building on switching linear dynamical systems (SLDS), we develop a model class and Bayesian inference algorithms that not only discover these dynamical units but also, by learning how transition probabilities depend on observations or continuous latent states, explain their switching behavior.  Our key innovation is to design these recurrent SLDS models to enable recent Pólya-gamma auxiliary variable techniques and thus make approximate Bayesian learning and inference in these models easy, fast, and scalable.},
	language = {en},
	urldate = {2025-07-12},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Linderman, Scott and Johnson, Matthew and Miller, Andrew and Adams, Ryan and Blei, David and Paninski, Liam},
	month = apr,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {914--922},
}

@article{pals_inferring_2024,
	title = {Inferring stochastic low-rank recurrent neural networks from neural data},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/209423f076b6479ab3a4f45886e30306-Abstract-Conference.html},
	language = {en},
	urldate = {2025-07-12},
	journal = {Advances in Neural Information Processing Systems},
	author = {Pals, Matthijs and Sağtekin, A. Erdem and Pei, Felix and Gloeckler, Manuel and Macke, Jakob H.},
	month = dec,
	year = {2024},
	pages = {18225--18264},
}

@article{sedler_expressive_2023,
	title = {Expressive architectures enhance interpretability of dynamics-based neural population models},
	url = {https://nbdt.scholasticahq.com/article/73987-expressive-architectures-enhance-interpretability-of-dynamics-based-neural-population-models, https://nbdt.scholasticahq.com/article/73987-expressive-architectures-enhance-interpretability-of-dynamics-based-neural-population-models},
	doi = {10.51628/001c.73987},
	abstract = {Artificial neural networks that can recover latent dynamics from recorded neural activity may provide a powerful avenue for identifying and interpreting the dynamical motifs underlying biological computation. Given that neural variance alone does not uniquely determine a latent dynamical system, interpretable architectures should prioritize accurate and low-dimensional latent dynamics. In this work, we evaluated the performance of sequential autoencoders (SAEs) in recovering latent chaotic attractors from simulated neural datasets. We found that SAEs with widely-used recurrent neural network (RNN)-based dynamics were unable to infer accurate firing rates at the true latent state dimensionality, and that larger RNNs relied upon dynamical features not present in the data. On the other hand, SAEs with neural ordinary differential equation (NODE)-based dynamics inferred accurate rates at the true latent state dimensionality, while also recovering latent trajectories and fixed point structure. Ablations reveal that this is mainly because NODEs (1) allow use of higher-capacity multi-layer perceptrons (MLPs) to model the vector field and (2) predict the derivative rather than the next state. Decoupling the capacity of the dynamics model from its latent dimensionality enables NODEs to learn the requisite low-D dynamics where RNN cells fail. Additionally, the fact that the NODE predicts derivatives imposes a useful autoregressive prior on the latent states. The suboptimal interpretability of widely-used RNN based dynamics may motivate substitution for alternative architectures, such as NODE, that enable learning of accurate dynamics in low-dimensional latent spaces.},
	language = {en},
	urldate = {2025-07-12},
	journal = {Neurons, Behavior, Data analysis, and Theory},
	author = {Sedler, Andrew R. and Versteeg, Christopher and Pandarinath, Chethan},
	month = mar,
	year = {2023},
	note = {Publisher: The neurons, behavior, data analysis and theory collective},
	pages = {1--22},
}

@article{koppe_identifying_2019,
	title = {Identifying nonlinear dynamical systems via generative recurrent neural networks with applications to {fMRI}},
	volume = {15},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007263},
	doi = {10.1371/journal.pcbi.1007263},
	abstract = {A major tenet in theoretical neuroscience is that cognitive and behavioral processes are ultimately implemented in terms of the neural system dynamics. Accordingly, a major aim for the analysis of neurophysiological measurements should lie in the identification of the computational dynamics underlying task processing. Here we advance a state space model (SSM) based on generative piecewise-linear recurrent neural networks (PLRNN) to assess dynamics from neuroimaging data. In contrast to many other nonlinear time series models which have been proposed for reconstructing latent dynamics, our model is easily interpretable in neural terms, amenable to systematic dynamical systems analysis of the resulting set of equations, and can straightforwardly be transformed into an equivalent continuous-time dynamical system. The major contributions of this paper are the introduction of a new observation model suitable for functional magnetic resonance imaging (fMRI) coupled to the latent PLRNN, an efficient stepwise training procedure that forces the latent model to capture the ‘true’ underlying dynamics rather than just fitting (or predicting) the observations, and of an empirical measure based on the Kullback-Leibler divergence to evaluate from empirical time series how well this goal of approximating the underlying dynamics has been achieved. We validate and illustrate the power of our approach on simulated ‘ground-truth’ dynamical systems as well as on experimental fMRI time series, and demonstrate that the learnt dynamics harbors task-related nonlinear structure that a linear dynamical model fails to capture. Given that fMRI is one of the most common techniques for measuring brain activity non-invasively in human subjects, this approach may provide a novel step toward analyzing aberrant (nonlinear) dynamics for clinical assessment or neuroscientific research.},
	language = {en},
	number = {8},
	urldate = {2025-07-12},
	journal = {PLOS Computational Biology},
	author = {Koppe, Georgia and Toutounji, Hazem and Kirsch, Peter and Lis, Stefanie and Durstewitz, Daniel},
	month = aug,
	year = {2019},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Covariance, Dynamical systems, Functional magnetic resonance imaging, Nonlinear dynamics, Nonlinear systems, Statistical models, System instability},
	pages = {e1007263},
}

@inproceedings{versteeg_expressive_2024,
	title = {Expressive dynamics models with nonlinear injective readouts enable reliable recovery of latent features from neural activity},
	url = {https://proceedings.mlr.press/v228/versteeg24a.html},
	abstract = {An emerging framework in neuroscience uses the rules that govern how a neural circuit’s state evolves over time to understand the circuit’s underlying computation. While these neural dynamics cannot be directly measured, new techniques attempt to estimate them by modeling observed neural recordings as a low-dimensional latent dynamical system embedded into a higher-dimensional neural space. How these models represent the readout from latent space to neural space can affect the interpretability of the latent representation – for example, for models with a linear readout could make simple, low-dimensional dynamics unfolding on a non-linear neural manifold appear excessively complex and high-dimensional. Additionally, standard readouts (both linear and non-linear) often lack injectivity, meaning that they don’t obligate changes in latent state to directly affect activity in the neural space. During training, non-injective readouts incentivize the model to invent dynamics that misrepresent the underlying system and computation. To address the challenges presented by non-linearity and non-injectivity, we combined a custom readout with a previously developed low-dimensional latent dynamics model to create the Ordinary Differential equations autoencoder with Injective Nonlinear readout (ODIN). We generated a synthetic spiking dataset by non-linearly embedding activity from a low-dimensional dynamical system into higher-D neural activity. We show that, in contrast to alternative models, ODIN is able to recover ground-truth latent activity from these data even when the nature of the system and embedding are unknown. Additionally, we show that ODIN enables the unsupervised recovery of underlying dynamical features (e.g., fixed points) and embedding geometry (e.g., the neural manifold) over alternative models. Overall, ODIN’s ability to recover ground-truth latent features with low dimensionality make it a promising method for distilling interpretable dynamics that can explain neural computation.},
	language = {en},
	urldate = {2025-07-12},
	booktitle = {Proceedings of the 2nd {NeurIPS} {Workshop} on {Symmetry} and {Geometry} in {Neural} {Representations}},
	publisher = {PMLR},
	author = {Versteeg, Christopher and Sedler, Andrew R. and McCart, Jonathan D. and Pandarinath, Chethan},
	month = aug,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {255--278},
}

@article{dillavou_demonstration_2022,
	title = {Demonstration of {Decentralized} {Physics}-{Driven} {Learning}},
	volume = {18},
	url = {https://link.aps.org/doi/10.1103/PhysRevApplied.18.014040},
	doi = {10.1103/PhysRevApplied.18.014040},
	abstract = {In typical artificial neural networks, neurons adjust according to global calculations of a central processor, but in the brain, neurons and synapses self-adjust based on local information. Contrastive learning algorithms have recently been proposed to train physical systems, such as fluidic, mechanical, or electrical networks, to perform machine-learning tasks from local evolution rules. However, to date, such systems have only been implemented in silico due to the engineering challenge of creating elements that autonomously evolve based on their own response to two sets of global boundary conditions. Here, we introduce and implement a physics-driven contrastive learning scheme for a network of variable resistors, using circuitry to locally compare the response of two identical networks subjected to the two different sets of boundary conditions. Using this method, our system effectively trains itself, optimizing its resistance values without the use of a central processor or external information storage. Once the system is trained for a specified allostery, regression, or classification task, the task is subsequently performed rapidly and automatically by the physical imperative to minimize power dissipation in response to the given voltage inputs. We demonstrate that, unlike typical computers, such learning systems are robust to extreme damage (and thus manufacturing defects) due to their decentralized learning. Our twin-network approach is therefore readily scalable to extremely large or nonlinear networks, where its distributed nature will be an enormous advantage; a laboratory network of only 500 edges will already outpace its in silico counterpart.},
	number = {1},
	urldate = {2025-07-10},
	journal = {Physical Review Applied},
	author = {Dillavou, Sam and Stern, Menachem and Liu, Andrea J. and Durian, Douglas J.},
	month = jul,
	year = {2022},
	note = {Publisher: American Physical Society},
	pages = {014040},
}

@article{rocks_limits_2019,
	title = {Limits of multifunctionality in tunable networks},
	volume = {116},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1806790116},
	doi = {10.1073/pnas.1806790116},
	abstract = {Nature is rife with networks that are functionally optimized to propagate inputs to perform specific tasks. Whether via genetic evolution or dynamic adaptation, many networks create functionality by locally tuning interactions between nodes. Here we explore this behavior in two contexts: strain propagation in mechanical networks and pressure redistribution in flow networks. By adding and removing links, we are able to optimize both types of networks to perform specific functions. We define a single function as a tuned response of a single “target” link when another, predetermined part of the network is activated. Using network structures generated via such optimization, we investigate how many simultaneous functions such networks can be programed to fulfill. We find that both flow and mechanical networks display qualitatively similar phase transitions in the number of targets that can be tuned, along with the same robust finite-size scaling behavior. We discuss how these properties can be understood in the context of constraint–satisfaction problems.},
	number = {7},
	urldate = {2025-07-10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Rocks, Jason W. and Ronellenfitsch, Henrik and Liu, Andrea J. and Nagel, Sidney R. and Katifori, Eleni},
	month = feb,
	year = {2019},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {2506--2511},
}

@misc{schmutz_high-dimensional_2025,
	title = {High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {High-dimensional neuronal activity from low-dimensional latent dynamics},
	url = {https://www.biorxiv.org/content/10.1101/2025.06.03.657632v1},
	doi = {10.1101/2025.06.03.657632},
	abstract = {Computation in recurrent networks of neurons has been hypothesized to occur at the level of low-dimensional latent dynamics, both in artificial systems and in the brain. This hypothesis seems at odds with evidence from large-scale neuronal recordings in mice showing that neuronal population activity is high-dimensional. To demonstrate that low-dimensional latent dynamics and high-dimensional activity can be two sides of the same coin, we present an analytically solvable recurrent neural network (RNN) model whose dynamics can be exactly reduced to a low-dimensional dynamical system, but generates an activity manifold that has a high linear embedding dimension. This raises the question: Do low-dimensional latents explain the high-dimensional activity observed in mouse visual cortex? Spectral theory tells us that the covariance eigenspectrum alone does not allow us to recover the dimensionality of the latents, which can be low or high, when neurons are nonlinear. To address this indeterminacy, we develop Neural Cross-Encoder (NCE), an interpretable, nonlinear latent variable modeling method for neuronal recordings, and find that high-dimensional neuronal responses to drifting gratings and sponta-neous activity in visual cortex can be reduced to low-dimensional latents, while the responses to natural images cannot. We conclude that the high-dimensional activity measured in certain conditions, such as in the absence of a stimulus, is explained by low-dimensional latents that are nonlinearly processed by individual neurons.},
	language = {en},
	urldate = {2025-06-22},
	publisher = {bioRxiv},
	author = {Schmutz, Valentin and Haydaroglu, Ali and Wang, Shuqi and Feng, Yixiao and Carandini, Matteo and Harris, Kenneth D.},
	month = jun,
	year = {2025},
	note = {Pages: 2025.06.03.657632
Section: New Results},
}

@misc{chen_whole-brain_2025,
	title = {Whole-brain, all-optical interrogation of neuronal dynamics underlying gut interoception in zebrafish},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.03.26.645305v1},
	doi = {10.1101/2025.03.26.645305},
	abstract = {Internal signals from the body and external signals from the environment are processed by brain-wide circuits to guide behavior. However, the complete brain-wide circuit activity underlying interoception—the perception of bodily signals—and its interactions with sensorimotor circuits remain unclear due to technical barriers to accessing whole-brain activity at the cellular level during organ physiology perturbations. We developed an all-optical system for whole-brain neuronal imaging in behaving larval zebrafish during optical uncaging of gut-targeted nutrients and visuo-motor stimulation. Widespread neural activity throughout the brain encoded nutrient delivery, unfolding on multiple timescales across many specific peripheral and central regions. Evoked activity depended on delivery location and occurred with amino acids and D-glucose, but not L-glucose. Many gut-sensitive neurons also responded to swimming and visual stimuli, with brainstem areas primarily integrating gut and motor signals and midbrain regions integrating gut and visual signals. This platform links body-brain communication studies to brain-wide neural computation in awake, behaving vertebrates.},
	language = {en},
	urldate = {2025-06-13},
	publisher = {bioRxiv},
	author = {Chen, Weiyu and James, Ben and Ruetten, Virginia M. S. and Banala, Sambashiva and Wei, Ziqiang and Fleishman, Greg and Rubinov, Mikail and Fishman, Mark C. and Engert, Florian and Lavis, Luke D. and Fitzgerald, James E. and Ahrens, Misha B.},
	month = mar,
	year = {2025},
	note = {Pages: 2025.03.26.645305
Section: New Results},
}

@article{sugihara_detecting_2012,
	title = {Detecting {Causality} in {Complex} {Ecosystems}},
	volume = {338},
	url = {https://www.science.org/doi/10.1126/science.1227079},
	doi = {10.1126/science.1227079},
	abstract = {Identifying causal networks is important for effective policy and management recommendations on climate, epidemiology, financial regulation, and much else. We introduce a method, based on nonlinear state space reconstruction, that can distinguish causality from correlation. It extends to nonseparable weakly connected dynamic systems (cases not covered by the current Granger causality paradigm). The approach is illustrated both by simple models (where, in contrast to the real world, we know the underlying equations/relations and so can check the validity of our method) and by application to real ecological systems, including the controversial sardine-anchovy-temperature problem.},
	number = {6106},
	urldate = {2025-06-12},
	journal = {Science},
	author = {Sugihara, George and May, Robert and Ye, Hao and Hsieh, Chih-hao and Deyle, Ethan and Fogarty, Michael and Munch, Stephan},
	month = oct,
	year = {2012},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {496--500},
}

@inproceedings{karniol-tambour_modeling_2023,
	title = {Modeling state-dependent communication between brain regions with switching nonlinear dynamical systems},
	url = {https://openreview.net/forum?id=WQwV7Y8qwa},
	abstract = {Understanding how multiple brain regions interact to produce behavior is a major challenge in systems neuroscience, with many regions causally implicated in common tasks such as sensory processing and decision making. A precise description of interactions between regions remains an open problem. Moreover, neural dynamics are nonlinear and non-stationary. Here, we propose MR-SDS, a multiregion, switching nonlinear state space model that decomposes global dynamics into local and cross-communication components in the latent space. MR-SDS includes directed interactions between brain regions, allowing for estimation of state-dependent communication signals, and accounts for sensory inputs effects. We show that our model accurately recovers latent trajectories, vector fields underlying switching nonlinear dynamics, and cross-region communication profiles in three simulations. We then apply our method to two large-scale, multi-region neural datasets involving mouse decision making. The first includes hundreds of neurons per region, recorded simultaneously at single-cell-resolution across 3 distant cortical regions. The second is a mesoscale widefield dataset of 8 adjacent cortical regions imaged across both hemispheres. On these multi-region datasets, our model outperforms existing piece-wise linear multi-region models and reveals multiple distinct dynamical states and a rich set of cross-region communication profiles.},
	language = {en},
	urldate = {2025-06-11},
	author = {Karniol-Tambour, Orren and Zoltowski, David M. and Diamanti, E. Mika and Pinto, Lucas and Brody, Carlos D. and Tank, David W. and Pillow, Jonathan W.},
	month = oct,
	year = {2023},
}

@article{nejatbakhsh_predicting_2023,
	title = {Predicting the effect of micro-stimulation on macaque prefrontal activity based on spontaneous circuit dynamics},
	volume = {5},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.5.043211},
	doi = {10.1103/PhysRevResearch.5.043211},
	abstract = {This article is part of the Physical Review Research collection titled Physics of Neuroscience.},
	number = {4},
	urldate = {2025-06-08},
	journal = {Physical Review Research},
	author = {Nejatbakhsh, Amin and Fumarola, Francesco and Esteki, Saleh and Toyoizumi, Taro and Kiani, Roozbeh and Mazzucato, Luca},
	month = dec,
	year = {2023},
	note = {Publisher: American Physical Society},
	pages = {043211},
}

@article{depasquale_neural_2024,
	title = {Neural population dynamics underlying evidence accumulation in multiple rat brain regions},
	volume = {13},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.84955},
	doi = {10.7554/eLife.84955},
	abstract = {Accumulating evidence to make decisions is a core cognitive function. Previous studies have tended to estimate accumulation using either neural or behavioral data alone. Here, we develop a unified framework for modeling stimulus-driven behavior and multi-neuron activity simultaneously. We applied our method to choices and neural recordings from three rat brain regions—the posterior parietal cortex (PPC), the frontal orienting fields (FOF), and the anterior-dorsal striatum (ADS)—while subjects performed a pulse-based accumulation task. Each region was best described by a distinct accumulation model, which all differed from the model that best described the animal’s choices. FOF activity was consistent with an accumulator where early evidence was favored while the ADS reflected near perfect accumulation. Neural responses within an accumulation framework unveiled a distinct association between each brain region and choice. Choices were better predicted from all regions using a comprehensive, accumulation-based framework and different brain regions were found to differentially reflect choice-related accumulation signals: FOF and ADS both reflected choice but ADS showed more instances of decision vacillation. Previous studies relating neural data to behaviorally inferred accumulation dynamics have implicitly assumed that individual brain regions reflect the whole-animal level accumulator. Our results suggest that different brain regions represent accumulated evidence in dramatically different ways and that accumulation at the whole-animal level may be constructed from a variety of neural-level accumulators.},
	urldate = {2025-06-03},
	journal = {eLife},
	author = {DePasquale, Brian and Brody, Carlos D and Pillow, Jonathan W},
	editor = {Forstmann, Birte U and Frank, Michael J and Turner, Brandon},
	month = aug,
	year = {2024},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {decision-making, evidence accumulation, latent variable models},
	pages = {e84955},
}

@article{semedo_cortical_2019,
	title = {Cortical {Areas} {Interact} through a {Communication} {Subspace}},
	volume = {102},
	issn = {0896-6273},
	url = {https://www.cell.com/neuron/abstract/S0896-6273(19)30053-4},
	doi = {10.1016/j.neuron.2019.01.026},
	language = {English},
	number = {1},
	urldate = {2025-06-01},
	journal = {Neuron},
	author = {Semedo, João D. and Zandvakili, Amin and Machens, Christian K. and Yu, Byron M. and Kohn, Adam},
	month = apr,
	year = {2019},
	pmid = {30770252},
	note = {Publisher: Elsevier},
	keywords = {area V2, corticocortical, dimensionality reduction, inter-areal communication, macaque, neural population, neural variability, primary visual cortex, vision, visual cortex},
	pages = {249--259.e4},
}

@article{macdowell_multiplexed_2025,
	title = {Multiplexed subspaces route neural activity across brain-wide networks},
	volume = {16},
	copyright = {2025 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-025-58698-2},
	doi = {10.1038/s41467-025-58698-2},
	abstract = {Cognition is flexible, allowing behavior to change on a moment-by-moment basis. Such flexibility relies on the brain’s ability to route information through different networks of brain regions to perform different cognitive computations. However, the mechanisms that determine which network of regions is active are not well understood. Here, we combined cortex-wide calcium imaging with high-density electrophysiological recordings in eight cortical and subcortical regions of mice to understand the interactions between regions. We found different dimensions within the population activity of each region were functionally connected with different cortex-wide ‘subspace networks’ of regions. These subspace networks were multiplexed; each region was functionally connected with multiple independent, yet overlapping, subspace networks. The subspace network that was active changed from moment-to-moment. These changes were associated with changes in the geometric relationship between the neural response within a region and the subspace dimensions: when neural responses were aligned with (i.e., projected along) a subspace dimension, neural activity was increased in the associated regions. Together, our results suggest that changing the geometry of neural representations within a brain region may allow the brain to flexibly engage different brain-wide networks, thereby supporting cognitive flexibility.},
	language = {en},
	number = {1},
	urldate = {2025-06-01},
	journal = {Nature Communications},
	author = {MacDowell, Camden J. and Libby, Alexandra and Jahn, Caroline I. and Tafazoli, Sina and Ardalan, Adel and Buschman, Timothy J.},
	month = apr,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cognitive control, Dynamical systems},
	pages = {3359},
}

@article{hu_tackling_2024,
	title = {Tackling the curse of dimensionality with physics-informed neural networks},
	volume = {176},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608024002934},
	doi = {10.1016/j.neunet.2024.106369},
	abstract = {The curse-of-dimensionality taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional partial differential equations (PDEs), as Richard E. Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerical PDEs in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. We develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs’ and PINNs’ residual into pieces corresponding to different dimensions and randomly samples a subset of these dimensional pieces in each iteration of training PINNs. We prove theoretically the convergence and other desired properties of the proposed method. We demonstrate in various diverse tests that the proposed method can solve many notoriously hard high-dimensional PDEs, including the Hamilton–Jacobi-Bellman (HJB) and the Schrödinger equations in tens of thousands of dimensions very fast on a single GPU using the PINNs mesh-free approach. Notably, we solve nonlinear PDEs with nontrivial, anisotropic, and inseparable solutions in less than one hour for 1000 dimensions and in 12 h for 100,000 dimensions on a single GPU using SDGD with PINNs. Since SDGD is a general training methodology of PINNs, it can be applied to any current and future variants of PINNs to scale them up for arbitrary high-dimensional PDEs.},
	urldate = {2025-05-29},
	journal = {Neural Networks},
	author = {Hu, Zheyuan and Shukla, Khemraj and Karniadakis, George Em and Kawaguchi, Kenji},
	month = aug,
	year = {2024},
	keywords = {Curse of dimensionality, Physics-informed neural networks},
	pages = {106369},
}

@article{wang_kolmogorov_2025,
	title = {Kolmogorov {Arnold} {Informed} neural network: {A} physics-informed deep learning framework for solving forward and inverse problems based on {Kolmogorov} {Arnold} {Networks}},
	volume = {433},
	issn = {00457825},
	shorttitle = {Kolmogorov {Arnold} {Informed} neural network},
	url = {http://arxiv.org/abs/2406.11045},
	doi = {10.1016/j.cma.2024.117518},
	abstract = {AI for partial differential equations (PDEs) has garnered significant attention, particularly with the emergence of Physics-informed neural networks (PINNs). The recent advent of Kolmogorov-Arnold Network (KAN) indicates that there is potential to revisit and enhance the previously MLP-based PINNs. Compared to MLPs, KANs offer interpretability and require fewer parameters. PDEs can be described in various forms, such as strong form, energy form, and inverse form. While mathematically equivalent, these forms are not computationally equivalent, making the exploration of different PDE formulations significant in computational physics. Thus, we propose different PDE forms based on KAN instead of MLP, termed Kolmogorov-Arnold-Informed Neural Network (KINN) for solving forward and inverse problems. We systematically compare MLP and KAN in various numerical examples of PDEs, including multi-scale, singularity, stress concentration, nonlinear hyperelasticity, heterogeneous, and complex geometry problems. Our results demonstrate that KINN significantly outperforms MLP regarding accuracy and convergence speed for numerous PDEs in computational solid mechanics, except for the complex geometry problem. This highlights KINN's potential for more efficient and accurate PDE solutions in AI for PDEs.},
	urldate = {2025-05-29},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Wang, Yizheng and Sun, Jia and Bai, Jinshuai and Anitescu, Cosmin and Eshaghi, Mohammad Sadegh and Zhuang, Xiaoying and Rabczuk, Timon and Liu, Yinghua},
	month = jan,
	year = {2025},
	note = {arXiv:2406.11045 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis},
	pages = {117518},
}

@article{anagnostopoulos_residual-based_2024,
	title = {Residual-based attention in physics-informed neural networks},
	volume = {421},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782524000616},
	doi = {10.1016/j.cma.2024.116805},
	abstract = {Driven by the need for more efficient and seamless integration of physical models and data, physics-informed neural networks (PINNs) have seen a surge of interest in recent years. However, ensuring the reliability of their convergence and accuracy remains a challenge. In this work, we propose an efficient, gradient-less weighting scheme for PINNs that accelerates the convergence of dynamic or static systems. This simple yet effective attention mechanism is a bounded function of the evolving cumulative residuals and aims to make the optimizer aware of problematic regions at no extra computational cost or adversarial learning. We illustrate that this general method consistently achieves one order of magnitude faster convergence than vanilla PINNs and a minimum relative L2 error of O(10−5), on typical benchmarks of the literature. The method is further tested on the inverse solution of the Navier–Stokes within the brain perivascular spaces, where it considerably improves the prediction accuracy. Furthermore, an ablation study is performed for each case to identify the contribution of the components that enhance the vanilla PINN formulation. Evident from the convergence trajectories is the ability of the optimizer to effectively escape from poor local minima or saddle points while focusing on the challenging domain regions, which consistently have a high residual score. We believe that alongside exact boundary conditions and other model reparameterizations, this type of attention mask could be an essential element for fast training of both PINNs and neural operators.},
	urldate = {2025-05-29},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Anagnostopoulos, Sokratis J. and Toscano, Juan Diego and Stergiopulos, Nikolaos and Karniadakis, George Em},
	month = mar,
	year = {2024},
	keywords = {Adaptive weights, Fast convergence, PINNs accuracy, Residual-based attention},
	pages = {116805},
}

@misc{dabholkar_finding_2025,
	title = {Finding separatrices of dynamical flows with {Deep} {Koopman} {Eigenfunctions}},
	url = {http://arxiv.org/abs/2505.15231},
	doi = {10.48550/arXiv.2505.15231},
	abstract = {Many natural systems, including neural circuits involved in decision making, can be modeled as high-dimensional dynamical systems with multiple stable states. While existing analytical tools primarily describe behavior near stable equilibria, characterizing separatrices -- the manifolds that delineate boundaries between different basins of attraction -- remains challenging, particularly in high-dimensional settings. Here, we introduce a numerical framework leveraging Koopman Theory combined with Deep Neural Networks to effectively characterize separatrices. Specifically, we approximate Koopman Eigenfunctions (KEFs) associated with real positive eigenvalues, which vanish precisely at the separatrices. Utilizing these scalar KEFs, optimization methods efficiently locate separatrices even in complex systems. We demonstrate our approach on synthetic benchmarks, ecological network models, and recurrent neural networks trained on neuroscience-inspired tasks. Moreover, we illustrate the practical utility of our method by designing optimal perturbations that can shift systems across separatrices, enabling predictions relevant to optogenetic stimulation experiments in neuroscience.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Dabholkar, Kabir V. and Barak, Omri},
	month = may,
	year = {2025},
	note = {arXiv:2505.15231 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{mezic_koopman_2021,
	title = {Koopman {Operator}, {Geometry}, and {Learning} of {Dynamical} {Systems}},
	volume = {68},
	issn = {0002-9920, 1088-9477},
	url = {https://www.ams.org/notices/202107/rnoti-p1087.pdf},
	doi = {10.1090/noti2306},
	language = {en},
	number = {07},
	urldate = {2025-05-25},
	journal = {Notices of the American Mathematical Society},
	author = {Mezić, Igor},
	month = aug,
	year = {2021},
	pages = {1},
}

@inproceedings{mezic_applications_2015,
	title = {On applications of the spectral theory of the {Koopman} operator in dynamical systems and control theory},
	url = {https://ieeexplore.ieee.org/document/7403328/},
	doi = {10.1109/CDC.2015.7403328},
	abstract = {Recent contributions have extended the applicability of Koopman operator theory from dynamical systems to control. Stability theory got reformulated in terms of spectral properties of the Koopman operator [1], providing a nice link between the way we treat linear systems and nonlinear systems and opening the door for the use of classical linear e.g. pole placement theory in the fully nonlinear setting. New concepts such as isostables proved useful in the context of optimal control. Here, using Kato Decomposition we develop Koopman expansion for general LTI systems. We also interpret stable and unstable subspaces in terms of zero level sets of Koopman eigenfunctions. We then utilize conjugacy properties of Koopman eigenfunctions to extend these results to globally stable systems. In conclusion, we discuss how the classical Hamilton-Jacobi-Bellman setting for optimal control can be reformulated in operator-theoretic terms and point the applicability of spectral operator theory in max-plus algebra to it. Geometric theories such as differential positivity have been also related to spectral theories of the Koopman operator [2], in cases when the attractor is a fixed point or a limit cycle, pointing the way to the more general case of quasiperiodic and chaotic attractors.},
	urldate = {2025-05-25},
	booktitle = {2015 54th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Mezić, Igor},
	month = dec,
	year = {2015},
	keywords = {Eigenvalues and eigenfunctions, Level set, Linear systems, Matrix decomposition, Optimal control, Trajectory},
	pages = {7034--7041},
}

@article{kaiser_data-driven_2021,
	title = {Data-driven discovery of {Koopman} eigenfunctions for control},
	volume = {2},
	issn = {2632-2153},
	url = {https://dx.doi.org/10.1088/2632-2153/abf0f5},
	doi = {10.1088/2632-2153/abf0f5},
	abstract = {Data-driven transformations that reformulate nonlinear systems in a linear framework have the potential to enable the prediction, estimation, and control of strongly nonlinear dynamics using linear systems theory. The Koopman operator has emerged as a principled linear embedding of nonlinear dynamics, and its eigenfunctions establish intrinsic coordinates along which the dynamics behave linearly. Previous studies have used finite-dimensional approximations of the Koopman operator for model-predictive control approaches. In this work, we illustrate a fundamental closure issue of this approach and argue that it is beneficial to first validate eigenfunctions and then construct reduced-order models in these validated eigenfunctions. These coordinates form a Koopman-invariant subspace by design and, thus, have improved predictive power. We show then how the control can be formulated directly in these intrinsic coordinates and discuss potential benefits and caveats of this perspective. The resulting control architecture is termed Koopman Reduced Order Nonlinear Identification and Control (KRONIC). It is further demonstrated that these eigenfunctions can be approximated with data-driven regression and power series expansions, based on the partial differential equation governing the infinitesimal generator of the Koopman operator. Validating discovered eigenfunctions is crucial and we show that lightly damped eigenfunctions may be faithfully extracted from EDMD or an implicit formulation. These lightly damped eigenfunctions are particularly relevant for control, as they correspond to nearly conserved quantities that are associated with persistent dynamics, such as the Hamiltonian. KRONIC is then demonstrated on a number of relevant examples, including (a) a nonlinear system with a known linear embedding, (b) a variety of Hamiltonian systems, and (c) a high-dimensional double-gyre model for ocean mixing.},
	language = {en},
	number = {3},
	urldate = {2025-05-22},
	journal = {Machine Learning: Science and Technology},
	author = {Kaiser, Eurika and Kutz, J Nathan and Brunton, Steven L},
	month = jun,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {035023},
}

@misc{versteeg_computation-through-dynamics_2025,
	title = {Computation-through-{Dynamics} {Benchmark}: {Simulated} datasets and quality metrics for dynamical models of neural activity},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Computation-through-{Dynamics} {Benchmark}},
	url = {https://www.biorxiv.org/content/10.1101/2025.02.07.637062v2},
	doi = {10.1101/2025.02.07.637062},
	abstract = {A primary goal of systems neuroscience is to discover how ensembles of neurons transform inputs into goal-directed behavior, a process known as neural computation. A powerful framework for understanding neural computation uses neural dynamics – the rules that describe the temporal evolution of neural activity – to explain how goal-directed input-output transformations occur. As dynamical rules are not directly observable, we need computational models that can infer neural dynamics from recorded neural activity. We typically validate such models using synthetic datasets with known ground-truth dynamics, but unfortunately existing synthetic datasets don’t reflect fundamental features of neural computation and are thus poor proxies for neural systems. Further, the field lacks validated metrics for quantifying the accuracy of the dynamics inferred by models. The Computation-through-Dynamics Benchmark (CtDB) fills these critical gaps by providing: 1) synthetic datasets that reflect computational properties of biological neural circuits, 2) interpretable metrics for quantifying model performance, and 3) a standardized pipeline for training and evaluating models with or without known external inputs. In this manuscript, we demonstrate how CtDB can help guide the development, tuning, and troubleshooting of neural dynamics models. In summary, CtDB provides a critical platform for model developers to better understand and characterize neural computation through the lens of dynamics.},
	language = {en},
	urldate = {2025-05-22},
	publisher = {bioRxiv},
	author = {Versteeg, Christopher and McCart, Jonathan D. and Ostrow, Mitchell and Zoltowski, David M. and Washington, Clayton B. and Driscoll, Laura and Codol, Olivier and Michaels, Jonathan A. and Linderman, Scott W. and Sussillo, David and Pandarinath, Chethan},
	month = feb,
	year = {2025},
	note = {Pages: 2025.02.07.637062
Section: New Results},
}

@inproceedings{meng_koopman-based_2024,
	title = {Koopman-{Based} {Learning} of {Infinitesimal} {Generators} without {Operator} {Logarithm}},
	url = {https://ieeexplore.ieee.org/document/10886084},
	doi = {10.1109/CDC56724.2024.10886084},
	abstract = {To retrieve transient transition information of unknown systems from discrete-time observations, the Koopman operator structure has gained significant attention in recent years, particularly for its ability to avoid time derivatives through the Koopman operator logarithm. However, the effectiveness of these logarithm-based methods has only been demonstrated within a restrictive function space. In this paper, we propose a logarithm-free technique for learning the infinitesimal generator without disrupting the Koopman operator learning framework.},
	urldate = {2025-05-20},
	booktitle = {2024 {IEEE} 63rd {Conference} on {Decision} and {Control} ({CDC})},
	author = {Meng, Yiming and Zhou, Ruikun and Ornik, Melkior and Liu, Jun},
	month = dec,
	year = {2024},
	note = {ISSN: 2576-2370},
	keywords = {Accuracy, Convergence, Dictionaries, Generators, Koopman operators, Numerical simulation, System identification, Transient analysis, Tuning, Unknown nonlinear systems, infinitesimal generator, system identification, verification},
	pages = {8302--8307},
}

@article{brunton_notes_2019,
	title = {Notes on {Koopman} {Operator} {Theory}},
	url = {https://fluids.ac.uk/files/meetings/KoopmanNotes.1575558616.pdf},
	language = {en},
	author = {Brunton, Steven L},
	year = {2019},
}

@article{brunton_koopman_2016,
	title = {Koopman {Invariant} {Subspaces} and {Finite} {Linear} {Representations} of {Nonlinear} {Dynamical} {Systems} for {Control}},
	volume = {11},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0150171},
	doi = {10.1371/journal.pone.0150171},
	abstract = {In this work, we explore finite-dimensional linear representations of nonlinear dynamical systems by restricting the Koopman operator to an invariant subspace spanned by specially chosen observable functions. The Koopman operator is an infinite-dimensional linear operator that evolves functions of the state of a dynamical system. Dominant terms in the Koopman expansion are typically computed using dynamic mode decomposition (DMD). DMD uses linear measurements of the state variables, and it has recently been shown that this may be too restrictive for nonlinear systems. Choosing the right nonlinear observable functions to form an invariant subspace where it is possible to obtain linear reduced-order models, especially those that are useful for control, is an open challenge. Here, we investigate the choice of observable functions for Koopman analysis that enable the use of optimal linear control techniques on nonlinear problems. First, to include a cost on the state of the system, as in linear quadratic regulator (LQR) control, it is helpful to include these states in the observable subspace, as in DMD. However, we find that this is only possible when there is a single isolated fixed point, as systems with multiple fixed points or more complicated attractors are not globally topologically conjugate to a finite-dimensional linear system, and cannot be represented by a finite-dimensional linear Koopman subspace that includes the state. We then present a data-driven strategy to identify relevant observable functions for Koopman analysis by leveraging a new algorithm to determine relevant terms in a dynamical system by ℓ1-regularized regression of the data in a nonlinear function space; we also show how this algorithm is related to DMD. Finally, we demonstrate the usefulness of nonlinear observable subspaces in the design of Koopman operator optimal control laws for fully nonlinear systems using techniques from linear optimal control.},
	language = {en},
	number = {2},
	urldate = {2025-05-16},
	journal = {PLOS ONE},
	author = {Brunton, Steven L. and Brunton, Bingni W. and Proctor, Joshua L. and Kutz, J. Nathan},
	month = feb,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Dynamical systems, Eigenvalues, Eigenvectors, Fluid dynamics, Nonlinear dynamics, Nonlinear systems, Polynomials},
	pages = {e0150171},
}

@article{tu_dynamic_2014,
	title = {On dynamic mode decomposition: {Theory} and applications},
	volume = {1},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2158-2491},
	shorttitle = {On dynamic mode decomposition},
	url = {https://www.aimsciences.org/en/article/doi/10.3934/jcd.2014.1.391},
	doi = {10.3934/jcd.2014.1.391},
	abstract = {Originally introduced in the fluid mechanics community, dynamic mode decomposition (DMD) has emerged as a powerful tool for analyzing the dynamics of nonlinear systems.    However, existing DMD theory deals primarily with sequential time series for which the measurement dimension is much larger than the number of measurements taken.    We present a theoretical framework in which we define DMD as the eigendecomposition of an approximating linear operator.    This generalizes DMD to a larger class of datasets, including nonsequential time series.    We demonstrate the utility of this approach by presenting novel sampling strategies that increase computational efficiency and mitigate the effects of noise, respectively.    We also introduce the concept of linear consistency, which helps explain the potential pitfalls of applying DMD to rank-deficient datasets, illustrating with examples.    Such computations are not considered in the existing literature but can be understood using our more general framework.    In addition, we show that our theory strengthens the connections between DMD and Koopman operator theory.    It also establishes connections between DMD and other techniques, including the eigensystem realization algorithm (ERA), a system identification method, and linear inverse modeling (LIM), a method from climate science.    We show that under certain conditions, DMD is equivalent to LIM.},
	language = {en},
	number = {2},
	urldate = {2025-05-16},
	journal = {Journal of Computational Dynamics},
	author = {Tu, Jonathan H. and Rowley, Clarence W. and Luchtenburg, Dirk M. and Brunton, Steven L. and Kutz, J. Nathan},
	month = dec,
	year = {2014},
	note = {Publisher: Journal of Computational Dynamics},
	pages = {391--421},
}

@article{schmid_dynamic_2010,
	title = {Dynamic mode decomposition of numerical and experimental data},
	volume = {656},
	issn = {1469-7645, 0022-1120},
	url = {https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/abs/dynamic-mode-decomposition-of-numerical-and-experimental-data/AA4C763B525515AD4521A6CC5E10DBD4},
	doi = {10.1017/S0022112010001217},
	abstract = {The description of coherent features of fluid flow is essential to our understanding of fluid-dynamical and transport processes. A method is introduced that is able to extract dynamic information from flow fields that are either generated by a (direct) numerical simulation or visualized/measured in a physical experiment. The extracted dynamic modes, which can be interpreted as a generalization of global stability modes, can be used to describe the underlying physical mechanisms captured in the data sequence or to project large-scale problems onto a dynamical system of significantly fewer degrees of freedom. The concentration on subdomains of the flow field where relevant dynamics is expected allows the dissection of a complex flow into regions of localized instability phenomena and further illustrates the flexibility of the method, as does the description of the dynamics within a spatial framework. Demonstrations of the method are presented consisting of a plane channel flow, flow over a two-dimensional cavity, wake flow behind a flexible membrane and a jet passing between two cylinders.},
	language = {en},
	urldate = {2025-05-16},
	journal = {Journal of Fluid Mechanics},
	author = {Schmid, Peter J.},
	month = aug,
	year = {2010},
	pages = {5--28},
}

@inproceedings{noauthor_aps_nodate,
	title = {{APS} -61st {Annual} {Meeting} of the {APS} {Division} of {Fluid} {Dynamics} - {Event} - {Dynamic} {Mode} {Decomposition} of numerical and experimental data},
	volume = {Volume 53, Number 15},
	url = {https://meetings.aps.org/Meeting/DFD08/Event/91003},
	urldate = {2025-05-16},
	booktitle = {Bulletin of the {American} {Physical} {Society}},
	publisher = {American Physical Society},
}

@incollection{powell_radial_1987,
	address = {USA},
	title = {Radial basis functions for multivariable interpolation: a review},
	isbn = {978-0-19-853612-3},
	shorttitle = {Radial basis functions for multivariable interpolation},
	urldate = {2025-05-16},
	booktitle = {Algorithms for approximation},
	publisher = {Clarendon Press},
	author = {Powell, M. J. D.},
	month = jan,
	year = {1987},
	pages = {143--167},
}

@article{orr_centre_nodate,
	title = {Centre for {Cognitive} {Science}, {University} of {Edinburgh}, 2, {Buccleuch} {Place}, {Edinburgh} {EH8} {9LW}, {Scotland} {April} 1996},
	abstract = {This document is an introduction to radial basis function (RBF) networks, a type of arti cial neural network for application to problems of supervised learning (e.g. regression, classi cation and time series prediction). It is now only available in PostScript2 (an older and now unsupported hyper-text version3 may be available for a while longer).},
	language = {en},
	author = {Orr, Mark J L},
}

@article{vinograd_causal_2024,
	title = {Causal evidence of a line attractor encoding an affective state},
	volume = {634},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07915-x},
	doi = {10.1038/s41586-024-07915-x},
	abstract = {Continuous attractors are an emergent property of neural population dynamics that have been hypothesized to encode continuous variables such as head direction and eye position1–4. In mammals, direct evidence of neural implementation of a continuous attractor has been hindered by the challenge of targeting perturbations to specific neurons within contributing ensembles2,3. Dynamical systems modelling has revealed that neurons in the hypothalamus exhibit approximate line-attractor dynamics in male mice during aggressive encounters5. We have previously hypothesized that these dynamics may encode the variable intensity and persistence of an aggressive internal state. Here we report that these neurons also showed line-attractor dynamics in head-fixed mice observing aggression6. This allowed us to identify and manipulate line-attractor-contributing neurons using two-photon calcium imaging and holographic optogenetic perturbations. On-manifold perturbations yielded integration of optogenetic stimulation pulses and persistent activity that drove the system along the line attractor, while transient off-manifold perturbations were followed by rapid relaxation back into the attractor. Furthermore, single-cell stimulation and imaging revealed selective functional connectivity among attractor-contributing neurons. Notably, individual differences among mice in line-attractor stability were correlated with the degree of functional connectivity among attractor-contributing neurons. Mechanistic recurrent neural network modelling indicated that dense subnetwork connectivity and slow neurotransmission7 best recapitulate our empirical findings. Our work bridges circuit and manifold levels3, providing causal evidence of continuous attractor dynamics encoding an affective internal state in the mammalian hypothalamus.},
	language = {en},
	number = {8035},
	urldate = {2025-05-16},
	journal = {Nature},
	author = {Vinograd, Amit and Nair, Aditya and Kim, Joseph H. and Linderman, Scott W. and Anderson, David J.},
	month = oct,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Aggression, Ca2+ imaging, Dynamical systems, Multiphoton microscopy, Neural circuits},
	pages = {910--918},
}

@article{mezic_spectral_2005,
	title = {Spectral {Properties} of {Dynamical} {Systems}, {Model} {Reduction} and {Decompositions}},
	volume = {41},
	issn = {1573-269X},
	url = {https://doi.org/10.1007/s11071-005-2824-x},
	doi = {10.1007/s11071-005-2824-x},
	abstract = {In this paper we discuss two issues related to model reduction of deterministic or stochastic processes. The first is the relationship of the spectral properties of the dynamics on the attractor of the original, high-dimensional dynamical system with the properties and possibilities for model reduction. We review some elements of the spectral theory of dynamical systems. We apply this theory to obtain a decomposition of the process that utilizes spectral properties of the linear Koopman operator associated with the asymptotic dynamics on the attractor. This allows us to extract the almost periodic part of the evolving process. The remainder of the process has continuous spectrum. The second topic we discuss is that of model validation, where the original, possibly high-dimensional dynamics and the dynamics of the reduced model – that can be deterministic or stochastic – are compared in some norm. Using the “statistical Takens theorem” proven in (Mezić, I. and Banaszuk, A. Physica D, 2004) we argue that comparison of average energy contained in the finite-dimensional projection is one in the hierarchy of functionals of the field that need to be checked in order to assess the accuracy of the projection.},
	language = {en},
	number = {1},
	urldate = {2025-05-16},
	journal = {Nonlinear Dynamics},
	author = {Mezić, Igor},
	month = aug,
	year = {2005},
	keywords = {Applied Dynamical Systems, Diffusion  Processes and Stochastic Analysis on  Manifolds, Dynamical Systems, Ergodic Theory, Model Theory, Nonlinear Dynamics and Chaos Theory, model reduction, spectral theory of dynamical systems},
	pages = {309--325},
}

@article{mezic_koopman_2016,
	series = {10th {IFAC} {Symposium} on {Nonlinear} {Control} {Systems} {NOLCOS} 2016},
	title = {Koopman {Mode} {Decomposition} for {Periodic}/{Quasi}-periodic {Time} {Dependence}*},
	volume = {49},
	issn = {2405-8963},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896316318262},
	doi = {10.1016/j.ifacol.2016.10.246},
	abstract = {In this paper we propose an extension of Koopman operator framework for non-autonomous systems with periodic and quasi-periodic time dependence. Using a time parametrized family of Koopman operators and the associated time dependent eigenvalues and eigenfunctions, and concepts from Floquet theory, we extend the notion of the Koopman Mode Decomposition. We illustrate our framework on several examples.},
	number = {18},
	urldate = {2025-05-16},
	journal = {IFAC-PapersOnLine},
	author = {Mezic, Igor and Surana, Amit},
	month = jan,
	year = {2016},
	keywords = {Non-autonomous Systems, Nonlinear Dynamics, Operator Theory},
	pages = {690--697},
}

@article{lan_linearization_2013,
	title = {Linearization in the large of nonlinear systems and {Koopman} operator spectrum},
	volume = {242},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01672789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167278912002102},
	doi = {10.1016/j.physd.2012.08.017},
	abstract = {According to the Hartman–Grobman Theorem, a nonlinear system can be linearized in a neighborhood of a hyperbolic stationary point. Here, we extend this linearization around stable (unstable) equilibria or periodic orbits to the whole basin of attraction, for both discrete diffeomorphisms and flows. We discuss the connection of the linearizing transformation to the spectrum of Koopman operator.},
	language = {en},
	number = {1},
	urldate = {2025-05-16},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Lan, Yueheng and Mezić, Igor},
	month = jan,
	year = {2013},
	pages = {42--53},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2025-05-15},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
}

@article{low_remapping_2023,
	title = {Remapping in a recurrent neural network model of navigation and context inference},
	volume = {12},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.86943},
	doi = {10.7554/eLife.86943},
	abstract = {Neurons in navigational brain regions provide information about position, orientation, and speed relative to environmental landmarks. These cells also change their firing patterns (‘remap’) in response to changing contextual factors such as environmental cues, task conditions, and behavioral states, which influence neural activity throughout the brain. How can navigational circuits preserve their local computations while responding to global context changes? To investigate this question, we trained recurrent neural network models to track position in simple environments while at the same time reporting transiently-cued context changes. We show that these combined task constraints (navigation and context inference) produce activity patterns that are qualitatively similar to population-wide remapping in the entorhinal cortex, a navigational brain region. Furthermore, the models identify a solution that generalizes to more complex navigation and inference tasks. We thus provide a simple, general, and experimentally-grounded model of remapping as one neural circuit performing both navigation and context inference.},
	urldate = {2025-05-15},
	journal = {eLife},
	author = {Low, Isabel IC and Giocomo, Lisa M and Williams, Alex H},
	editor = {Ostojic, Srdjan and Colgin, Laura L},
	month = jul,
	year = {2023},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {Recurrent neural network models, attractor manifolds, dynamic coding, latent state, medial entorhinal cortex, navigation},
	pages = {RP86943},
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities.},
	volume = {79},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554},
	doi = {10.1073/pnas.79.8.2554},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	number = {8},
	urldate = {2025-05-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hopfield, J J},
	month = apr,
	year = {1982},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {2554--2558},
}

@misc{noauthor_flexible_nodate,
	title = {Flexible {Control} of {Mutual} {Inhibition}: {A} {Neural} {Model} of {Two}-{Interval} {Discrimination}},
	shorttitle = {Flexible {Control} of {Mutual} {Inhibition}},
	url = {https://www.science.org/doi/10.1126/science.1104171},
	language = {en},
	urldate = {2025-05-15},
	doi = {10.1126/science.1104171},
}

@article{machens_flexible_2005,
	title = {Flexible {Control} of {Mutual} {Inhibition}: {A} {Neural} {Model} of {Two}-{Interval} {Discrimination}},
	volume = {307},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Flexible {Control} of {Mutual} {Inhibition}},
	url = {https://www.science.org/doi/10.1126/science.1104171},
	doi = {10.1126/science.1104171},
	abstract = {Networks adapt to environmental demands by switching between distinct dynamical behaviors. The activity of frontal-lobe neurons during two-interval discrimination tasks is an example of these adaptable dynamics. Subjects first perceive a stimulus, then hold it in working memory, and finally make a decision by comparing it with a second stimulus. We present a simple mutual-inhibition network model that captures all three task phases within a single framework. The model integrates both working memory and decision making because its dynamical properties are easily controlled without changing its connectivity. Mutual inhibition between nonlinear units is a useful design motif for networks that must display multiple behaviors.},
	language = {en},
	number = {5712},
	urldate = {2025-05-15},
	journal = {Science},
	author = {Machens, Christian K. and Romo, Ranulfo and Brody, Carlos D.},
	month = feb,
	year = {2005},
	pages = {1121--1124},
}

@article{vyas_computation_2020-1,
	title = {Computation {Through} {Neural} {Population} {Dynamics}},
	volume = {43},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-neuro-092619-094115},
	doi = {10.1146/annurev-neuro-092619-094115},
	abstract = {Significant experimental, computational, and theoretical work has identified rich structure within the coordinated activity of interconnected neural populations. An emerging challenge now is to uncover the nature of the associated computations, how they are implemented, and what role they play in driving behavior. We term this computation through neural population dynamics. If successful, this framework will reveal general motifs of neural population activity and quantitatively describe how neural population dynamics implement computations necessary for driving goal-directed behavior. Here, we start with a mathematical primer on dynamical systems theory and analytical tools necessary to apply this perspective to experimental data. Next, we highlight some recent discoveries resulting from successful application of dynamical systems. We focus on studies spanning motor control, timing, decision-making, and working memory. Finally, we briefly discuss promising recent lines of investigation and future directions for the computation through neural population dynamics framework.},
	language = {en},
	number = {Volume 43, 2020},
	urldate = {2025-05-15},
	journal = {Annual Review of Neuroscience},
	author = {Vyas, Saurabh and Golub, Matthew D. and Sussillo, David and Shenoy, Krishna V.},
	month = jul,
	year = {2020},
	note = {Publisher: Annual Reviews},
	pages = {249--275},
}

@article{barak_recurrent_2017,
	series = {Computational {Neuroscience}},
	title = {Recurrent neural networks as versatile tools of neuroscience research},
	volume = {46},
	issn = {0959-4388},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438817300429},
	doi = {10.1016/j.conb.2017.06.003},
	abstract = {Recurrent neural networks (RNNs) are a class of computational models that are often used as a tool to explain neurobiological phenomena, considering anatomical, electrophysiological and computational constraints. RNNs can either be designed to implement a certain dynamical principle, or they can be trained by input–output examples. Recently, there has been large progress in utilizing trained RNNs both for computational tasks, and as explanations of neural phenomena. I will review how combining trained RNNs with reverse engineering can provide an alternative framework for modeling in neuroscience, potentially serving as a powerful hypothesis generation tool. Despite the recent progress and potential benefits, there are many fundamental gaps towards a theory of these networks. I will discuss these challenges and possible methods to attack them.},
	urldate = {2025-05-15},
	journal = {Current Opinion in Neurobiology},
	author = {Barak, Omri},
	month = oct,
	year = {2017},
	pages = {1--6},
}

@article{liu_encoding_2024,
	title = {Encoding of female mating dynamics by a hypothalamic line attractor},
	volume = {634},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07916-w},
	doi = {10.1038/s41586-024-07916-w},
	abstract = {Females exhibit complex, dynamic behaviours during mating with variable sexual receptivity depending on hormonal status1–4. However, how their brains encode the dynamics of mating and receptivity remains largely unknown. The ventromedial hypothalamus, ventrolateral subdivision contains oestrogen receptor type 1-positive neurons that control mating receptivity in female mice5,6. Here, unsupervised dynamical system analysis of calcium imaging data from these neurons during mating uncovered a dimension with slow ramping activity, generating a line attractor in neural state space. Neural perturbations in behaving females demonstrated relaxation of population activity back into the attractor. During mating, population activity integrated male cues to ramp up along this attractor, peaking just before ejaculation. Activity in the attractor dimension was positively correlated with the degree of receptivity. Longitudinal imaging revealed that attractor dynamics appear and disappear across the oestrus cycle and are hormone dependent. These observations suggest that a hypothalamic line attractor encodes a persistent, escalating state of female sexual arousal or drive during mating. They also demonstrate that attractors can be reversibly modulated by hormonal status, on a timescale of days.},
	language = {en},
	number = {8035},
	urldate = {2025-05-15},
	journal = {Nature},
	author = {Liu, Mengyu and Nair, Aditya and Coria, Nestor and Linderman, Scott W. and Anderson, David J.},
	month = oct,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Dynamical systems, Sexual behaviour},
	pages = {901--909},
}

@article{driscoll_flexible_2024,
	title = {Flexible multitask computation in recurrent networks utilizes shared dynamical motifs},
	volume = {27},
	copyright = {2024 The Author(s)},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-024-01668-6},
	doi = {10.1038/s41593-024-01668-6},
	abstract = {Flexible computation is a hallmark of intelligent behavior. However, little is known about how neural networks contextually reconfigure for different computations. In the present work, we identified an algorithmic neural substrate for modular computation through the study of multitasking artificial recurrent neural networks. Dynamical systems analyses revealed learned computational strategies mirroring the modular subtask structure of the training task set. Dynamical motifs, which are recurring patterns of neural activity that implement specific computations through dynamics, such as attractors, decision boundaries and rotations, were reused across tasks. For example, tasks requiring memory of a continuous circular variable repurposed the same ring attractor. We showed that dynamical motifs were implemented by clusters of units when the unit activation function was restricted to be positive. Cluster lesions caused modular performance deficits. Motifs were reconfigured for fast transfer learning after an initial phase of learning. This work establishes dynamical motifs as a fundamental unit of compositional computation, intermediate between neuron and network. As whole-brain studies simultaneously record activity from multiple specialized systems, the dynamical motif framework will guide questions about specialization and generalization.},
	language = {en},
	number = {7},
	urldate = {2025-05-15},
	journal = {Nature Neuroscience},
	author = {Driscoll, Laura N. and Shenoy, Krishna and Sussillo, David},
	month = jul,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Decision, Dynamical systems},
	pages = {1349--1363},
}

@article{mante_context-dependent_2013,
	title = {Context-dependent computation by recurrent dynamics in prefrontal cortex},
	volume = {503},
	copyright = {2013 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature12742},
	doi = {10.1038/nature12742},
	abstract = {Prefrontal cortex is thought to have a fundamental role in flexible, context-dependent behaviour, but the exact nature of the computations underlying this role remains largely unknown. In particular, individual prefrontal neurons often generate remarkably complex responses that defy deep understanding of their contribution to behaviour. Here we study prefrontal cortex activity in macaque monkeys trained to flexibly select and integrate noisy sensory inputs towards a choice. We find that the observed complexity and functional roles of single neurons are readily understood in the framework of a dynamical process unfolding at the level of the population. The population dynamics can be reproduced by a trained recurrent neural network, which suggests a previously unknown mechanism for selection and integration of task-relevant inputs. This mechanism indicates that selection and integration are two aspects of a single dynamical process unfolding within the same prefrontal circuits, and potentially provides a novel, general framework for understanding context-dependent computations.},
	language = {en},
	number = {7474},
	urldate = {2025-05-15},
	journal = {Nature},
	author = {Mante, Valerio and Sussillo, David and Shenoy, Krishna V. and Newsome, William T.},
	month = nov,
	year = {2013},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cognitive neuroscience},
	pages = {78--84},
}

@article{carnevale_dynamic_2015,
	title = {Dynamic {Control} of {Response} {Criterion} in {Premotor} {Cortex} during {Perceptual} {Detection} under {Temporal} {Uncertainty}},
	volume = {86},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627315003645},
	doi = {10.1016/j.neuron.2015.04.014},
	abstract = {Under uncertainty, the brain uses previous knowledge to transform sensory inputs into the percepts on which decisions are based. When the uncertainty lies in the timing of sensory evidence, however, the mechanism underlying the use of previously acquired temporal information remains unknown. We study this issue in monkeys performing a detection task with variable stimulation times. We use the neural correlates of false alarms to infer the subject’s response criterion and find that it modulates over the course of a trial. Analysis of premotor cortex activity shows that this modulation is represented by the dynamics of population responses. A trained recurrent network model reproduces the experimental findings and demonstrates a neural mechanism to benefit from temporal expectations in perceptual detection. Previous knowledge about the probability of stimulation over time can be intrinsically encoded in the neural population dynamics, allowing a flexible control of the response criterion over time.},
	number = {4},
	urldate = {2025-05-15},
	journal = {Neuron},
	author = {Carnevale, Federico and de Lafuente, Victor and Romo, Ranulfo and Barak, Omri and Parga, Néstor},
	month = may,
	year = {2015},
	pages = {1067--1077},
}

@inproceedings{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {https://openreview.net/forum?id=Bkg6RiCqY7},
	urldate = {2025-05-14},
	booktitle = {7th {International} {Conference} on {Learning} {Representations}, {ICLR} 2019, {New} {Orleans}, {LA}, {USA}, {May} 6-9, 2019},
	publisher = {OpenReview.net},
	author = {Loshchilov, Ilya and Hutter, Frank},
	year = {2019},
}

@misc{kingma_adam_2017-1,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{stein_ecological_2013,
	title = {Ecological {Modeling} from {Time}-{Series} {Inference}: {Insight} into {Dynamics} and {Stability} of {Intestinal} {Microbiota}},
	volume = {9},
	issn = {1553-7358},
	shorttitle = {Ecological {Modeling} from {Time}-{Series} {Inference}},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003388},
	doi = {10.1371/journal.pcbi.1003388},
	abstract = {The intestinal microbiota is a microbial ecosystem of crucial importance to human health. Understanding how the microbiota confers resistance against enteric pathogens and how antibiotics disrupt that resistance is key to the prevention and cure of intestinal infections. We present a novel method to infer microbial community ecology directly from time-resolved metagenomics. This method extends generalized Lotka–Volterra dynamics to account for external perturbations. Data from recent experiments on antibiotic-mediated Clostridium difficile infection is analyzed to quantify microbial interactions, commensal-pathogen interactions, and the effect of the antibiotic on the community. Stability analysis reveals that the microbiota is intrinsically stable, explaining how antibiotic perturbations and C. difficile inoculation can produce catastrophic shifts that persist even after removal of the perturbations. Importantly, the analysis suggests a subnetwork of bacterial groups implicated in protection against C. difficile. Due to its generality, our method can be applied to any high-resolution ecological time-series data to infer community structure and response to external stimuli.},
	language = {en},
	number = {12},
	urldate = {2025-05-14},
	journal = {PLOS Computational Biology},
	author = {Stein, Richard R. and Bucci, Vanni and Toussaint, Nora C. and Buffie, Charlie G. and Rätsch, Gunnar and Pamer, Eric G. and Sander, Chris and Xavier, João B.},
	month = dec,
	year = {2013},
	note = {Publisher: Public Library of Science},
	keywords = {Antibiotics, Clindamycin, Clostridium difficile, Community ecology, Gastrointestinal tract, Gut bacteria, Microbial ecosystems, Species interactions},
	pages = {e1003388},
}

@misc{chung_empirical_2014,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1412.3555},
	doi = {10.48550/arXiv.1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
	urldate = {2025-05-13},
	publisher = {arXiv},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	month = dec,
	year = {2014},
	note = {arXiv:1412.3555 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{jones_steady-state_2019,
	title = {Steady-state reduction of generalized {Lotka}-{Volterra} systems in the microbiome},
	volume = {99},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.99.032403},
	doi = {10.1103/PhysRevE.99.032403},
	abstract = {The generalized Lotka-Volterra (gLV) equations, a classic model from theoretical ecology, describe the population dynamics of a set of interacting species. As the number of species in these systems grow in number, their dynamics become increasingly complex and intractable. We introduce steady-state reduction (SSR), a method that reduces a gLV system of many ecological species into two-dimensional subsystems that each obey gLV dynamics and whose basis vectors are steady states of the high-dimensional model. We apply this method to an experimentally-derived model of the gut microbiome in order to observe the transition between “healthy” and “diseased” microbial states. Specifically, we use SSR to investigate how fecal microbiota transplantation, a promising clinical treatment for dysbiosis, can revert a diseased microbial state to health.},
	number = {3},
	urldate = {2025-05-06},
	journal = {Physical Review E},
	author = {Jones, Eric W. and Carlson, Jean M.},
	month = mar,
	year = {2019},
	note = {Publisher: American Physical Society},
	pages = {032403},
}

@article{mauroy_isostables_2013,
	title = {Isostables, isochrons, and {Koopman} spectrum for the action–angle representation of stable fixed point dynamics},
	volume = {261},
	issn = {0167-2789},
	url = {https://www.sciencedirect.com/science/article/pii/S0167278913001620},
	doi = {10.1016/j.physd.2013.06.004},
	abstract = {For asymptotically periodic systems, a powerful (phase) reduction of the dynamics is obtained by computing the so-called isochrons, i.e. the sets of points that converge toward the same trajectory on the limit cycle. Motivated by the analysis of excitable systems, a similar reduction has been attempted for non-periodic systems admitting a stable fixed point. In this case, the isochrons can still be defined but they do not capture the asymptotic behavior of the trajectories. Instead, the sets of interest–that we call “isostables”–are defined in the literature as the sets of points that converge toward the same trajectory on a stable slow manifold of the fixed point. However, it turns out that this definition of the isostables holds only for systems with slow–fast dynamics. Also, efficient methods for computing the isostables are missing. The present paper provides a general framework for the definition and the computation of the isostables of stable fixed points, which is based on the spectral properties of the so-called Koopman operator. More precisely, the isostables are defined as the level sets of a particular eigenfunction of the Koopman operator. Through this approach, the isostables are unique and well-defined objects related to the asymptotic properties of the system. Also, the framework reveals that the isostables and the isochrons are two different but complementary notions which define a set of action–angle coordinates for the dynamics. In addition, an efficient algorithm for computing the isostables is obtained, which relies on the evaluation of Laplace averages along the trajectories. The method is illustrated with the excitable FitzHugh–Nagumo model and with the Lorenz model. Finally, we discuss how these methods based on the Koopman operator framework relate to the global linearization of the system and to the derivation of special Lyapunov functions.},
	urldate = {2025-05-04},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Mauroy, A. and Mezić, I. and Moehlis, J.},
	month = oct,
	year = {2013},
	keywords = {Action–angle coordinates, Excitable systems, Isochrons, Koopman operator, Lyapunov function, Nonlinear dynamics},
	pages = {19--30},
}

@article{lusch_deep_2018,
	title = {Deep learning for universal linear embeddings of nonlinear dynamics},
	volume = {9},
	copyright = {2018 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-07210-0},
	doi = {10.1038/s41467-018-07210-0},
	abstract = {Identifying coordinate transformations that make strongly nonlinear dynamics approximately linear has the potential to enable nonlinear prediction, estimation, and control using linear theory. The Koopman operator is a leading data-driven embedding, and its eigenfunctions provide intrinsic coordinates that globally linearize the dynamics. However, identifying and representing these eigenfunctions has proven challenging. This work leverages deep learning to discover representations of Koopman eigenfunctions from data. Our network is parsimonious and interpretable by construction, embedding the dynamics on a low-dimensional manifold. We identify nonlinear coordinates on which the dynamics are globally linear using a modified auto-encoder. We also generalize Koopman representations to include a ubiquitous class of systems with continuous spectra. Our framework parametrizes the continuous frequency using an auxiliary network, enabling a compact and efficient embedding, while connecting our models to decades of asymptotics. Thus, we benefit from the power of deep learning, while retaining the physical interpretability of Koopman embeddings.},
	language = {en},
	number = {1},
	urldate = {2025-05-04},
	journal = {Nature Communications},
	author = {Lusch, Bethany and Kutz, J. Nathan and Brunton, Steven L.},
	month = nov,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Applied mathematics, Nonlinear phenomena},
	pages = {4950},
}

@article{koopman_hamiltonian_1931,
	title = {Hamiltonian {Systems} and {Transformation} in {Hilbert} {Space}},
	volume = {17},
	url = {https://www.pnas.org/doi/10.1073/pnas.17.5.315},
	doi = {10.1073/pnas.17.5.315},
	number = {5},
	urldate = {2025-05-04},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Koopman, B. O.},
	month = may,
	year = {1931},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {315--318},
}

@article{naiman_operator_2023,
	title = {An {Operator} {Theoretic} {Approach} for {Analyzing} {Sequence} {Neural} {Networks}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26111},
	doi = {10.1609/aaai.v37i8.26111},
	abstract = {Analyzing the inner mechanisms of deep neural networks is a fundamental task in machine learning. Existing work provides limited analysis or it depends on local theories, such as fixed-point analysis. In contrast, we propose to analyze trained neural networks using an operator theoretic approach which is rooted in Koopman theory, the Koopman Analysis of Neural Networks (KANN). Key to our method is the Koopman operator, which is a linear object that globally represents the dominant behavior of the network dynamics. The linearity of the Koopman operator facilitates analysis via its eigenvectors and eigenvalues. Our method reveals that the latter eigendecomposition holds semantic information related to the neural network inner workings. For instance,  the eigenvectors highlight positive and negative n-grams in the sentiments analysis task; similarly, the eigenvectors capture the salient features of healthy heart beat signals in the ECG classification problem.},
	language = {en},
	number = {8},
	urldate = {2025-05-04},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Naiman, Ilan and Azencot, Omri},
	month = jun,
	year = {2023},
	note = {Number: 8},
	keywords = {ML: Time-Series/Data Streams},
	pages = {9268--9276},
}

@article{redman_identifying_2024,
	title = {Identifying {Equivalent} {Training} {Dynamics}},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/2a07348a6a7b2c208ab5cb1ee0e78ab5-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-04},
	journal = {Advances in Neural Information Processing Systems},
	author = {Redman, William T. and Bello-Rivas, Juan and Fonoberova, Maria and Mohr, Ryan and Kevrekidis, Yannis G. and Mezić, Igor},
	month = dec,
	year = {2024},
	pages = {23603--23629},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	shorttitle = {Physics-informed neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	urldate = {2025-05-04},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	month = feb,
	year = {2019},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	pages = {686--707},
}

@article{sirignano_dgm_2018,
	title = {{DGM}: {A} deep learning algorithm for solving partial differential equations},
	volume = {375},
	issn = {0021-9991},
	shorttitle = {{DGM}},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118305527},
	doi = {10.1016/j.jcp.2018.08.029},
	abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve high-dimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the differential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton–Jacobi–Bellman PDE and Burgers' equation. The deep learning algorithm approximates the general solution to the Burgers' equation for a continuum of different boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method (DGM)” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.},
	urldate = {2025-05-04},
	journal = {Journal of Computational Physics},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	month = dec,
	year = {2018},
	keywords = {Deep learning, High-dimensional partial differential equations, Machine learning, Partial differential equations},
	pages = {1339--1364},
}

@article{sussillo_opening_2013,
	title = {Opening the {Black} {Box}: {Low}-{Dimensional} {Dynamics} in {High}-{Dimensional} {Recurrent} {Neural} {Networks}},
	volume = {25},
	issn = {0899-7667},
	shorttitle = {Opening the {Black} {Box}},
	url = {https://doi.org/10.1162/NECO_a_00409},
	doi = {10.1162/NECO_a_00409},
	abstract = {Recurrent neural networks (RNNs) are useful tools for learning nonlinear relationships between time-varying inputs and outputs with complex temporal dependencies. Recently developed algorithms have been successful at training RNNs to perform a wide variety of tasks, but the resulting networks have been treated as black boxes: their mechanism of operation remains unknown. Here we explore the hypothesis that fixed points, both stable and unstable, and the linearized dynamics around them, can reveal crucial aspects of how RNNs implement their computations. Further, we explore the utility of linearization in areas of phase space that are not true fixed points but merely points of very slow movement. We present a simple optimization technique that is applied to trained RNNs to find the fixed and slow points of their dynamics. Linearization around these slow regions can be used to explore, or reverse-engineer, the behavior of the RNN. We describe the technique, illustrate it using simple examples, and finally showcase it on three high-dimensional RNN examples: a 3-bit flip-flop device, an input-dependent sine wave generator, and a two-point moving average. In all cases, the mechanisms of trained networks could be inferred from the sets of fixed and slow points and the linearized dynamics around them.},
	number = {3},
	urldate = {2025-05-04},
	journal = {Neural Computation},
	author = {Sussillo, David and Barak, Omri},
	month = mar,
	year = {2013},
	pages = {626--649},
}

@inproceedings{yeung_learning_2019,
	title = {Learning {Deep} {Neural} {Network} {Representations} for {Koopman} {Operators} of {Nonlinear} {Dynamical} {Systems}},
	url = {https://ieeexplore.ieee.org/document/8815339},
	doi = {10.23919/ACC.2019.8815339},
	abstract = {The Koopman operator has recently garnered much attention for its value in dynamical systems analysis and data-driven model discovery. However, its application has been hindered by the computational complexity of extended dynamic mode decomposition; this requires a combinatorially large basis set to adequately describe many nonlinear systems of interest, e.g. cyber-physical infrastructure systems, biological networks, social systems, and fluid dynamics. Often the dictionaries generated for these problems are manually curated, requiring domain-specific knowledge and painstaking tuning. In this paper we introduce a computational framework for learning Koopman operators of nonlinear dynamical systems using deep learning. We show that this novel method automatically selects efficient deep dictionaries, requiring much lower dimensional dictionaries while outperforming state-of-the-art methods. We benchmark this method on partially observed nonlinear systems, including the glycolytic oscillator and show it is able to predict on test data quantitatively 100 steps into the future, using only a single timepoint as an initial condition, and quantitative oscillatory behavior 400 steps into the future.},
	urldate = {2025-04-30},
	booktitle = {2019 {American} {Control} {Conference} ({ACC})},
	author = {Yeung, Enoch and Kundu, Soumya and Hodas, Nathan},
	month = jul,
	year = {2019},
	note = {ISSN: 2378-5861},
	keywords = {Koopman operator},
	pages = {4832--4839},
}

@article{maheswaranathan_universality_2019,
	title = {Universality and individuality in neural dynamics across large populations of recurrent networks},
	volume = {2019},
	issn = {1049-5258},
	abstract = {Task-based modeling with recurrent neural networks (RNNs) has emerged as a popular way to infer the computational function of different brain regions. These models are quantitatively assessed by comparing the low-dimensional neural representations of the model with the brain, for example using canonical correlation analysis (CCA). However, the nature of the detailed neurobiological inferences one can draw from such efforts remains elusive. For example, to what extent does training neural networks to solve common tasks uniquely determine the network dynamics, independent of modeling architectural choices? Or alternatively, are the learned dynamics highly sensitive to different model choices? Knowing the answer to these questions has strong implications for whether and how we should use task-based RNN modeling to understand brain dynamics. To address these foundational questions, we study populations of thousands of networks, with commonly used RNN architectures, trained to solve neuroscientifically motivated tasks and characterize their nonlinear dynamics. We find the geometry of the RNN representations can be highly sensitive to different network architectures, yielding a cautionary tale for measures of similarity that rely on representational geometry, such as CCA. Moreover, we find that while the geometry of neural dynamics can vary greatly across architectures, the underlying computational scaffold-the topological structure of fixed points, transitions between them, limit cycles, and linearized dynamics-often appears universal across all architectures.},
	language = {eng},
	journal = {Advances in Neural Information Processing Systems},
	author = {Maheswaranathan, Niru and Williams, Alex H. and Golub, Matthew D. and Ganguli, Surya and Sussillo, David},
	month = dec,
	year = {2019},
	pmid = {32782422},
	pmcid = {PMC7416639},
	pages = {15629--15641},
}

@article{maheswaranathan_reverse_2019,
	title = {Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics},
	volume = {32},
	issn = {1049-5258},
	abstract = {Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it-to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.},
	language = {eng},
	journal = {Advances in Neural Information Processing Systems},
	author = {Maheswaranathan, Niru and Williams, Alex H. and Golub, Matthew D. and Ganguli, Surya and Sussillo, David},
	month = dec,
	year = {2019},
	pmid = {32782423},
	pmcid = {PMC7416638},
	pages = {15696--15705},
}

@inproceedings{deka_koopman-based_2022,
	title = {Koopman-based {Neural} {Lyapunov} functions for general attractors},
	url = {https://ieeexplore.ieee.org/abstract/document/9992927},
	doi = {10.1109/CDC51059.2022.9992927},
	abstract = {Koopman spectral theory has grown in the past decade as a powerful tool for dynamical systems analysis and control. In this paper, we show how recent data-driven techniques for estimating Koopman-Invariant subspaces with neural networks can be leveraged to extract Lyapunov certificates for the underlying system. In our work, we specifically focus on systems with a limit-cycle, beyond just an isolated equilibrium point, and use Koopman eigenfunctions to efficiently parameterize candidate Lyapunov functions to construct forward-invariant sets under some (unknown) attractor dynamics. Additionally, when the dynamics are polynomial and when neural networks are replaced by polynomials as a choice of function approximators in our approach, one can further leverage Sum-of-Squares programs and/or nonlinear programs to yield provably correct Lyapunov certificates. In such a polynomial case, our Koopman-based approach for constructing Lyapunov functions uses significantly fewer decision variables compared to directly formulating and solving a Sum-of-Squares optimization problem.},
	urldate = {2025-04-30},
	booktitle = {2022 {IEEE} 61st {Conference} on {Decision} and {Control} ({CDC})},
	author = {Deka, Shankar A. and Valle, Alonso M. and Tomlin, Claire J.},
	month = dec,
	year = {2022},
	note = {ISSN: 2576-2370},
	keywords = {Approximation algorithms, Control systems, Eigenvalues and eigenfunctions, Koopman operator, Limit-cycles, Neural networks, Nonlinear dynamical systems, Trajectory},
	pages = {5123--5128},
}

@article{brunton_chaos_2017,
	title = {Chaos as an intermittently forced linear system},
	volume = {8},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-017-00030-8},
	doi = {10.1038/s41467-017-00030-8},
	abstract = {Abstract
            Understanding the interplay of order and disorder in chaos is a central challenge in modern quantitative science. Approximate linear representations of nonlinear dynamics have long been sought, driving considerable interest in Koopman theory. We present a universal, data-driven decomposition of chaos as an intermittently forced linear system. This work combines delay embedding and Koopman theory to decompose chaotic dynamics into a linear model in the leading delay coordinates with forcing by low-energy delay coordinates; this is called the Hankel alternative view of Koopman (HAVOK) analysis. This analysis is applied to the Lorenz system and real-world examples including Earth’s magnetic field reversal and measles outbreaks. In each case, forcing statistics are non-Gaussian, with long tails corresponding to rare intermittent forcing that precedes switching and bursting phenomena. The forcing activity demarcates coherent phase space regions where the dynamics are approximately linear from those that are strongly nonlinear.},
	language = {en},
	number = {1},
	urldate = {2024-08-15},
	journal = {Nature Communications},
	author = {Brunton, Steven L. and Brunton, Bingni W. and Proctor, Joshua L. and Kaiser, Eurika and Kutz, J. Nathan},
	month = may,
	year = {2017},
	keywords = {Koopman operator},
	pages = {19},
}

@article{deka_supervised_2023,
	title = {Supervised {Learning} of {Lyapunov} {Functions} {Using} {Laplace} {Averages} of {Approximate} {Koopman} {Eigenfunctions}},
	volume = {7},
	issn = {2475-1456},
	url = {https://ieeexplore.ieee.org/abstract/document/10171181},
	doi = {10.1109/LCSYS.2023.3291657},
	abstract = {Modern data-driven techniques have rapidly progressed beyond modelling and systems identification, with a growing interest in learning high-level dynamical properties of a system, such as safe-set invariance, reachability, input-to-state stability etc. In this letter, we propose a novel supervised Deep Learning technique for constructing Lyapunov certificates, by leveraging Koopman Operator theory-based numerical tools (Extended Dynamic Mode Decomposition and Generalized Laplace Analysis) to robustly and efficiently generate explicit ground truth data for training. This is in stark contrast to existing Deep Learning methods where the loss functions plainly penalize Lyapunov condition violation in the absence of labelled data for direct regression. Furthermore, our approach leads to a linear parameterization of Lyapunov candidate functions in terms of stable eigenfunctions of the Koopman operator, making them more interpretable compared to standard DNN-based architecture. We demonstrate and validate our approach numerically using 2-dimensional and 10-dimensional examples.},
	urldate = {2025-04-30},
	journal = {IEEE Control Systems Letters},
	author = {Deka, Shankar A. and Dimarogonas, Dimos V.},
	year = {2023},
	keywords = {Convergence, Data-driven modeling, Deep learning, Eigenvalues and eigenfunctions, Koopman operator, Lyapunov function, Lyapunov methods, Neural networks, Stability analysis, Trajectory, machine learning, neural networks},
	pages = {3072--3077},
}

@article{e_deep_2018,
	title = {The {Deep} {Ritz} {Method}: {A} {Deep} {Learning}-{Based} {Numerical} {Algorithm} for {Solving} {Variational} {Problems}},
	volume = {6},
	issn = {2194-671X},
	shorttitle = {The {Deep} {Ritz} {Method}},
	url = {https://doi.org/10.1007/s40304-018-0127-z},
	doi = {10.1007/s40304-018-0127-z},
	abstract = {We propose a deep learning-based method, the Deep Ritz Method, for numerically solving variational problems, particularly the ones that arise from partial differential equations. The Deep Ritz Method is naturally nonlinear, naturally adaptive and has the potential to work in rather high dimensions. The framework is quite simple and fits well with the stochastic gradient descent method used in deep learning. We illustrate the method on several problems including some eigenvalue problems.},
	language = {en},
	number = {1},
	urldate = {2025-04-06},
	journal = {Communications in Mathematics and Statistics},
	author = {E, Weinan and Yu, Bing},
	month = mar,
	year = {2018},
	keywords = {35Q68, Deep Ritz Method, Eigenvalue problems, PDE, Variational problems},
	pages = {1--12},
}

@misc{ye_generalist_2025,
	title = {A {Generalist} {Intracortical} {Motor} {Decoder}},
	copyright = {http://creativecommons.org/licenses/by-nc/4.0/},
	url = {http://biorxiv.org/lookup/doi/10.1101/2025.02.02.634313},
	doi = {10.1101/2025.02.02.634313},
	abstract = {Mapping the relationship between neural activity and motor behavior is a central aim of sensorimotor neuroscience and neurotechnology. While most progress to this end has relied on restricting complexity, the advent of foundation models instead proposes integrating a breadth of data as an alternate avenue for broadly advancing downstream modeling. We quantify this premise for motor decoding from intracortical microelectrode data, pretraining an autoregressive Transformer on 2000 hours of neural population spiking activity paired with diverse motor covariates from over 30 monkeys and humans. The resulting model is broadly useful, benefiting decoding on 8 downstream decoding tasks and generalizing to a variety of neural distribution shifts. However, we also highlight that scaling autoregressive Transformers seems unlikely to resolve limitations stemming from sensor variability and output stereotypy in neural datasets. Code: https://github.com/joel99/ndt3},
	language = {en},
	urldate = {2025-04-03},
	author = {Ye, Joel and Rizzoglio, Fabio and Smoulder, Adam and Mao, Hongwei and Ma, Xuan and Marino, Patrick and Chowdhury, Raeed H and Moore, Dalton D and Blumenthal, Gary and Hockeimer, Will and Kunigk, Nicolas G. and Mayo, J Patrick and Batista, Aaron P and Chase, Steven M and Rouse, Adam G and Boninger, Michael L. and Greenspon, Charles and Schwartz, Andrew B. and Hatsopoulos, Nicholas and Miller, Lee E and Bouchard, Kristofer and Collinger, Jennifer and Wehbe, Leila and Gaunt, Robert},
	month = feb,
	year = {2025},
}

@article{finkelstein_attractor_2021,
	title = {Attractor dynamics gate cortical information flow during decision-making},
	volume = {24},
	issn = {1097-6256, 1546-1726},
	url = {https://www.nature.com/articles/s41593-021-00840-6},
	doi = {10.1038/s41593-021-00840-6},
	language = {en},
	number = {6},
	urldate = {2025-02-13},
	journal = {Nature Neuroscience},
	author = {Finkelstein, Arseny and Fontolan, Lorenzo and Economo, Michael N. and Li, Nuo and Romani, Sandro and Svoboda, Karel},
	month = jun,
	year = {2021},
	pages = {843--850},
}

@article{stringer_high-dimensional_2019,
	title = {High-dimensional geometry of population responses in visual cortex},
	volume = {571},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1346-5},
	doi = {10.1038/s41586-019-1346-5},
	language = {en},
	number = {7765},
	urldate = {2025-01-29},
	journal = {Nature},
	author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Carandini, Matteo and Harris, Kenneth D.},
	month = jul,
	year = {2019},
	pages = {361--365},
}

@article{pals_trained_2024,
	title = {Trained recurrent neural networks develop phase-locked limit cycles in a working memory task},
	volume = {20},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1011852},
	doi = {10.1371/journal.pcbi.1011852},
	abstract = {Neural oscillations are ubiquitously observed in many brain areas. One proposed functional role of these oscillations is that they serve as an internal clock, or ‘frame of reference’. Information can be encoded by the timing of neural activity relative to the
              phase
              of such oscillations. In line with this hypothesis, there have been multiple empirical observations of such
              phase codes
              in the brain. Here we ask: What kind of neural dynamics support phase coding of information with neural oscillations? We tackled this question by analyzing recurrent neural networks (RNNs) that were trained on a working memory task. The networks were given access to an external reference oscillation and tasked to produce an oscillation, such that the phase difference between the reference and output oscillation maintains the identity of transient stimuli. We found that networks converged to stable oscillatory dynamics. Reverse engineering these networks revealed that each phase-coded memory corresponds to a separate limit cycle attractor. We characterized how the stability of the attractor dynamics depends on both reference oscillation amplitude and frequency, properties that can be experimentally observed. To understand the connectivity structures that underlie these dynamics, we showed that trained networks can be described as two phase-coupled oscillators. Using this insight, we condensed our trained networks to a reduced model consisting of two functional modules: One that generates an oscillation and one that implements a coupling function between the internal oscillation and external reference.
            
            In summary, by reverse engineering the dynamics and connectivity of trained RNNs, we propose a mechanism by which neural networks can harness reference oscillations for working memory. Specifically, we propose that a phase-coding network generates autonomous oscillations which it couples to an external reference oscillation in a multi-stable fashion.},
	language = {en},
	number = {2},
	urldate = {2024-12-31},
	journal = {PLOS Computational Biology},
	author = {Pals, Matthijs and Macke, Jakob H. and Barak, Omri},
	editor = {Graham, Lyle J.},
	month = feb,
	year = {2024},
	pages = {e1011852},
}

@article{sorscher_neural_2022,
	title = {Neural representational geometry underlies few-shot concept learning},
	volume = {119},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2200800119},
	doi = {10.1073/pnas.2200800119},
	abstract = {Understanding the neural basis of the remarkable human cognitive capacity to learn novel concepts from just one or a few sensory experiences constitutes a fundamental problem. We propose a simple, biologically plausible, mathematically tractable, and computationally powerful neural mechanism for few-shot learning of naturalistic concepts. We posit that the concepts that can be learned from few examples are defined by tightly circumscribed manifolds in the neural firing-rate space of higher-order sensory areas. We further posit that a single plastic downstream readout neuron learns to discriminate new concepts based on few examples using a simple plasticity rule. We demonstrate the computational power of our proposal by showing that it can achieve high few-shot learning accuracy on natural visual concepts using both macaque inferotemporal cortex representations and deep neural network (DNN) models of these representations and can even learn novel visual concepts specified only through linguistic descriptors. Moreover, we develop a mathematical theory of few-shot learning that links neurophysiology to predictions about behavioral outcomes by delineating several fundamental and measurable geometric properties of neural representations that can accurately predict the few-shot learning performance of naturalistic concepts across all our numerical simulations. This theory reveals, for instance, that high-dimensional manifolds enhance the ability to learn new concepts from few examples. Intriguingly, we observe striking mismatches between the geometry of manifolds in the primate visual pathway and in trained DNNs. We discuss testable predictions of our theory for psychophysics and neurophysiological experiments.},
	language = {en},
	number = {43},
	urldate = {2024-12-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Sorscher, Ben and Ganguli, Surya and Sompolinsky, Haim},
	month = oct,
	year = {2022},
	pages = {e2200800119},
}

@article{hastie_surprises_2022,
	title = {Surprises in high-dimensional ridgeless least squares interpolation},
	volume = {50},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-50/issue-2/Surprises-in-high-dimensional-ridgeless-least-squares-interpolation/10.1214/21-AOS2133.full},
	doi = {10.1214/21-AOS2133},
	number = {2},
	urldate = {2024-12-17},
	journal = {The Annals of Statistics},
	author = {Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J.},
	month = apr,
	year = {2022},
}

@article{ostrow_beyond_2023,
	title = {Beyond {Geometry}: {Comparing} the {Temporal} {Structure} of {Computation} in {Neural} {Circuits} with {Dynamical} {Similarity} {Analysis}},
	volume = {36},
	shorttitle = {Beyond {Geometry}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/6ac807c9b296964409b277369e55621a-Abstract-Conference.html},
	language = {en},
	urldate = {2024-08-15},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ostrow, Mitchell and Eisen, Adam and Kozachkov, Leo and Fiete, Ila},
	month = dec,
	year = {2023},
	pages = {33824--33837},
}

@misc{brenner_tractable_2022,
	title = {Tractable {Dendritic} {RNNs} for {Reconstructing} {Nonlinear} {Dynamical} {Systems}},
	url = {http://arxiv.org/abs/2207.02542},
	abstract = {In many scientific disciplines, we are interested in inferring the nonlinear dynamical system underlying a set of observed time series, a challenging task in the face of chaotic behavior and noise. Previous deep learning approaches toward this goal often suffered from a lack of interpretability and tractability. In particular, the high-dimensional latent spaces often required for a faithful embedding, even when the underlying dynamics lives on a lower-dimensional manifold, can hamper theoretical analysis. Motivated by the emerging principles of dendritic computation, we augment a dynamically interpretable and mathematically tractable piecewise-linear (PL) recurrent neural network (RNN) by a linear spline basis expansion. We show that this approach retains all the theoretically appealing properties of the simple PLRNN, yet boosts its capacity for approximating arbitrary nonlinear dynamical systems in comparatively low dimensions. We employ two frameworks for training the system, one combining back-propagation-through-time (BPTT) with teacher forcing, and another based on fast and scalable variational inference. We show that the dendritically expanded PLRNN achieves better reconstructions with fewer parameters and dimensions on various dynamical systems benchmarks and compares favorably to other methods, while retaining a tractable and interpretable structure.},
	urldate = {2023-06-11},
	publisher = {arXiv},
	author = {Brenner, Manuel and Hess, Florian and Mikhaeil, Jonas M. and Bereska, Leonard and Monfared, Zahra and Kuo, Po-Chen and Durstewitz, Daniel},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02542 [nlin, physics:physics]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Nonlinear Sciences - Chaotic Dynamics, Physics - Computational Physics},
}

@article{keshtkaran_large-scale_2022,
	title = {A large-scale neural network training framework for generalized estimation of single-trial population dynamics},
	volume = {19},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/s41592-022-01675-0},
	doi = {10.1038/s41592-022-01675-0},
	language = {en},
	number = {12},
	urldate = {2023-04-19},
	journal = {Nature Methods},
	author = {Keshtkaran, Mohammad Reza and Sedler, Andrew R. and Chowdhury, Raeed H. and Tandon, Raghav and Basrai, Diya and Nguyen, Sarah L. and Sohn, Hansem and Jazayeri, Mehrdad and Miller, Lee E. and Pandarinath, Chethan},
	month = dec,
	year = {2022},
	pages = {1572--1577},
}

@techreport{schimel_ilqr-vae_2021,
	type = {preprint},
	title = {{iLQR}-{VAE} : control-based learning of input-driven dynamics with applications to neural data},
	shorttitle = {{iLQR}-{VAE}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2021.10.07.463540},
	abstract = {A
            bstract
          
          Understanding how neural dynamics give rise to behaviour is one of the most fundamental questions in systems neuroscience. To achieve this, a common approach is to record neural populations in behaving animals, and model these data as emanating from a latent dynamical system whose state trajectories can then be related back to behavioural observations via some form of decoding. As recordings are typically performed in localized circuits that form only a part of the wider implicated network, it is important to simultaneously learn the local dynamics and infer any unobserved external input that might drive them. Here, we introduce iLQR-VAE, a control-based approach to variational inference in nonlinear dynamical systems, capable of learning both latent dynamics, initial conditions, and ongoing external inputs. As in recent deep learning approaches, our method is based on an input-driven sequential variational autoencoder (VAE). The main novelty lies in the use of the powerful iterative linear quadratic regulator algorithm (iLQR) in the recognition model. Optimization of the standard evidence lower-bound requires differentiating through iLQR solutions, which is made possible by recent advances in differentiable control. Importantly, the recognition model is naturally tied to the generative model, greatly reducing the number of free parameters and ensuring high-quality inference throughout the course of learning. Moreover, iLQR can be used to perform inference flexibly on heterogeneous trials of varying lengths. This allows for instance to evaluate the model on a single long trial after training on smaller chunks. We demonstrate the effectiveness of iLQR-VAE on a range of synthetic systems, with autonomous as well as input-driven dynamics. We further apply it to neural and behavioural recordings in non-human primates performing two different reaching tasks, and show that iLQR-VAE yields high-quality kinematic reconstructions from the neural data.},
	language = {en},
	urldate = {2023-03-30},
	institution = {Neuroscience},
	author = {Schimel, Marine and Kao, Ta-Chu and Jensen, Kristopher T. and Hennequin, Guillaume},
	month = oct,
	year = {2021},
	doi = {10.1101/2021.10.07.463540},
}

@article{pandarinath_inferring_2018,
	title = {Inferring single-trial neural population dynamics using sequential auto-encoders},
	volume = {15},
	issn = {1548-7091, 1548-7105},
	url = {http://www.nature.com/articles/s41592-018-0109-9},
	doi = {10.1038/s41592-018-0109-9},
	language = {en},
	number = {10},
	urldate = {2023-03-29},
	journal = {Nature Methods},
	author = {Pandarinath, Chethan and O’Shea, Daniel J. and Collins, Jasmine and Jozefowicz, Rafal and Stavisky, Sergey D. and Kao, Jonathan C. and Trautmann, Eric M. and Kaufman, Matthew T. and Ryu, Stephen I. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shenoy, Krishna V. and Abbott, L. F. and Sussillo, David},
	month = oct,
	year = {2018},
	pages = {805--815},
}
