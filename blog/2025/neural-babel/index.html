<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Neural Babel: What Do Neural Networks Talk About? | UniReps Blog </title> <meta name="author" content="UniReps Blog"> <meta name="description" content="Translating internal representations of neural networks into natural language."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/blog/assets/img/unireps_favicon.png?3a42502b41deab62714411b8479222c3"> <link rel="stylesheet" href="/blog/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unireps.org//blog/2025/neural-babel/"> <script src="/blog/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/blog/assets/js/distillpub/template.v2.js"></script> <script src="/blog/assets/js/distillpub/transforms.v2.js"></script> <script src="/blog/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Neural Babel: What Do Neural Networks Talk About?",
            "description": "Translating internal representations of neural networks into natural language.",
            "published": "October 26, 2025",
            "authors": [
              
              {
                "author": "Sike Ogieva",
                "authorURL": "https://linkedin.com/in/sike",
                "affiliations": [
                  {
                    "name": "Amherst College",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog//"> <span class="font-weight-bold">UniReps</span> Blog </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Neural Babel: What Do Neural Networks Talk About?</h1> <p>Translating internal representations of neural networks into natural language.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#1-introduction">1. Introduction</a> </div> <div> <a href="#2-teaching-machines-to-point-at-things">2. Teaching Machines to Point at Things</a> </div> <div> <a href="#3-conference">3. Conference</a> </div> <ul> <li> <a href="#3-1-training-the-speaker-listener">3.1. Training the Speaker-Listener</a> </li> <li> <a href="#3-2-cross-rule-validation-the-baseline-signal-problem">3.2. Cross-Rule Validation - The Baseline Signal Problem</a> </li> </ul> <div> <a href="#4-translation">4. Translation</a> </div> <ul> <li> <a href="#4-1-training-the-translator">4.1. Training the Translator</a> </li> <li> <a href="#4-2-adjusted-evaluation-metrics">4.2. Adjusted Evaluation Metrics</a> </li> </ul> <div> <a href="#5-conclusions">5. Conclusions</a> </div> <ul> <li> <a href="#5-1-what-the-cross-rule-validation-tells-us">5.1. What the Cross-Rule Validation Tells Us</a> </li> <li> <a href="#5-2-what-the-adjusted-metrics-tell-us">5.2. What the Adjusted Metrics Tell Us</a> </li> </ul> </nav> </d-contents> <p>Code for this project can be found at: https://github.com/sike25/neural_syntax</p> <h2 id="1-introduction">1. Introduction</h2> <p>Imagine overhearing a conversation in a language you don’t speak. The speakers understand each other perfectly, but you have no idea what they’re saying. In this project, the speakers were neural networks, and the language emerged spontaneously when they were trained to collaboratively solve a task. We tried to build a translator for this “neuralese” and this is what we found.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/confused-math-lady.jpg" sizes="95vw"></source> <img src="/blog/assets/img/2025-10-25-neural-babel/confused-math-lady.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Me trying to understand neuralese</p> <h2 id="2-teaching-machines-to-point-at-things">2. Teaching Machines to Point at Things</h2> <p>Take a world of objects W and a subset of this world X.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/w-and-x-example.jpg" sizes="95vw"></source> <img src="/blog/assets/img/2025-10-25-neural-babel/w-and-x-example.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong>Fig. 1.</strong> A world W of objects has a subset X decided by some rule.</p> <p>Before scrolling, how would YOU describe this selection?</p> <details><summary>Look at the rule</summary> <p>RULE: ‘blue’</p> </details> <p>A neural network called the speaker is given \(W\) and \(X\) and outputs a neuralese vector \(V\) that ideally captures this rule. Another network called the listener takes in \(V\) and an element of the world \(W_i \in W\) and predicts whether or not \(W_i\) belongs in \(X\). So the listener never sees \(X\), and it relies entirely on the speaker’s neuralese output to understand \(X\).</p> <p>Andreas and Klein (2017) <d-cite key="andreas2017"></d-cite> shows us that this “language” could be negated via linear transformation to take on the opposite meaning. Now, this project attempts to figure out whether these vectors can be directly translated.</p> <p>For training data, Andreas and Klein use labels from the GENX dataset <d-cite key="genx"></d-cite>. We forewent this dataset and generated our own. Each object had a color, shape, and outline thickness encoded row by row.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/matrix-as-object.jpg" sizes="95vw"></source> <img src="/blog/assets/img/2025-10-25-neural-babel/matrix-as-object.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong>Fig. 2.</strong> An object \(W_i\) of a world \(W\), represented as a one-hot encoded matrix.</p> <p>Our dataset had 80,730 unique worlds of 5 objects each. Subsets were created using 72 unique rules: single feature rule <code class="language-plaintext highlighter-rouge">red</code>, single feature negation <code class="language-plaintext highlighter-rouge">not red</code>, two features of different types joined by and/or <code class="language-plaintext highlighter-rouge">red and circle</code>, <code class="language-plaintext highlighter-rouge">triangle or thick-outline</code>. Skipping over world-rule combinations that resulted in empty subsets, we gathered a dataset of 1,705,833 (world \(W\), subset \(X\), rule) entries.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/dataset-entry.jpg" sizes="95vw"></source> <img src="/blog/assets/img/2025-10-25-neural-babel/dataset-entry.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong>Fig. 3.</strong> An entry from our generated dataset.</p> <p>Training separate networks to evolve languages in order to play a communication game has also been done in Gupta et al. (2021) <d-cite key="gupta2021"></d-cite>, Lazaridou et al. (2018) <d-cite key="lazaridou2018"></d-cite> and Andreas et al. (2018) <d-cite key="andreas2018"></d-cite>.</p> <h2 id="3-conference">3. Conference</h2> <h3 id="31-training-the-speaker-listener">3.1. Training the Speaker-Listener</h3> <p>The speaker-listener system achieved <strong>99.56%</strong> test accuracy on an unseen test set, with accuracy climbing from 60% to 95% by epoch 1, implying that the task was easily learned.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/speaker-listener.jpg" sizes="95vw"></source> <img src="/blog/assets/img/2025-10-25-neural-babel/speaker-listener.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong>Fig. 4.</strong> The Speaker-Listener Architecture.</p> <p>To prevent the speaker from encoding positional shortcuts (“select positions [0,2,3]”) and force it to learn semantic rules (“select purple circles”), the world objects are shuffled before being fed to the listener.</p> <h3 id="32-cross-rule-validation---the-baseline-signal-problem">3.2. Cross-Rule Validation - The Baseline Signal Problem</h3> <p>After training, we needed to verify that the speaker actually learned to encode rules meaningfully. Did “red objects” produce similar neuralese across different worlds? Did <code class="language-plaintext highlighter-rouge">red</code> neuralese differ significantly from <code class="language-plaintext highlighter-rouge">triangle</code> neuralese?</p> <p>Using the trained speaker, we generated 100 neuralese vectors each for 9 different rules (like <code class="language-plaintext highlighter-rouge">red</code>, <code class="language-plaintext highlighter-rouge">green or triangle</code>, <code class="language-plaintext highlighter-rouge">not purple</code>, etc.). Then we measured how similar these vectors were to each other using cosine similarity. We expected that neuralese for the same rule should be similar (high within-rule similarity), while neuralese for different rules should be different (low cross-rule similarity), but the similarities for both categories were high (\(0.908 \pm 0.090\) and \(0.865 \pm 0.097\) respectively).</p> <p>We guessed that the neuralese might contain a massive “baseline signal” that concealed the actual messages. So we normalized the neuralese by computing the average vector across all examples, then subtracting it from each vector. This brought the cosine similarity for same-rule neuralese down to \(0.246 \pm 0.519\) (moderate similarity) and cross-rule similarity to \(-0.069 \pm 0.500\) (negative similarity).</p> <p>This analysis showed that rule information did exist in the neuralese, just hidden beneath the baseline. We therefore inferred that we should normalize the neuralese before attempting to translate them.</p> <h2 id="4-translation">4. Translation</h2> <h3 id="41-training-the-translator">4.1. Training the Translator</h3> <p>Having established that the speaker-listener system could communicate, the central question comes up. Can this emergent neuralese be translated into natural language?</p> <p>If the neuralese vectors have encoded semantic information about the rules, then an appropriate neural network ought to be able to reverse-engineer these rules from the vector alone.</p> <p>The translator network is a 5-layer multilayer perceptron which takes a normalized 12-dimensional neuralese vector as its input and outputs a 3-token sequence which it classifies over our 13-word vocabulary (9 features + <code class="language-plaintext highlighter-rouge">and</code>, <code class="language-plaintext highlighter-rouge">not</code>, <code class="language-plaintext highlighter-rouge">or</code> + <code class="language-plaintext highlighter-rouge">&lt;blank&gt;</code>). The translator was trained on 1,364,666 training examples (40% of the dataset).</p> <p>Evaluation on an unseen test set yielded modest results. The network correctly predicted individual tokens <strong>63.76%</strong> of the time, and got the entire 3-token rule right <strong>38.17%</strong> of the time.</p> <h3 id="42-adjusted-evaluation-metrics">4.2. Adjusted Evaluation Metrics</h3> <p>Next, we considered the possibility that raw accuracy metrics could be painting an incomplete picture. Look at this example:</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/adjusted-accuracy-example.jpg" sizes="95vw"></source> <img src="/blog/assets/img/2025-10-25-neural-babel/adjusted-accuracy-example.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong>Fig. 5.</strong> Different rules can describe the same target subset.</p> <p>Notice how different rules can produce the same target subset? We then decided to compute a metric where a predicted rule is accurate if it produces the same subset of the world as the ground truth rule even if it is different from the ground truth rule. We called this the <strong>adjusted accuracy</strong> and calculated it at <strong>43.24%</strong>.</p> <p>Consider another example:</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/description-accuracy-example.jpg" sizes="95vw"></source> <img src="/blog/assets/img/2025-10-25-neural-babel/description-accuracy-example.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong>Fig. 6.</strong> Predicted rules can perfectly describe the target subset but include other elements. We can consider this phenomenon a sign of partial learning.</p> <p>Here, the predicted rule correctly describes all the objects in the target subset even though it incorrectly includes the purple triangle. Another adjusted metric tracked whether the rule correctly describes all the selected objects. We called this the <strong>description accuracy</strong> and calculated it at <strong>51.44%</strong>.</p> <p>We also found that only <strong>61% of predicted rules were even semantically valid</strong>. Here are some real examples of malformed rules from our test set:</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/malformed-rules-example.jpg" sizes="95vw"></source> <img src="/blog/assets/img/2025-10-25-neural-babel/malformed-rules-example.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong>Fig. 7.</strong> The translator can produce incoherent or malformed rules.</p> <p>This meant that some portion of the incorrectly predicted rules were not rules at all. So we wondered, if we filtered only for semantically valid predictions, what would the evaluation metrics look like? Conditioning on semantic validity raised the adjusted accuracy from <strong>43.24% to 70.81%</strong> and the description accuracy from <strong>51.44% to 84.23%</strong>.</p> <table> <thead> <tr> <th> </th> <th style="text-align: center">All Predictions</th> <th style="text-align: center">Semantically Valid Only</th> </tr> </thead> <tbody> <tr> <td>Raw Sequence Accuracy</td> <td style="text-align: center">38.17%</td> <td style="text-align: center">-</td> </tr> <tr> <td>Adjusted Sequence Accuracy</td> <td style="text-align: center">43.24%</td> <td style="text-align: center">70.81%</td> </tr> <tr> <td>Description Sequence Accuracy</td> <td style="text-align: center">51.44%</td> <td style="text-align: center">84.23%</td> </tr> </tbody> </table> <h2 id="5-conclusions">5. Conclusions</h2> <h3 id="51-what-the-cross-rule-validation-tells-us">5.1. What the Cross-Rule Validation Tells Us</h3> <p>Remember our baseline signal problem? After normalization, same-rule neuralese vectors had cosine similarities around 0.246— moderate, but nowhere near the ~1.0 we’d expect if neuralese vectors were perfect rule encodings. The concept of rules exists in neuralese, but neuralese is not equal to the rules themselves.</p> <p>If neuralese were theoretically equivalent to our selective rules—if <code class="language-plaintext highlighter-rouge">red</code> always mapped to some canonical <code class="language-plaintext highlighter-rouge">red</code> vector—we’d see cosine similarities approaching 1.0 within rule categories. So we know these vectors are contaminated with other (possibly useful) data. Perhaps information about the specific world being described. Perhaps metadata about the selection itself—how many objects are selected, their distribution across feature types, spatial patterns in the original (pre-shuffled) arrangement.</p> <p>So the speaker might learn some rule concepts while also exploiting easier statistical patterns.</p> <blockquote> <p>The concept of rules exists in neuralese, but neuralese is not equal to the rules themselves.</p> </blockquote> <h3 id="52-what-the-adjusted-metrics-tell-us">5.2. What the Adjusted Metrics Tell Us</h3> <p>The translator should not have struggled as much as it did to produce well-formed 3-token sequences from such a limited and consistent grammar. However, the strong performance on semantically valid rules suggests that the translation task itself is feasible. The bottleneck appears to be the token-by-token classification approach: the MLP architecture struggled to construct coherent rules when predicting each token independently, even though it could successfully translate neuralese when it did produce valid sequences.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/blog/assets/bibliography/2025-10-25-neural-babel.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"UniReps/unireps.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 UniReps Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>