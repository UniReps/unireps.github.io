<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding Adversarial Vulnerabilities and Emergent Patterns in Multimodal RL | UniReps Blog </title> <meta name="author" content="UniReps Blog"> <meta name="description" content="Using a simplified multimodal RL agent to explore adversarial vulnerabilities that emerge when using different modalities."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/blog/assets/img/unireps_favicon.png?3a42502b41deab62714411b8479222c3"> <link rel="stylesheet" href="/blog/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unireps.org//blog/2025/multimodalEmergingPatterns/"> <script src="/blog/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/blog/assets/js/distillpub/template.v2.js"></script> <script src="/blog/assets/js/distillpub/transforms.v2.js"></script> <script src="/blog/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Understanding Adversarial Vulnerabilities and Emergent Patterns in Multimodal RL",
            "description": "Using a simplified multimodal RL agent to explore adversarial vulnerabilities that emerge when using different modalities.",
            "published": "October 08, 2025",
            "authors": [
              
              {
                "author": "Shayan Jalalipour",
                "authorURL": "shayan2@pdx.edu",
                "affiliations": [
                  {
                    "name": "Portland State University",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Danielle Justo",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Smith College",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Banafsheh Rekabdar",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Portland State University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog//"> <span class="font-weight-bold">UniReps</span> Blog </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Understanding Adversarial Vulnerabilities and Emergent Patterns in Multimodal RL</h1> <p>Using a simplified multimodal RL agent to explore adversarial vulnerabilities that emerge when using different modalities.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#abstract">Abstract</a> </div> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#background">Background</a> </div> <ul> <li> <a href="#related-works">Related Works</a> </li> <li> <a href="#adversarial-attacks">Adversarial Attacks</a> </li> <li> <a href="#adversarial-defenses">Adversarial Defenses</a> </li> <li> <a href="#soft-actor-critic-models">Soft Actor-Critic Models</a> </li> </ul> <div> <a href="#methodology">Methodology</a> </div> <ul> <li> <a href="#agent">Agent</a> </li> <li> <a href="#environment">Environment</a> </li> <li> <a href="#adversarial-attack">Adversarial Attack</a> </li> <li> <a href="#adversarial-defenses">Adversarial Defenses</a> </li> <li> <a href="#adversarial-evaluation">Adversarial Evaluation</a> </li> </ul> <div> <a href="#results">Results</a> </div> <ul> <li> <a href="#baseline-performance">Baseline Performance</a> </li> <li> <a href="#attacking-an-un-defended-model">Attacking an Un-Defended Model</a> </li> <li> <a href="#attacking-a-defended-model">Attacking a Defended Model</a> </li> </ul> <div> <a href="#conclusions">Conclusions</a> </div> <div> <a href="#reproducibility-details">Reproducibility Details</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Deep learning systems were once tailored to a single input type, for instance RGB pixels in image models <d-cite key="resnet2015"></d-cite> or tokenized text for language generation <d-cite key="Radford2018ImprovingLU"></d-cite>. Over the last few years we have watched machine learning expand toward richer multimodal setups that fuse information sources <d-cite key="jiao-multimodal-survey"></d-cite>. Teams now blend sensors on autonomous platforms <d-cite key="panduru2025exploring"></d-cite>, translate content across formats such as image to text or text to speech, and pair visual and language prompts for both large language models <d-cite key="openai2024gpt4technicalreport"></d-cite> and image generators <d-cite key="rombach2022highresolutionimagesynthesislatent"></d-cite>. Multimodal development is vibrant and fast moving.</p> <p>With that momentum comes a pressing need to evaluate how these systems handle adversarial pressure. Every deep learning stack depends on trust, especially when it supports safety critical decisions. We must understand how attackers can exploit multimodal pipelines and how the interaction between modalities shapes new strengths and new points of failure. While single modality models have a long history of research on perturbations and data poisoning, the community still has only partial visibility into how those threats appear when modalities overlap.</p> <p>Live reinforcement learning agents raise the stakes even further. These policies operate within dynamic environments rather than static classification tasks, so defenses must respect timing, feedback, and control constraints. They also learn without labeled supervision, which complicates how we adapt classic adversarial tooling.</p> <p>Exploring the impact of these attacks on multimodal reinforcement learning is of critical importance because these agents also see use in high risk robotics and autonomous platforms. A brittle policy in those domains risks severe safety incidents and expensive hardware failures.</p> <p>Our study aims to map the interplay between adversarial attacks, defense strategies, and modality combinations on a baseline multimodal reinforcement learning agent. We document baseline performance and walk through empirical findings that show how different modality pairings shift behavior when attacked jointly or separately. We highlight how influence varies by modality and how defensive choices reshape those dynamics when interacting with different modalities.</p> <p>To support this investigation we extend open-source code provided by authors of DDiffPG to create and release a full pipeline to prepare datasets, train reference models, and evaluate attack and defense mixes across modalities.</p> <p>In this blog post we’ll talk about:</p> <ul> <li>Providing a testbed for adversarial evaluation of a multi-modal RL agent.</li> <li>Characterizing how attacking one or both modalities changes behavior.</li> <li>Showing that defenses introduce emergent patterns across modalities, sometimes improving robustness and sometimes destabilizing policies.</li> </ul> <h3 id="background--related-works">Background &amp; Related Works</h3> <p>Research on adversarial robustness has largely centered on language and vision systems, especially as large language models expanded into multimodal applications such as text to image and image to text experiences <d-cite key="wu2025dissectingadversarialrobustnessmultimodal,wang2025manipulatingmultimodalagentscrossmodal"></d-cite>. Investigations into decision making agents that blend multiple sensors remain comparatively sparse, with most of the attention directed toward autonomous driving stacks <d-cite key="chi_autonomous_survey2024,roheda2021multimodal"></d-cite>.</p> <p>As embodied agents gain capability and broader deployment, we need a clearer view of how combined modalities influence security. Building resilient multimodal pipelines is essential for maintaining trust in systems that operate in high risk settings with steadily increasing task complexity.</p> <h3 id="adversarial-attacks">Adversarial Attacks</h3> <p>Adversarial attacks are a branch of machine learning focused on manipulating model behavior in unintended or harmful ways. In particular, adversarial attacks are aimed at a “victim” model often designed with the flaws of a particular type of model or architecture in mind. These attacks typically involve introducing carefully designed “perturbations” to the input, which are intended to mislead or alter the model’s outputs. Depending on the attacker’s level of access to the internals of the model, attacks are classified as “white box” (full access, such as weights or gradient values), “gray box” (partial access, such as particular values or weights) or “black box” (no access at all, only inputs and outputs can be discerned around the black box). While perturbing inputs is the most common form of adversarial attack, other methods such as dataset “Poisoning” exist which alter training data to induce some desired behavior from the victim model.</p> <h3 id="adversarial-attacks-1">Adversarial Attacks</h3> <p>Adversarial attacks are a critical area of study in modern machine learning, focusing on how models can be manipulated into producing incorrect or harmful outputs. These attacks exploit weaknesses in model architectures or training processes by introducing subtle, carefully designed <em>perturbations</em> to the input data. Although these changes are often imperceptible to humans, they can drastically alter a model’s predictions, revealing vulnerabilities in even the most sophisticated AI systems.</p> <h4 id="types-of-adversarial-attacks">Types of Adversarial Attacks</h4> <p>Adversarial attacks are typically categorized based on the attacker’s level of access to the model’s internal information:</p> <p><strong>1. White-Box Attacks</strong><br> In white-box attacks, the attacker has full access to the model’s internals, including parameters, gradients, and architecture details. This allows for precise and highly effective perturbations tailored to exploit specific weaknesses in the model.</p> <p><strong>2. Gray-Box Attacks</strong><br> Gray-box attacks occur when the attacker has only partial access to the model. They may know certain weights, gradients, or architecture components, but not the entire system. These attacks are less direct than white-box methods but still capable of misleading models effectively.</p> <p><strong>3. Black-Box Attacks</strong><br> In black-box attacks, the attacker has no insight into the model’s internal workings. They can only observe inputs and outputs, using this limited information to infer how to alter the input data. Black-box attacks often rely on query-based or transfer-based strategies to achieve success.</p> <h4 id="beyond-input-perturbations-data-poisoning">Beyond Input Perturbations: Data Poisoning</h4> <p>While most adversarial attacks occur during inference by altering input data, another powerful form of attack targets the <strong>training phase</strong> itself. Known as <em>data poisoning</em>, this method involves introducing manipulated samples into the training dataset. Over time, these poisoned samples bias the model’s learning process, leading to unintended or malicious behaviors once deployed.</p> <h4 id="importance-of-adversarial-robustness">Importance of Adversarial Robustness</h4> <p>As AI systems become increasingly integrated into critical domains such as healthcare, finance, and autonomous systems, ensuring their resilience to adversarial manipulation is essential. Developing models that can detect, resist, and adapt to these attacks is a cornerstone of building trustworthy, safe, and reliable machine learning applications.</p> <h3 id="adversarial-defenses">Adversarial Defenses</h3> <p>Defending machine learning models against adversarial attacks is a multifaceted challenge that requires both proactive and reactive strategies. Broadly, adversarial defenses can be divided into three primary categories:</p> <h4 id="1-adversarial-training">1. Adversarial Training</h4> <p>This approach involves <strong>augmenting the training process with adversarial samples</strong> <d-cite key="zizzo2021certified,tramer_adaptive_2020,kuzina_defending_2022"></d-cite>. By exposing the model to perturbed examples during training, it learns to recognize and resist similar manipulations at inference time. Adversarial training effectively strengthens the model’s decision boundaries, making it more robust against future attacks.</p> <h4 id="2-attack-and-anomaly-detection">2. Attack and Anomaly Detection</h4> <p>Another common line of defense focuses on <strong>detecting adversarial activity</strong> by identifying perturbations, abnormal patterns, or suspicious data points before they can affect model performance <d-cite key="roth_odds_2019,fidel_when_2020,guo_detecting_2019,GolchinAnomolyDetection"></d-cite>. These methods often rely on secondary classifiers, statistical models, or clustering techniques to flag inconsistencies between normal and adversarial inputs.</p> <h4 id="3-input-filtering-and-perturbation-removal">3. Input Filtering and Perturbation Removal</h4> <p>A third approach aims to <strong>remove or disrupt adversarial perturbations</strong> before the input reaches the model <d-cite key="zhang2021defense,nie_diffusion_2022,yoon_adversarial_2021"></d-cite>. Techniques in this category may apply noise injection, smoothing, or reconstruction mechanisms that effectively “clean” the input data, neutralizing the adversarial effect.</p> <h4 id="defense-methods-used-in-this-study">Defense Methods Used in This Study</h4> <p>In our experiments, we employ three specific defense mechanisms aligned with the categories above:</p> <ul> <li> <strong>Disruption:</strong> Adding Gaussian noise to the input to reduce the impact of finely tuned perturbations.</li> <li> <strong>Detection:</strong> Utilizing a neural network classifier and traditional clustering techniques to identify anomalous inputs.</li> <li> <strong>Filtering:</strong> Applying a Variational Auto-Encoder (VAE) to reconstruct and denoise the input, effectively filtering out adversarial artifacts.</li> </ul> <p>Together, these methods provide complementary layers of protection, enhancing the overall robustness of the model against diverse forms of adversarial interference.</p> <h3 id="soft-actor-critic-models">Soft Actor-Critic Models</h3> <p><strong>Soft Actor-Critic (SAC)</strong> <d-cite key="haarnoja2018softactorcritic"></d-cite> is a reinforcement learning (RL) algorithm designed for continuous control tasks, such as the Ant-agent used in our <em>Ant-Maze</em> experiments. SAC combines two key components: an <strong>Actor</strong>, which represents the agent’s policy, and one or more <strong>Critics</strong>, which estimate value functions to guide learning.</p> <p>What makes SAC “soft” compared to traditional Actor-Critic methods is its inclusion of an <strong>entropy term</strong> in the objective function. This encourages the policy to remain more stochastic during training, promoting exploration rather than premature convergence to sub-optimal behaviors. In effect, SAC balances learning performance with policy diversity, achieving both <strong>stability</strong> and <strong>efficiency</strong> in complex, high-dimensional environments.</p> <h2 id="methodology">Methodology</h2> <h3 id="agent">Agent</h3> <p>To establish a baseline, we train a Soft Actor-Critic (SAC) agent <d-cite key="haarnoja2018softactorcritic"></d-cite> to control the MuJoCo Ant environment <d-cite key="todorov2012mujoco,towers2024gymnasium"></d-cite>. The Ant is a four-legged quadruped equipped with eight rotors, one at each joint, enabling coordinated movement across its four limbs, each composed of two interconnected links joined by a motorized joint.</p> <p>The agent’s observation space captures a comprehensive set of physical states to guide its behavior. It includes a velocity modality that records both linear and angular velocities (in meters and radians per second, respectively) for every limb, joint, and link of the Ant. Complementing this, an angular modality tracks joint angles in radians, covering the orientation between each limb link, the relative angles of the limbs to the torso, and the overall torso orientation.</p> <p>In addition, the agent receives a z-coordinate reading representing the torso’s height above the ground in meters. Notably, all observations are unbounded, formally defined within the continuous range (-∞, ∞), allowing unrestricted representation of the agent’s motion dynamics.</p> <p>The detailed observation space is outlined in the table below:</p> <table> <thead> <tr> <th>Num</th> <th>Observation</th> <th>Name</th> <th>Joint</th> <th>Unit</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>z-coordinate of the torso (centre)</td> <td>torso</td> <td>free</td> <td>position (m)</td> </tr> <tr> <td>1</td> <td>x-orientation of the torso (centre)</td> <td>torso</td> <td>free</td> <td>angle (rad)</td> </tr> <tr> <td>2</td> <td>y-orientation of the torso (centre)</td> <td>torso</td> <td>free</td> <td>angle (rad)</td> </tr> <tr> <td>3</td> <td>z-orientation of the torso (centre)</td> <td>torso</td> <td>free</td> <td>angle (rad)</td> </tr> <tr> <td>4</td> <td>w-orientation of the torso (centre)</td> <td>torso</td> <td>free</td> <td>angle (rad)</td> </tr> <tr> <td>5</td> <td>angle between torso and front left link on front left</td> <td>hip_1 (front_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>6</td> <td>angle between the two links on the front left</td> <td>ankle_1 (front_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>7</td> <td>angle between torso and front right link on front right</td> <td>hip_2 (front_right_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>8</td> <td>angle between the two links on the front right</td> <td>ankle_2 (front_right_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>9</td> <td>angle between torso and back left link on back left</td> <td>hip_3 (back_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>10</td> <td>angle between the two links on the back left</td> <td>ankle_3 (back_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>11</td> <td>angle between torso and back right link on back right</td> <td>hip_4 (right_back_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>12</td> <td>angle between the two links on the back right</td> <td>ankle_4 (right_back_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>13</td> <td>x-coordinate velocity of the torso</td> <td>torso</td> <td>free</td> <td>velocity (m/s)</td> </tr> <tr> <td>14</td> <td>y-coordinate velocity of the torso</td> <td>torso</td> <td>free</td> <td>velocity (m/s)</td> </tr> <tr> <td>15</td> <td>z-coordinate velocity of the torso</td> <td>torso</td> <td>free</td> <td>velocity (m/s)</td> </tr> <tr> <td>16</td> <td>x-coordinate angular velocity of the torso</td> <td>torso</td> <td>free</td> <td>angular velocity (rad/s)</td> </tr> <tr> <td>17</td> <td>y-coordinate angular velocity of the torso</td> <td>torso</td> <td>free</td> <td>angular velocity (rad/s)</td> </tr> <tr> <td>18</td> <td>z-coordinate angular velocity of the torso</td> <td>torso</td> <td>free</td> <td>angular velocity (rad/s)</td> </tr> <tr> <td>19</td> <td>angular velocity of the angle between torso and front left link</td> <td>hip_1 (front_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>20</td> <td>angular velocity of the angle between front left links</td> <td>ankle_1 (front_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>21</td> <td>angular velocity of the angle between torso and front right link</td> <td>hip_2 (front_right_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>22</td> <td>angular velocity of the angle between front right links</td> <td>ankle_2 (front_right_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>23</td> <td>angular velocity of the angle between torso and back left link</td> <td>hip_3 (back_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>24</td> <td>angular velocity of the angle between back left links</td> <td>ankle_3 (back_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>25</td> <td>angular velocity of the angle between torso and back right link</td> <td>hip_4 (right_back_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>26</td> <td>angular velocity of the angle between back right links</td> <td>ankle_4 (right_back_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> </tbody> </table> <p><em>Table: Observations space for the MuJoCo Ant-Maze task. Ranges for all values are -Inf to +Inf.</em></p> <p>The action space consists of 8 torque values applied to the rotors:</p> <table> <thead> <tr> <th>Num</th> <th>Action</th> <th>Name</th> <th>Joint</th> <th>Type (Unit)</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>Torque applied on the rotor between the torso and back right hip</td> <td>hip_4 (right_back_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>1</td> <td>Torque applied on the rotor between the back right two links</td> <td>angle_4 (right_back_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>2</td> <td>Torque applied on the rotor between the torso and front left hip</td> <td>hip_1 (front_left_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>3</td> <td>Torque applied on the rotor between the front left two links</td> <td>angle_1 (front_left_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>4</td> <td>Torque applied on the rotor between the torso and front right hip</td> <td>hip_2 (front_right_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>5</td> <td>Torque applied on the rotor between the front right two links</td> <td>angle_2 (front_right_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>6</td> <td>Torque applied on the rotor between the torso and back left hip</td> <td>hip_3 (back_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>7</td> <td>Torque applied on the rotor between the back left two links</td> <td>angle_3 (back_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> </tbody> </table> <p><em>Table: MuJoCo Ant agent action space. All values range between [-1, 1].</em></p> <p>Below is an illustration of the body and rotor layout used in our experiments:</p> <div class="l-body-outset"> <div class="row"> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/example_ant.png" alt="Example Mujoco Ant body." style="width: 100%; max-width: 300px;"> <figcaption>Example MuJoCo Ant body <d-cite key="towers2024gymnasium"></d-cite>.</figcaption> </figure> </div> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/ant_joints.png" alt="Rotor layout for Ant." style="width: 100%; max-width: 300px;"> <figcaption>Rotor layout for Ant <d-cite key="towers2024gymnasium"></d-cite>.</figcaption> </figure> </div> </div> </div> <h3 id="environment">Environment</h3> <p>In addition to training the agent to embody its quadruped ant, we train it for the AI Gymnasium task “Ant Maze” <d-cite key="towers2024gymnasium"></d-cite>. This task requires the agent to learn how to walk with its body then utilize its learned movements to maneuver around obstacles to reach a predetermined destination. This can be seen in pathing and exploration heat map figures throughout this paper.</p> <h3 id="adversarial-attack">Adversarial Attack</h3> <p>We rely on the Fast Gradient Sign Method (FGSM) <d-cite key="goodfellow2015explaining"></d-cite> as the baseline attack across our experiments. FGSM is a white box technique that perturbs inputs using the sign of the gradient from the victim model. The standard form is:</p> \[\mathbf{x}_{\text{adv}} = \mathbf{x} + \epsilon\,\mathrm{sign}\big(\nabla_{\mathbf{x}} J(\theta,\mathbf{x},y)\big)\] <ul> <li> <strong>$\mathbf{x}$</strong>: The original input provided to the model.</li> <li> <strong>$y$</strong>: The true (ground-truth) label associated with the input.</li> <li> <strong>$\epsilon$</strong>: A small scalar value that controls the magnitude of the perturbation applied to the input.</li> <li> <strong>$J(\theta, \mathbf{x}, y)$</strong>: The loss function, parameterized by model weights $\theta$, input $\mathbf{x}$, and label $y$.</li> <li> <strong>$\nabla_{\mathbf{x}} J(\theta, \mathbf{x}, y)$</strong>: The gradient of the loss function with respect to the input, indicating how changes in $\mathbf{x}$ affect the loss.</li> <li> <strong>$\mathrm{sign}(\cdot)$</strong>: The element-wise sign function that extracts the direction (positive or negative) of each gradient component.</li> <li> <strong>$\mathbf{x}_{\text{adv}}$</strong>: The resulting adversarial example, formed by perturbing the original input $\mathbf{x}$ to maximize the model’s prediction error.</li> </ul> <h4 id="modifying-fgsm">Modifying FGSM</h4> <p>FGSM is typically employed as an attack against image classifiers and perturbations are often in the form of pixel values. To get this to work in our case, we have to apply similar perturbations in the vector form matching our observation space. So we modify perturbations to represent values relative to which modality is being targeted (such as radians or meters per second).</p> <p>The <strong>Fast Gradient Sign Method (FGSM)</strong> is traditionally designed for <strong>supervised learning</strong> tasks, particularly in classification, where each input is paired with a corresponding label. In such cases, the attack relies on the loss function, which depends on the model parameters, input, and true label. However, applying FGSM directly to <strong>reinforcement learning (RL)</strong> models presents a unique challenge, as RL agents often don’t use labeled data.</p> <p>To address this, we reformulate FGSM in the following way. Instead of using a label-based loss, we leverage the <strong>critic’s Q-value</strong> as the optimization target. The modified FGSM equation is expressed as:</p> \[\mathbf{x}_{\text{adv}} = \mathbf{x} + \epsilon\,\mathrm{sign}\big(\nabla_{\mathbf{x}} Q_{\theta}(\mathbf{x}, a)\big)\] <p>Here, the <strong>Q-function</strong> from the critic network replaces the conventional loss function. This adaptation enables FGSM to generate adversarial perturbations that specifically target the <strong>policy’s valuation process</strong>, influencing how the agent perceives the consequences of its actions. By perturbing inputs based on the critic’s gradients, we effectively reorient the attack for any unsupervised evaluation the critic is capable of.</p> <h3 id="adversarial-defenses-1">Adversarial Defenses</h3> <p>Our evaluation includes three defense themes. We start with a disruption baseline that applies scaled gaussian noise to the observation vector. We then explore adversarial detection by training classifiers to flag perturbed observations and compare them with traditional clustering tools such as K Means and Gaussian Mixture Models (GMM). Finally, we assess a purification pipeline built on a defense VAE that reconstructs benign versions of the inputs.</p> <p>Both the neural network detectors and the clustering approaches rely on a dataset collected during SAC training and evaluation. The agent runs for three million steps. We treat the first one point five million steps as a warm up phase to avoid logging data from an agent that has not yet learned to move reliably. During the final 1.5 million steps we record observations into a benign dataset. We also generate a matching adversarial dataset by applying FGSM perturbations to those observations without feeding the altered signals back to the agent. The result is a paired corpus of benign and adversarial samples for every modality.</p> <h4 id="gaussian-noise-defense">Gaussian Noise Defense</h4> <p>The gaussian noise filter serves as the baseline defense. We sample perturbations from a normal distribution and scale them by $\epsilon$:</p> \[\mathbf{x_{def}} = x + \epsilon \cdot \mathbf{n}, \quad \text{where } \mathbf{n} \sim \mathcal{N}(0, \mathbf{I})\] <p>Targeted adversarial perturbations are often sensitive to small changes. By injecting a modest level of noise we can disrupt their structure and blunt the attack, accepting some degradation from the added randomness. This approach is simple and computationally inexpensive.</p> <h4 id="defense-vae">Defense VAE</h4> <p>Our defense VAE follows prior work on variational autoencoder purification <d-cite key="li2019defensevaefastaccuratedefense,shayanNDVAE"></d-cite>. The model trains on paired benign and adversarial samples like those described above. The encoder observes both forms, while the decoder learns to reconstruct the benign target. Over time the encode decode pathway maps adversarial inputs back into the benign observation space.</p> <p>We adapt the architecture from image defenses to match our one dimensional observation vector. The compact network uses four fully connected layers with ReLU activation rather than convolutional stacks. Since the modalities are not arranged as a sequence with positional structure, we also avoid one dimensional convolutions.</p> <h4 id="adversarial-detection">Adversarial Detection</h4> <p>Detection centric defenses focus on identifying an attack rather than intervening directly in the control loop. We therefore evaluate them on prediction accuracy and F1 score but do not alter the agent mid run. A production system could take many actions once an attack is detected, yet that follow up is outside the scope of this study.</p> <p>Using the labeled dataset we train Support Vector Machines (SVM), K Nearest Neighbors, and neural network classifiers to distinguish benign from adversarial observations. For comparison we also fit simpler clustering approaches such as K means and Gaussian Mixture Models to the same data.</p> <p>Classifier accuracy reflects binary predictions on benign versus adversarial inputs. For the unsupervised clustering methods we assign cluster labels to maximize accuracy after fitting the two cluster model.</p> <h3 id="adversarial-evaluation">Adversarial Evaluation</h3> <p>We use the following process to test the effects of adversarial attacks on different modalities of our baseline agent:</p> <ol> <li>The agent is trained normally on benign inputs.</li> <li>Every evaluation episode, the input is perturbed with an attack: First across both modalities simultaneously, then focused on only target modalities in the agent’s observation.</li> <li>Compare adversarial evaluation to benign training performance and identify effects of the attack on the agent.</li> </ol> <p>To better understand how defenses influence model performance, each defense method is tested under two different setups. In the first setup, the model is trained normally, and the defense is applied only during evaluation (filtering or otherwise “Defending” the input before it reaches the model). In the second setup, the model is also exposed to the defense during training by processing benign inputs through the same method before learning. The model is then attacked in the usual way. Comparing these two configurations reveals how much model tuning contributes to each defense’s effectiveness.</p> <h2 id="results">Results</h2> <h3 id="baseline-performance">Baseline Performance</h3> <p>We begin by training an SAC agent on the Ant Maze task for three million steps so that it reliably clears a simple obstacle. The configuration mirrors common SAC settings with a replay buffer of one million transitions, $\tau=0.05$, and $\gamma=0.99$.</p> <p>The figures below trace the training story. Rewards rise and level off after the run, the exploration heatmap shows how the agent learns an efficient route, and evaluation rewards peak once the policy stabilizes. The path visual confirms that the agent reaches the goal consistently.</p> <p>With no adversarial pressure this baseline agent handles the maze it was trained on.</p> <div class="l-page-outset"> <div class="row"> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/baseline_train.png" alt="SAC training reward." style="width: 100%; max-width: 400px;"> <figcaption>Training reward progression (3M steps).</figcaption> </figure> </div> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/baseline_explore.png" alt="Exploration heatmap." style="width: 100%; max-width: 400px;"> <figcaption>Exploration heatmap during learning.</figcaption> </figure> </div> </div> </div> <div class="l-page-outset"> <div class="row"> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/baseline_eval.png" alt="Evaluation reward." style="width: 100%; max-width: 400px;"> <figcaption>Evaluation performance plateaus after training.</figcaption> </figure> </div> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/baseline_paths.png" alt="Paths to goal." style="width: 100%; max-width: 400px;"> <figcaption>Typical paths from start to goal after training.</figcaption> </figure> </div> </div> </div> <h3 id="attacking-an-un-defended-model">Attacking an Un-Defended Model</h3> <p>With the modified FGSM attack ($\epsilon=0.005$) we perturb the observation vector during evaluation. The attack seeks the lowest value outcome predicted by the critic, pushing the policy toward poor decisions.</p> <h4 id="attacking-both-modalities">Attacking Both Modalities</h4> <p>When we perturb both modalities, reward traces reveal the story immediately. Sharp drops in the purple line show successful attacks, while quick recoveries indicate attack attempts that did not succeed. The path comparisons illustrate what those swings look like in the environment: a failed attack nudges the agent onto an alternate route, whereas a successful one leaves the agent wandering near the start.</p> <h4 id="attacking-individual-modalities">Attacking Individual Modalities</h4> <p>We then target each modality on its own with FGSM. The performance plot highlights how velocity (green) and angle (blue) perturbations differ from the full multimodal attack. Velocity covers roughly one quarter of the observation vector, so those attacks fail more often and produce more consistant reward. Angle perturbations cover more of the observation and are proportionally more effective.</p> <p>These results reinforce a practical takeaway: the share of the observation space tied to a modality limits how much damage an attacker can cause when only that modality is manipulated. However this has potential to change on different systems, if an agent is severely biased towards using a single modality.</p> <figure class="l-page"> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/fgsm_eval.png" alt="FGSM evaluation performance." style="width: 100%; max-width: 600px;"> <figcaption>Benign (red) vs adversarial (purple) evaluation. Sharp reward drops indicate successful attacks.</figcaption> </figure> <div class="l-page-outset"> <div class="row"> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/fgsm_path_success.png" alt="FGSM success example." style="width: 100%; max-width: 350px;"> <figcaption>FGSM attack success: agent gets lost early.</figcaption> </figure> </div> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/fgsm_path_fail.png" alt="FGSM failure example." style="width: 100%; max-width: 350px;"> <figcaption>FGSM attack failure: minimal route alteration.</figcaption> </figure> </div> </div> </div> <p>Modality-specific FGSM shows that attack effectiveness scales with the attacked fraction of the observation space.</p> <figure class="l-page"> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/individual_modality_fgsm.png" alt="Modality-specific FGSM results." style="width: 100%; max-width: 600px;"> <figcaption>Performance under FGSM for both modalities (purple), angles-only (blue), and velocity-only (green).</figcaption> </figure> <h3 id="attacking-a-defended-model">Attacking a Defended Model</h3> <p>With the baseline established, we introduce defenses to see how they reshape the interaction between attacker and agent.</p> <h4 id="gaussian-noise-defense-1">Gaussian Noise Defense</h4> <p>The gaussian noise filter is the simplest option. We add noise drawn from a normal distribution with mean zero and standard deviation one, scaled by 0.005.</p> <p>As shown in the noise defense results, even small amounts of injected noise can significantly disrupt the adversarial attack, leading to higher rewards during evaluation. Training the model with noise further improves its resilience, helping it adapt to the presence of randomness and reducing the attack’s overall success rate. In this setup, we evaluate two conditions: one where the model encounters noise only during evaluation, and another where it is pre-trained on noisy data.</p> <p>Both approaches, training with noise (orange line) and applying noise only during evaluation (red line), show clear improvements compared to the undefended model (purple line). The trained model in particular exhibits more frequent reward peaks and fewer sharp drops, indicating that controlled noise not only weakens the attacker’s influence but also enhances the model’s overall stability under adversarial conditions.</p> <p>When applying Gaussian noise defenses to attacks targeting specific modalities, an interesting shift in behavior emerges. For velocity-targeted attacks, the results closely mirror those of the full multimodal noise defense, showing that noise effectively mitigates the attack’s impact. However, when the angular modality is targeted, the pattern changes noticeably.</p> <p>In this case, training the model on noisy angular data appears to destabilize performance rather than improve it. The angular noise defense results show that the best-performing configuration occurs when noise is introduced only during evaluation, without exposing the model to it during training. The reason for this discrepancy remains unclear, it may be related to the angular modality’s dominant influence within the observation space, or potentially other underlying interactions in the model’s learning dynamics. Regardless, this behavior was observed consistently across multiple runs, suggesting a modality-specific sensitivity to noise based defenses.</p> <div class="l-page-outset"> <div class="row"> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/noise_defense_fgsm.png" alt="Noise defense under multimodal FGSM." style="width: 100%; max-width: 400px;"> <figcaption>Defense only (red), trained with noise (orange), undefended (purple).</figcaption> </figure> </div> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/angular_noise_defense_fgsm.png" alt="Noise defense for angle-only attacks." style="width: 100%; max-width: 400px;"> <figcaption>Angle-only attacks: defense at eval (purple) outperforms training-through-noise (green) and baseline (blue).</figcaption> </figure> </div> </div> </div> <h4 id="defense-vae-1">Defense-VAE</h4> <p>The compact Defense VAE offers another perspective. Running it only during evaluation lifts rewards under FGSM, but training the agent on VAE filtered inputs prevents the policy from solving the task. Single modality attacks expose its weaknesses: the VAE performs best when both modalities are perturbed together (as seen with the blue line in the figure below) and struggles with narrow attacks.</p> <p>This pattern alligns with how the VAE learned. Training on paired benign and adversarial samples covering the entire observation space primes it to recognize broad perturbations. Once the attacker focuses on a single modality the reconstruction no longer matches the training distribution, so protection fades.</p> <div class="l-page-outset"> <div class="row"> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/vae_both_modalities.png" alt="VAE defense under multimodal FGSM." style="width: 100%; max-width: 400px;"> <figcaption>VAE defense (blue) vs undefended adversarial inputs (purple) at $\epsilon=0.007$.</figcaption> </figure> </div> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/vae_individual_modalities.png" alt="VAE defense under modality-specific FGSM." style="width: 100%; max-width: 400px;"> <figcaption>VAE under both-modalities (blue), velocity-only (red), angle-only (orange) attacks.</figcaption> </figure> </div> </div> </div> <h4 id="detection-results-summary">Detection Results (Summary)</h4> <p>The classifier roundup shows a clear trend: higher $\epsilon$ values (stronger perturbations) make detection easier. Angular perturbations are more detectable in general, and stand out at $\epsilon=0.007$, while velocity attacks become more visible at $\epsilon=0.015$. However clustering baselines such as K means and GMM hover near chance overall.</p> <p>The broader lesson is that modality combinations shift detection difficulty, and scaling the attack changes that balance even when success rates stay similar.</p> <p>The detailed results for all detection methods are shown in the table below:</p> <table> <thead> <tr> <th>Detection Method</th> <th>Multi-modal</th> <th> </th> <th> </th> <th>Velocity</th> <th> </th> <th> </th> <th>Angular</th> <th> </th> <th> </th> </tr> </thead> <tbody> <tr> <td> </td> <td>Accuracy</td> <td>F1 Score</td> <td>Epsilon</td> <td>Accuracy</td> <td>F1 Score</td> <td>Epsilon</td> <td>Accuracy</td> <td>F1 Score</td> <td>Epsilon</td> </tr> <tr> <td>SVM</td> <td>0.6027</td> <td>0.6936</td> <td>0.007</td> <td>0.4993</td> <td>0.6227</td> <td>0.007</td> <td>0.5606</td> <td><strong>0.6596</strong></td> <td>0.007</td> </tr> <tr> <td>SVM</td> <td>0.7188</td> <td>0.7608</td> <td>0.015</td> <td>0.6070</td> <td>0.7012</td> <td>0.015</td> <td>0.5048</td> <td>0.3782</td> <td>0.015</td> </tr> <tr> <td>KNN</td> <td>0.5909</td> <td>0.5897</td> <td>0.007</td> <td>0.5164</td> <td>0.4718</td> <td>0.007</td> <td>0.5391</td> <td>0.5412</td> <td>0.007</td> </tr> <tr> <td>KNN</td> <td>0.6606</td> <td>0.6522</td> <td>0.015</td> <td>0.5850</td> <td>0.5769</td> <td>0.015</td> <td>0.5152</td> <td>0.3962</td> <td>0.015</td> </tr> <tr> <td>NN</td> <td>0.7349</td> <td>0.725</td> <td>0.007</td> <td>0.5494</td> <td>0.6437</td> <td>0.007</td> <td>0.7104</td> <td>0.7372</td> <td>0.007</td> </tr> <tr> <td>NN</td> <td><strong>0.9892</strong></td> <td><strong>0.9892</strong></td> <td>0.015</td> <td><strong>0.8766</strong></td> <td><strong>0.8810</strong></td> <td>0.015</td> <td><strong>0.8211</strong></td> <td>0.8264</td> <td>0.015</td> </tr> <tr> <td>GMM</td> <td>0.4989</td> <td>N/A</td> <td>0.007</td> <td>0.4984</td> <td>N/A</td> <td>0.007</td> <td>0.4996</td> <td>N/A</td> <td>0.007</td> </tr> <tr> <td>GMM</td> <td>0.5094</td> <td>N/A</td> <td>0.015</td> <td>0.4976</td> <td>N/A</td> <td>0.015</td> <td>0.5021</td> <td>N/A</td> <td>0.015</td> </tr> <tr> <td>Kmeans</td> <td>0.5033</td> <td>N/A</td> <td>0.007</td> <td>0.4998</td> <td>N/A</td> <td>0.007</td> <td>0.5026</td> <td>N/A</td> <td>0.007</td> </tr> <tr> <td>Kmeans</td> <td>0.5416</td> <td>N/A</td> <td>0.015</td> <td>0.4602</td> <td>N/A</td> <td>0.015</td> <td>0.4855</td> <td>N/A</td> <td>0.015</td> </tr> </tbody> </table> <p><em>Table: Classifier performance across modalities with FGSM epsilon (scaling) values of 0.007 and 0.015.</em></p> <h2 id="conclusions">Conclusions</h2> <p>Our experiments confirm that a multimodal agent can be pushed off course through attacks on any of its inputs. The larger a modality’s footprint in the observation vector, the more influence an attacker gains. Defenses shape those dynamics in different ways: gaussian noise offers broad value with modality specific caveats, and the Defense VAE excels when the attack distribution matches its training data, even though the VAE is supposed to generalize well against new attack types/methods, it does not generalize as well across modalities.</p> <p>These findings underline the importance of viewing reinforcement learning robustness through a modality aware lens. Simple interventions already reveal nuanced behavior, and the pipeline we release provides a starting point for deeper explorations.</p> <p>The baseline setup here is intentionally lightweight, yet it highlights clear research directions for more complex agents and environments. We hope the insights and tools accelerate progress on safeguarding the next wave of multimodal systems.</p> <h2 id="reproducibility-details">Reproducibility Details</h2> <p>Experiments used SAC with 3M steps, replay size 1e6, $\tau=0.05$, $\gamma=0.99$, evaluation every 100 steps, and Adam learning rates as in our code. Defense-VAE used fully connected enc/dec layers (256-128-64 latent-64-128-256) with latent size 24, trained for 50 epochs. Classifier details and additional hyperparameters are available in the appendix of the paper and our codebase.</p> <h3 id="sac-hyperparameters">SAC Hyperparameters</h3> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Horizon Length</td> <td>1</td> </tr> <tr> <td>Memory Size</td> <td>$1 \times 10^6$</td> </tr> <tr> <td>Batch Size</td> <td>4096</td> </tr> <tr> <td>$N$-step</td> <td>1</td> </tr> <tr> <td>$\tau$ (Target smoothing coefficient)</td> <td>0.05</td> </tr> <tr> <td>$\gamma$ (Discount factor)</td> <td>0.99</td> </tr> <tr> <td>Warm-up Steps</td> <td>32</td> </tr> <tr> <td>Critic Class</td> <td>Double Q-Network</td> </tr> <tr> <td>Evaluation Frequency</td> <td>100</td> </tr> <tr> <td>Learning Rate (Alpha)</td> <td>0.005</td> </tr> <tr> <td>Update Times per Step</td> <td>8</td> </tr> </tbody> </table> <h3 id="vae-hyperparameters">VAE Hyperparameters</h3> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Optimizer</td> <td>Adam</td> </tr> <tr> <td>Learning Rate</td> <td>$1 \times 10^{-3}$</td> </tr> <tr> <td>Batch Size</td> <td>32</td> </tr> <tr> <td>Training Epochs</td> <td>50</td> </tr> <tr> <td>Latent Dimension</td> <td>24</td> </tr> <tr> <td>Encoder Layers</td> <td>5 (Observation, 256, 128, 64, Latent Dimension)</td> </tr> <tr> <td>Decoder Layers</td> <td>5 (Latent Dimension, 64, 128, 256, Observation)</td> </tr> </tbody> </table> <h3 id="neural-network-hyperparameters">Neural Network Hyperparameters</h3> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Input Dimension</td> <td>28</td> </tr> <tr> <td>Output Dimension</td> <td>1</td> </tr> <tr> <td>Hidden Layers</td> <td>5 (256, 256, 128, 32, 1)</td> </tr> <tr> <td>Learning Rate</td> <td>0.0001</td> </tr> <tr> <td>Training Epochs</td> <td>60</td> </tr> </tbody> </table> <h3 id="support-vector-machine-svm-hyperparameters">Support Vector Machine (SVM) Hyperparameters</h3> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>$C$</td> <td>1000</td> </tr> <tr> <td>Gamma</td> <td>1</td> </tr> <tr> <td>Degree</td> <td>3</td> </tr> <tr> <td>Decision Function</td> <td>One-vs-One (ovo)</td> </tr> </tbody> </table> <h3 id="gmm-hyperparameters">GMM Hyperparameters</h3> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Number of Components</td> <td>2</td> </tr> <tr> <td>Initialization Method</td> <td>k-means++</td> </tr> <tr> <td>Covariance Type</td> <td>full</td> </tr> <tr> <td>Convergence Tolerance ($\texttt{tol}$)</td> <td>0.001</td> </tr> <tr> <td>Regularization of Covariance ($\texttt{reg_covar}$)</td> <td>$1 \times 10^{-6}$</td> </tr> <tr> <td>Max Iterations</td> <td>100</td> </tr> <tr> <td>Number of Initializations ($\texttt{n_init}$)</td> <td>1</td> </tr> </tbody> </table> <h3 id="k-means">K-Means</h3> <p>K-Means clustering algorithm as implemented by Scikit-learn, with a k value of 2 clusters.</p> <d-appendix> <d-footnote> We release a lightweight pipeline adapted from DDiffPG <d-cite key="li2024learningmultimodalbehaviorsscratch"></d-cite> to reproduce training, dataset generation, attacks, and defenses. </d-footnote> </d-appendix> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/blog/assets/bibliography/2025-10-08-multimodalEmergingPatterns.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"UniReps/unireps.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 UniReps Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>