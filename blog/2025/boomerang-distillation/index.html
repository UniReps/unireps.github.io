<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Boomerang Distillation Enables Zero-Shot Model Size Interpolation | UniReps Blog </title> <meta name="author" content="UniReps Blog"> <meta name="description" content="Real-world deployments of LLMs require models of different sizes to meet performance, latency, and cost targets. Yet pretraining every size is prohibitively expensive, leaving large gaps in size-performance curves. We identify a novel phenomenon, Boomerang Distillation, which occurs when distilling a large language model into a smaller one. In this blog post, we describe how Boomerang Distillation can be used to create entire families of LLMs of fine-grained sizes from a single student-teacher pair without any additional training."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/blog/assets/img/unireps_favicon.png?3a42502b41deab62714411b8479222c3"> <link rel="stylesheet" href="/blog/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unireps.org//blog/2025/boomerang-distillation/"> <script src="/blog/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/blog/assets/js/distillpub/template.v2.js"></script> <script src="/blog/assets/js/distillpub/transforms.v2.js"></script> <script src="/blog/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Boomerang Distillation Enables Zero-Shot Model Size Interpolation",
            "description": "Real-world deployments of LLMs require models of different sizes to meet performance, latency, and cost targets. Yet pretraining every size is prohibitively expensive, leaving large gaps in size-performance curves. We identify a novel phenomenon, Boomerang Distillation, which occurs when distilling a large language model into a smaller one. In this blog post, we describe how Boomerang Distillation can be used to create entire families of LLMs of fine-grained sizes from a single student-teacher pair without any additional training.",
            "published": "October 31, 2025",
            "authors": [
              
              {
                "author": "Sara Kangaslahti",
                "authorURL": "https://skangasl.github.io/",
                "affiliations": [
                  {
                    "name": "Harvard University",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Nihal Nayak",
                "authorURL": "https://nihalnayak.github.io/",
                "affiliations": [
                  {
                    "name": "Harvard University",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Jonathan Geuter",
                "authorURL": "https://j-geuter.github.io/",
                "affiliations": [
                  {
                    "name": "Kempner Institute, Harvard University",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Marco Fumero",
                "authorURL": "https://gladia.di.uniroma1.it/authors/fumero/",
                "affiliations": [
                  {
                    "name": "Institute of Science and Technology Austria",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Francesco Locatello",
                "authorURL": "https://www.francescolocatello.com/",
                "affiliations": [
                  {
                    "name": "Institute of Science and Technology Austria",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "David Alvarez-Melis",
                "authorURL": "https://dmelis.github.io/",
                "affiliations": [
                  {
                    "name": "Kempner Institute, Harvard University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog//"> <span class="font-weight-bold">UniReps</span> Blog </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Boomerang Distillation Enables Zero-Shot Model Size Interpolation</h1> <p>Real-world deployments of LLMs require models of different sizes to meet performance, latency, and cost targets. Yet pretraining every size is prohibitively expensive, leaving large gaps in size-performance curves. We identify a novel phenomenon, Boomerang Distillation, which occurs when distilling a large language model into a smaller one. In this blog post, we describe how Boomerang Distillation can be used to create entire families of LLMs of fine-grained sizes from a single student-teacher pair without any additional training.</p> </d-title> <d-byline></d-byline> <d-article> <p>Today’s large language models power everything from chatbots on your phone to massive AI systems running in data centers. But not all devices can handle the same model sizes: a model that’s fast on a GPU cluster might be unusable on a laptop. AI model developers try to solve this by releasing “model families” consisting of different-sized versions of the same model. For example, Llama3 models come in sizes ranging from 1 billion to 70 billion parameters. Yet training each size from scratch is expensive, leaving big gaps between available models. What if we could fill those gaps <em>without any extra training</em>? That’s exactly what we explore with <strong>Boomerang Distillation</strong>, a new way to “recombine” parts of large and small models to create many intermediate sizes – all from a single training run.</p> <hr> <h2 id="current-approaches-for-training-llm-families-are-computationally-expensive">Current Approaches for Training LLM Families are Computationally Expensive</h2> <p>As training each model size from scratch is very computationally intensive, many modern LLM families start with one large pretrained model (the teacher) and distill it into smaller ones (the students). This procedure is called knowledge distillation. Typically, the student models learn with the usual next-token prediction objective, plus extra losses that make them imitate the teacher’s behavior (e.g. KL divergence and cosine distance). Distillation is more compute-friendly than training every model without a teacher, but it still requires training each model independently on up to a trillion tokens. This expensive process limits how many models developers can release, so we typically end up with a small set tuned for common GPU setups. Meanwhile, practitioners need models tailored to <em>their</em> hardware and compute budgets. Unless they train a new model themselves, they’re limited to a few prebuilt options, leaving large gaps in the trade-off between model compute and performance (Figure 1).</p> <div class="l-page-outset"> ![Landscape of pretrained families](/assets/img/2025-10-31-boomerang-distillation/model_sizes_new.jpeg "Figure 1") </div> <p>Figure 1: The landscape of pretrained LLM families. There are large gaps in size between available LLMs. Figure credit to <a href="https://qwen.ai/research" rel="external nofollow noopener" target="_blank">Qwen research</a></p> <hr> <h2 id="boomerang-distillation-creating-multiple-models-for-the-price-of-one">Boomerang distillation: creating multiple models for the price of one</h2> <p>Given the limitations of current approaches, how can we efficiently create models of different sizes? We show that surprisingly, distillation is not just useful for training good student models – with the right setup, we can mix and match parts of the teacher and student models to build intermediate models that smoothly trade off size and performance! We call this phenomenon <em>boomerang distillation</em>: starting with a large teacher, we distill a single smaller student, and then “boomerang” back toward the teacher by selectively swapping in teacher components, creating many models of intermediate size <strong>without any additional training</strong>.</p> <p>Intuitively, boomerang distillation works because we encourage each student layer in the distilled model to approximate the function of some block (contiguous set) of teacher layers. In this setup, each student layer can be thought of as a compact summary of one or more teacher layers. Then, swapping out a layer in the student and replacing it with its corresponding block of teacher layers produces a larger model with a <em>better approximation</em> of what the student is trying to compute. Each swap increases model size and improves similarity to the teacher, producing a new, usable model with no extra training required.</p> <p>Boomerang distillation consists of three key steps: (1) student initialization, (2) knowledge distillation, and (3) student patching (Figure 2). We explain each of these steps in detail below.</p> <div class="l-page-outset"> ![Overview of boomerang distillation](/assets/img/2025-10-31-boomerang-distillation/size_interpolation_v4.jpg "Figure 2") </div> <p>Figure 2: Overview of boomerang distillation. ➀ The student model is initialized by copying layers from the pretrained teacher model. ➁ The teacher model is distilled into the student model with cross-entropy loss, knowledge distillation loss, and cosine distance loss. ➂ After training the student model, a block of teacher layers corresponding to a student layer is inserted back into the model to get the interpolated intermediate model.</p> <h3 id="step-1-student-initialization">Step 1: Student initialization</h3> <p>We start aligning student layers to teacher blocks (a “block” is one or more contiguous teacher layers) by initializing the student with layers copied from the teacher (Figure 2, left). This creates a clean mapping: every student layer should stand in for a specific teacher block.</p> <p>For example, in Figure 2, we copy layers 1, 3, and 5 from the teacher to create the student. Then,</p> <ul> <li>The first student layer <strong>S1</strong> maps to teacher layers <strong>T1</strong> and <strong>T2</strong> </li> <li>The second student layer <strong>S2</strong> maps to teacher layers <strong>T3</strong> and <strong>T4</strong> </li> <li>The third student layer <strong>S3</strong> maps to teacher layer <strong>T5</strong> </li> </ul> <p>Copying gives us a strong head start, since each student layer behaves like the first layer in its corresponding teacher block.</p> <h3 id="step-2-knowledge-distillation">Step 2: Knowledge distillation</h3> <p>Once we have the student initialized, we train the model using knowledge distillation (Figure 2 center). To improve student performance, we use cross entropy loss for next token prediction and KL divergence loss to make the student’s predicted probabilities match those of the teacher. We also add a cosine distance loss between the outputs of the student layers and their corresponding teacher blocks.</p> <p>Continuing the example from Figure 2 above, we align</p> <ul> <li> <strong>S1</strong> with <strong>T2</strong>’s output (for the T1-T2 block)</li> <li> <strong>S2</strong> with <strong>T4</strong> (for T3-T4)</li> <li> <strong>S3</strong> with <strong>T5</strong> (for T5)</li> </ul> <p>This layer-to-block alignment trains each student layer to mimic the computations in its mapped teacher block.</p> <h3 id="step-3-student-patching">Step 3: Student patching</h3> <p>After distillation, we can assemble models of many sizes by patching student layers, i.e., replacing any student layer with its corresponding block of teacher layers (Figure 2, right). Because we enforced layer-wise alignment in Steps 1 and 2, each swap creates an intermediate model that <em>better approximates</em> the teacher.</p> <p>In our example from Figure 2, we can choose any subset of swaps:</p> <ul> <li>Replace <strong>S1</strong> with <strong>T1-T2</strong> </li> <li>Replace <strong>S2</strong> with <strong>T3-T4</strong> </li> <li>Replace <strong>S3</strong> with <strong>T5</strong> </li> </ul> <p>Choosing different combinations of swaps yields intermediate models of different sizes, giving a smooth menu of compute/accuracy trade-offs <strong>without additional training</strong>.</p> <hr> <h2 id="boomerang-distillation-interpolates-between-student-and-teacher-performance">Boomerang distillation interpolates between student and teacher performance</h2> <h3 id="what-are-the-necessary-conditions-for-boomerang-distillation-to-succeed">What are the necessary conditions for boomerang distillation to succeed?</h3> <p>To probe when boomerang distillation works, we run two simple stress tests. First, we study whether a student that’s randomly initialized (instead of copying layers from the teacher) could still yield useful intermediate models. Second, we try <em>naive pruning</em>: initialize the student from teacher layers, but skip the distillation step entirely.</p> <p>We find that across multiple model families and sizes, boomerang distillation creates intermediate models whose <strong>performance interpolates smoothly</strong> between the student and teacher (Figures 3 and 4). In contrast, both baselines’ performance sharply decreases with model size. This shows that <strong>both</strong> ingredients – the right initialization <em>and</em> knowledge distillation – matter. Leave out either step and the boomerang effect largely disappears.</p> <div class="l-page-outset"> ![Boomerang distillation produces models with smooth size–performance interpolation](/assets/img/2025-10-31-boomerang-distillation/qwen_layer_dropping.jpg "Figure 3") </div> <p>Figure 3: Boomerang distillation produces models with smooth size–performance interpolation, consistently outperforming naive layer pruning and interpolation from randomly initialized distilled models.</p> <div class="l-page-outset"> ![Boomerang distillation emerges across model families](/assets/img/2025-10-31-boomerang-distillation/all_classification_results.jpg "Figure 4") </div> <p>Figure 4: Boomerang distillation emerges across model families like Qwen, Pythia, and Llama.</p> <p>We also test which parts of the distillation objective are needed for boomerang distillation. To do so, we train four students with ablated loss terms and then interpolate from them. In Figure 5, we find that for most model sizes, leaving out the per-layer cosine distance (the green and purple lines) does not meaningfully reduce interpolation performance. This suggests that the initialization in Step 1 already provides enough alignment for boomerang distillation to work reasonably well without explicitly training every intermediate layer to match the teacher. That said, students distilled with the per-layer cosine term show more stable interpolation than those without. Looking ahead, we are interested in understanding whether we can keep that stability without explicit layer-wise alignment during distillation, because removing the need to keep the teacher in memory would significantly reduce the GPU footprint of training the student.</p> <div class="l-page-outset"> ![Per-layer loss yields stable and smoother interpolation performance](/assets/img/2025-10-31-boomerang-distillation/qwen_loss_type.jpg "Figure 5") </div> <p>Figure 5: Per-layer loss yields stable and smoother interpolation performance.</p> <h3 id="how-good-is-boomerang-distillation">How good is boomerang distillation?</h3> <h4 id="comparison-to-standard-distilled-models">Comparison to standard distilled models</h4> <p>Now that we have interpolated models from boomerang distillation, we compare them to same-size models trained with a standard distillation objective. For an apples-to-apples test, we use the same initialization and distillation setup as the small student model.</p> <p>We find that boomerang distillation creates intermediate models with performance on par with standard distilled models of the same size, even outperforming them at larger sizes (Figure 6). In practice, this means we only need to distill a <em>single student</em> to get a full lineup of intermediate models that perform similarly to models individually distilled with the same token budget. These interpolated models also stack up well against pretrained models like Pythia-2.8B and Llama-3.2-3B, which train on far more tokens than the student.</p> <p>We also observe that knowledge distillation can hurt models like Qwen at larger sizes (toward the right of Figure 6), likely because they originate from proprietary, high-quality data; retraining on public data (of presumably lower quality) can cause a drop in performance. With boomerang distillation, we mitigate this issue because we interpolate directly between the student and the teacher.</p> <div class="l-page-outset"> ![Interpolated models have comparable performance to standard distilled models](/assets/img/2025-10-31-boomerang-distillation/qwen_versus_distilled.jpg "Figure 6") </div> <p>Figure 6: Interpolated models produced using boomerang distillation have comparable performance to pretrained and standard distilled models.</p> <h4 id="comparison-to-layer-pruning">Comparison to layer pruning</h4> <p>How does boomerang distillation compare to smart layer pruning methods? Layer Collapse (LaCo)<d-cite key="yang2024laco"></d-cite> and ShortGPT<d-cite key="men2024shortgpt"></d-cite> are popular approaches that look for redundancy in a transformer and drop entire layers to shrink the model without training.</p> <p>In practice, boomerang distillation exhibits much better performance. As Figure 7 shows, its interpolated models consistently outperform layer-pruned models of the same size, and the gap widens as models get smaller. This is likely because boomerang distillation blends information from <strong>both</strong> the distilled student and the original teacher, so the intermediate models can use information from both models. Pruning, by contrast, compresses only the teacher; once you shrink far below the teacher’s size, quality tends to fall off.</p> <div class="l-page-outset"> ![Boomerang distillation outperforms layer pruning methods](/assets/img/2025-10-31-boomerang-distillation/qwen_pruning_method_comparison.jpg "Figure 7") </div> <p>Figure 7: Boomerang distillation performs significantly better than layer pruning methods.</p> <hr> <h2 id="future-directions">Future directions</h2> <p>We introduce boomerang distillation, a simple recipe for training-free creation of intermediate-sized language models: start with a big teacher model, distill a compact student, then mix and match student and teacher layers to build models that smoothly scale in size and performance. We are excited by the potential of boomerang distillation to shift how developers create model families. Instead of training many models from scratch, they can simply <strong>distill one student and then interpolate</strong>, swapping in teacher blocks as needed to produce a full lineup that covers different accuracy-latency targets. This opens the door to finer-grained LLM customization for real-world constraints. Looking ahead, extending these ideas to pretraining-scale token budgets and into other domains (such as vision) can build model families tailored to a wide range of deployment settings.</p> <hr> <p>This blog is adapted from our paper <a href="https://arxiv.org/abs/2510.05064" rel="external nofollow noopener" target="_blank">Boomerang Distillation Enables Zero-Shot Model Size Interpolation</a>. This blog post was first published on the <em>Deeper Learning</em> blog at the Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University.</p> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/blog/assets/bibliography//assets/bibliography/2025-10-31-boomerang-distillation.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"UniReps/unireps.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 UniReps Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>