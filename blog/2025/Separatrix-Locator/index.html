<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Separatrix Locator | UniReps Blog </title> <meta name="author" content="UniReps Blog"> <meta name="description" content="Finding Separatrices with Deep squashed Koopman Eigenfunctions"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/blog/assets/img/unireps_favicon.png?3a42502b41deab62714411b8479222c3"> <link rel="stylesheet" href="/blog/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unireps.org//blog/2025/Separatrix-Locator/"> <script src="/blog/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/blog/assets/js/distillpub/template.v2.js"></script> <script src="/blog/assets/js/distillpub/transforms.v2.js"></script> <script src="/blog/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}.equation-with-plot{position:relative;display:inline-block;cursor:help}.equation-with-plot .plot-tooltip{visibility:hidden;opacity:0;position:absolute;bottom:100%;left:50%;transform:translateX(-50%);background:white;border:2px solid #333;border-radius:8px;padding:0;box-shadow:0 4px 12px rgba(0,0,0,0.3);z-index:1000;transition:opacity .3s,visibility .3s;margin-bottom:10px;width:420px;height:320px}.equation-with-plot:hover .plot-tooltip{visibility:visible;opacity:1}.plot-tooltip iframe{display:block;width:100%;height:100%;border:0;border-radius:8px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Separatrix Locator",
            "description": "Finding Separatrices with Deep squashed Koopman Eigenfunctions",
            "published": "September 28, 2025",
            "authors": [
              
              {
                "author": "Kabir V. Dabholkar",
                "authorURL": "https://kabirdabholkar.github.io",
                "affiliations": [
                  {
                    "name": "Faculty of Mathematics, Technion",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Omri Barak",
                "authorURL": "https://barak.net.technion.ac.il",
                "affiliations": [
                  {
                    "name": "Rappaport Faculty of Medicine and Network Biology Research Laboratories, Technion",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog//"> <span class="font-weight-bold">UniReps</span> Blog </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Separatrix Locator</h1> <p>Finding Separatrices with Deep squashed Koopman Eigenfunctions</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#tl-dr">TL;DR</a> </div> <div> <a href="#introduction-fixed-points-and-beyond">Introduction: Fixed Points and Beyond</a> </div> <ul> <li> <a href="#finding-fixed-points">Finding Fixed Points</a> </li> <li> <a href="#setting">Setting</a> </li> </ul> <div> <a href="#the-sandwich-of-bistability">The Sandwich of Bistability</a> </div> <div> <a href="#squashed-koopman-eigenfunctions">(squashed) Koopman Eigenfunctions</a> </div> <div> <a href="#enter-deep-neural-networks">Enter Deep Neural Networks</a> </div> <div> <a href="#does-it-work">Does It Work?</a> </div> <div> <a href="#summary-and-outlook">Summary and Outlook</a> </div> </nav> </d-contents> <h2 id="tldr">TL;DR</h2> <p>Separatrices! These are boundaries between basins of attraction in dynamical systems. In high-dimensional systems like Recurrent Neural Networks, finding these boundaries can help reverse engineer their mechanism, or design optimal perturbations. But finding them is far from trivial. We recently developed a numerical method, based on approximating a Koopman Eigenfunction (KEF) of the dynamics using a deep neural network (DNN) <d-cite key="dabholkar_finding_2025"></d-cite>. While this approach works, these KEFs suffer from singularities at attractors, which makes them difficult targets for DNNs to approximate. In this blogpost we explain our original method, and also improve it by using a variant we call <em>squashed Koopman Eigenfunctions</em> (sKEFs), which alleviate the singularities. We show how they are linked to KEFs and replicate our results from the paper.</p> <p><strong>Code</strong>: We provide a Python package implementing this method at <a href="https://github.com/KabirDabholkar/separatrix_locator" rel="external nofollow noopener" target="_blank">github.com/KabirDabholkar/separatrix_locator</a>.</p> <p><strong>Interactive version</strong>: if the interactive plots below don’t work click <a href="https://unireps.github.io/blog/2025/Separatrix-Locator/" rel="external nofollow noopener" target="_blank">here</a>.</p> <h2 id="introduction-fixed-points-and-beyond">Introduction: Fixed Points and Beyond</h2> <p>Many natural and artificial systems — from neural circuits making decisions to ecosystems switching between healthy and diseased states — are modelled as <strong>multistable dynamical systems</strong>. Their behaviour is governed by multiple <strong>attractors</strong> in state space, each corresponding to a stable mode of activity. Understanding these systems often boils down to understanding their <strong>geometry</strong>: where are the stable states, and how are the different basins of attraction separated?</p> <p>For the last decade, a workhorse of neural circuit analysis has been <strong>fixed point analysis</strong>. By finding points where the flow vanishes and linearising around them, one can uncover local motifs underlying computation: line attractors, saddle points, limit cycles, and so on. This has yielded powerful insights into how trained RNNs implement cognitive tasks.</p> <h3 id="finding-fixed-points">Finding Fixed Points</h3> <p>First consider a bistable dynamical system in 2 dimensions. Below is a phase-portrait of such a system, with two stable fixed points and one unstable fixed point. Click on plot to realise trajectories of the dynamics.</p> <p><a href="https://unireps.github.io/blog/2025/Separatrix-Locator/" rel="external nofollow noopener" target="_blank">(click here if the plot below doesn’t load)</a></p> <div class="l-body" style="text-align: center; margin: 2rem 0;"> <iframe src="/blog/assets/html/2025-09-29-Separatrix/clickable_phase_portrait_simple.html" scrolling="no" style="width: 80%; height: 400px; border: none; border-radius: 8px; overflow: hidden;"> </iframe> </div> <p>Trajectories converge to either one of the two fixed points. This naturally suggests an algorithm: run the dynamics from many initial conditions to find the stable fixed points.</p> <p>Now try to click somewhere that will lead you exactly to the saddle point. Did you succeed? It’s almost impossible.</p> <p>This motivates developing a principled way to find such points. One solution is to define a specific scalar function of the dynamics whose only minima are given by all the fixed points. One such function is the kinetic energy \(q(\boldsymbol x)=\frac{1}{2}\Vert f(\boldsymbol x)\Vert^2\) <d-cite key="sussillo_opening_2013,golub_fixedpointfinder_2018"></d-cite>. By differentiating this function, one can perform gradient descent to find these minima. The interactive plot below realises such trajectories.</p> <p><a href="https://unireps.github.io/blog/2025/Separatrix-Locator/" rel="external nofollow noopener" target="_blank">(click here if the plot below doesn’t load)</a></p> <div class="l-body" style="text-align: center; margin: 2rem 0;"> <iframe src="/blog/assets/html/2025-09-29-Separatrix/gradient_descent_phase_portrait.html" scrolling="no" style="width: 80%; height: 400px; border: none; border-radius: 8px; overflow: hidden;"> </iframe> </div> <p>Now we can find both stable <em>and unstable</em> fixed points. Linearising around the fixed points provides an interpretable approximation of the dynamics in the neighbourhood of those points. Several works adopt this approach of fixed point finding to reverse-engineer either task-trained or data-trained RNNs <d-cite key="carnevale_dynamic_2015,maheswaranathan_reverse_2019,maheswaranathan_universality_2019,finkelstein_attractor_2021,mante_context-dependent_2013,liu_encoding_2024,driscoll_flexible_2024,chaisangmongkon_computing_2017,jaffe_modelling_2023,pagan_individual_2025,wang_flexible_2018"></d-cite>.</p> <p>But fixed points are only half the story.</p> <p>When a system receives a perturbation — for example, a sensory input or an optogenetic pulse — the key question is often not <em>where</em> it started, but <em>which side of the separatrix it ends up on</em>. The <strong>separatrix</strong> is the boundary in state space separating different basins of attraction. Crossing it means switching decisions, memories, or ecological states. Failing to cross means staying put. For high-dimensional systems, these boundaries are typically <strong>nonlinear, curved hypersurfaces</strong>, invisible to fixed points and local linearisations around them.</p> <blockquote> <p><strong>What if we could learn a single smooth scalar function whose zero level set <em>is</em> the separatrix?</strong></p> </blockquote> <p>Below is an example of such a function that we constructed for this simple system (click on it to run trajectories).</p> <p><a href="https://unireps.github.io/blog/2025/Separatrix-Locator/" rel="external nofollow noopener" target="_blank">(click here if the plot below doesn’t load)</a></p> <div class="l-body" style="text-align: center; margin: 2rem 0;"> <iframe src="/blog/assets/html/2025-09-29-Separatrix/absolute_value_gradient_descent.html" scrolling="no" style="width: 80%; height: 400px; border: none; border-radius: 8px; overflow: hidden;"> </iframe> </div> <p>Our main contribution is a numerical method to approximate such functions using deep neural networks in order to find separatrices in multistable dynamical systems in high-dimensions.</p> <h3 id="setting">Setting</h3> <p>We consider autonomous dynamical flows of the form:</p> \[\begin{equation} \dot{\boldsymbol{x}} = f(\boldsymbol{x}) \label{eq:ODE} \end{equation}\] <p>governing the state \(\boldsymbol{x} \in \mathbb R^N\), where \(\dot \square\) is shorthand for the time derivative \(\frac{d}{dt}\square\) and \(f: \mathbb R^N \to \mathbb R^N\) defines the dynamical flow.</p> <blockquote class="goal-box" id="goal"> <p><strong>The Goal:</strong><br> Find a smooth scalar function \(\psi:\mathbb{R}^N\to\mathbb{R}\) that grows as we move away from the separatix, i.e., \(\psi(\boldsymbol x)=0\) for \(x\in\text{separatrix}\) and grows as it moves away from the separatrix.</p> </blockquote> <h2 id="the-sandwich-of-bistability">The Sandwich of Bistability</h2> <p>Any bistable system can be decomposed as follows: it will have two attractors, their respective basins of attraction and the separatrix between them. This is like a cheese sandwich: the attractors are slices of bread, and the separatrix is the slice of cheese between them. We can call this the <strong>Sandwich of Bistability</strong>. In general, this sandwich could be arbitrarily oriented in \(\mathbb R^N\) and even nonlinearly warped.</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-09-29-Separatrix/sandwich_of_bistability.png" alt="Sandwich of Bistability" width="500"> <div style="max-width: 500px; margin: 0.5rem auto; text-align: center;"> <em>The Sandwich of Bistability: Two attractors and their basins of attraction (bread slices) separated by a separatrix (cheese slice). We only care about mapping the coordinates along bistable axis.</em> </div> </div> <p>With our scalar function \(\psi:\mathbb{R}^N\to\mathbb{R}\) we would like to perform a special type of dimensionality reduction: we only care to identify our location along the attractor–separatrix–attractor axis, i.e., along the depth of sandwich.</p> <p>One way to achieve this is to have this scalar observable \(\psi(\boldsymbol x)\) <em>imitate</em> the bistable dynamics along this axis. Thus we pick a simple example of bistable dynamics in 1D (hover your cursor over it to see the plot):</p> <div class="equation-with-plot"> $$ \begin{equation} \dot \psi = \lambda (\psi-\psi^3) \label{eq:sKEFsimple} \end{equation} $$ <div class="plot-tooltip"> <iframe src="/blog/assets/html/2025-09-29-Separatrix/bistable_1d_plot.html" scrolling="no"></iframe> </div> </div> <p>with \(\lambda&gt;0\), dropping the \(\boldsymbol x\) notation for a moment for clarity. This system has fixed point attractors at \(\pm 1\) and an unstable fixed point (a separatrix) at \(0\) – a 1D Sandwich of Bistability.</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-09-29-Separatrix/mapping_illustration_horizontal.png" alt="Mapping" width="500"> <div style="max-width: 500px; margin: 0.5rem auto; text-align: center;"> <em>Mapping the high-D state to a 1D bistable system.</em> </div> </div> <p>Now we want to couple the \(\psi\) dynamics with the \(\boldsymbol x\) dynamics so we bring back the \(\boldsymbol x\) dependence. Specifically as \(\boldsymbol x(t)\) evolves in time according to \(\eqref{eq:ODE}\):</p> \[\begin{equation} \frac{d}{dt}\bigg(\psi\big(\boldsymbol{x}(t)\big)\bigg) = \lambda\bigg[\psi\big(\boldsymbol{x}(t)\big) - \psi\big(\boldsymbol{x}(t)\big)^3\bigg]. \label{eq:sKEF} \end{equation}\] <p>This means that if we were to <em>observe</em> the value of \(\psi(\boldsymbol x)\) as \(\boldsymbol x\) evolved in time, that value would evolve according \(\eqref{eq:sKEFsimple}\).</p> <p>It seems that finding solutions to \(\eqref{eq:sKEF}\) could be the key to constructing a <em>separatrix locator</em>. The value \(\psi(\boldsymbol x)\) would be the coordinate of \(\boldsymbol x\) along the bistable axis. This value would be \(0\) when \(\boldsymbol x\) is anywhere on the separatrix, exactly our stated <a href="#goal">goal</a>.</p> <h2 id="squashed-koopman-eigenfunctions">(squashed) Koopman Eigenfunctions</h2> <p>At this stage, it’s worth noticing that the left hand side of \(\eqref{eq:sKEF}\) is actually a known object called the <a href="https://en.wikipedia.org/wiki/Lie_derivative" rel="external nofollow noopener" target="_blank">Lie derivative</a> of \(\psi\) along the flow given by \(f\), and also known as the infinitesimal generator of the <a href="https://en.wikipedia.org/wiki/Composition_operator" rel="external nofollow noopener" target="_blank">Koopman operator</a>, (See <d-cite key="brunton_notes_2019"></d-cite>).</p> <p>To make this link explicit, we first define the propagator function \(F_\tau(x(t)):=x(t+\tau)\) where \(x(t)\) is any solution to \(\eqref{eq:ODE}\). The Koopman operator \(\mathcal K_\tau\) is defined as</p> \[\mathcal K_\tau g = g \circ F_\tau\] <p>where \(g\) is any<d-footnote>\(g\) must belong to a Hilbert space, meaning that it must come with inner product (and it's associated norm), e.g., $$\langle f,g\rangle:=\int_{\mathbb R^N}f(\boldsymbol x)g(\boldsymbol x)d\boldsymbol x$$ thus requiring that the function be square integrable.</d-footnote> scalar function of the state-space \(\mathbb R^N\). Its infinitesimal generator \(\mathcal K\) (dropping the subscript) is essentially a time-derivative:</p> \[\begin{equation} \mathcal Kg = \lim_{\tau\to0} \frac{\mathcal K_\tau g - g}{\tau} =\lim_{\tau\to0} \frac{g\circ F_\tau - g}{\tau} = \frac{d}{d\tau} g \circ F_\tau\bigg\vert_{\tau=0}. \label{eq:koopman_generator} \end{equation}\] <p>The last version if evaluated on a trajectory \(x(t)\) is the left hand side of \(\eqref{eq:sKEF}\), allowing us to rewrite it compactly as</p> \[\begin{equation} \mathcal K\psi = \lambda (\psi-\psi^3). \label{eq:sKEF_compact} \end{equation}\] <p>This equation is <em>almost</em> an eigenfunction equation. All we need is to drop the cubic term:</p> \[\begin{equation} \mathcal K\phi = \lambda \phi. \label{eq:KEF} \end{equation}\] <p>In fact, the two problems are closely related. We can show that solutions to \(\eqref{eq:KEF}\) can be transformed into solutions of \(\eqref{eq:sKEF_compact}\) and vice versa by <em>squashing</em> and <em>unsquashing</em>. If \(\phi\) is a solution to \(\eqref{eq:KEF}\), then we can obtain a solution \(\psi\) to \(\eqref{eq:sKEF_compact}\) via:</p> \[\begin{equation} \psi(\boldsymbol{x}) = \frac{ \phi(\boldsymbol{x})}{ \sqrt{1+\phi(\boldsymbol{x})^2} } \label{eq:squash} \tag{squash} \end{equation}\] <p>Conversely, if \(\psi\) is a solution to \(\eqref{eq:sKEF_compact}\), then we can obtain a solution \(\phi\) to \(\eqref{eq:KEF}\) via:</p> \[\begin{equation} \phi(\boldsymbol{x}) = \frac{ \psi(\boldsymbol{x})}{ \sqrt{1-\psi(\boldsymbol{x})^2} } \label{eq:unsquash} \tag{unsquash} \end{equation}\] <p>We provide an informal derivation.</p> <details> <summary>Derivation: From eigenfunction to squashed eigenfunction and back</summary> <p>To do this, define the pointwise transforms \(\psi \;=\; u(\phi) \;:=\; \frac{\phi}{\sqrt{1+\phi^2}}, \qquad \phi \;=\; v(\psi) \;:=\; \frac{\psi}{\sqrt{1-\psi^2}}.\)</p> <hr> <p>First we will derive useful identity: the chain rule for the Koopman generator.</p> <h3 id="koopman-chain-rule">Koopman chain rule</h3> <p>Let \(\phi:\mathbb R^N \to \mathbb R\) be a smooth scalar observable, and let \(u:\mathbb R \to \mathbb R\) be a smooth scalar nonlinearity. Let \(\psi(\boldsymbol x) = u(\phi(\boldsymbol x)).\)</p> <p>The Koopman generator is</p> \[\mathcal K g(\boldsymbol x) = \nabla g(\boldsymbol x)\cdot f(\boldsymbol x),\] <p>for any \(g\) where \(f(\boldsymbol x)\) is the underlying vector field.</p> <p>By the multivariable chain rule for gradients,</p> \[\nabla \psi(\boldsymbol x) = u'\big(\phi(\boldsymbol x)\big)\,\nabla \phi(\boldsymbol x).\] <p>Applying the Koopman generator gives</p> \[\mathcal K \psi(\boldsymbol x) = \nabla \psi(\boldsymbol x)\cdot f(\boldsymbol x) = u'\big(\phi(\boldsymbol x)\big)\,\nabla \phi(\boldsymbol x)\cdot f(\boldsymbol x) = u'\big(\phi(\boldsymbol x)\big)\,\mathcal K \phi(\boldsymbol x).\] <p>Therefore, for any smooth \(u\) and \(\phi\),</p> \[\boxed{\;\mathcal K[u(\phi)] = u'(\phi)\,\mathcal K\phi\; }.\] <hr> <h3 id="from-mathcal-kphilambdaphi-to-mathcal-kpsilambdapsi-psi3">From \(\mathcal K\phi=\lambda\phi\) to \(\mathcal K\psi=\lambda(\psi-\psi^3)\)</h3> <p>Assume \(\mathcal K\phi \;=\; \lambda \phi.\)</p> <p>Recall that \(\psi = u(\phi)\) where \(u(z)=\dfrac{z}{\sqrt{1+z^2}}\). Compute \(u'(z)\):</p> \[\begin{align*} u'(z) &amp;= (1+z^2)^{-\frac{1}{2}} + z\cdot\Big(-\frac{1}{2}\Big)(1+z^2)^{-\frac{3}{2}}\cdot (2z) \\[2pt] &amp;= (1+z^2)^{-\frac{1}{2}} - z^2(1+z^2)^{-\frac{3}{2}} \\[2pt] &amp;= \frac{1+z^2-z^2}{(1+z^2)^{\frac{3}{2}}} \\[2pt] &amp;= \frac{1}{(1+z^2)^{\frac{3}{2}}} \end{align*}\] <p>By the Koopman chain rule,</p> \[\mathcal K\psi \;=\; u'(\phi)\,\mathcal K\phi \;=\; \frac{1}{(1+\phi^2)^{3/2}}\,\lambda\phi \;=\; \lambda\,\frac{\phi}{(1+\phi^2)^{3/2}}.\] <p>But</p> \[\psi - \psi^3 = \frac{\phi}{\sqrt{1+\phi^2}} - \frac{\phi^3}{(1+\phi^2)^{3/2}} = \frac{\phi(1+\phi^2)-\phi^3}{(1+\phi^2)^{3/2}} = \frac{\phi}{(1+\phi^2)^{3/2}}.\] <p>Hence \(\boxed{\;\mathcal K\psi \;=\; \lambda(\psi-\psi^3)\; }.\)</p> <hr> <h3 id="from-mathcal-kpsilambdapsi-psi3-back-to-mathcal-kphilambdaphi">From \(\mathcal K\psi=\lambda(\psi-\psi^3)\) back to \(\mathcal K\phi=\lambda\phi\)</h3> <p>Assume \(\mathcal K\psi \;=\; \lambda(\psi-\psi^3).\)</p> <p>Recall that \(\phi = v(\psi)\) where \(v(z)=\dfrac{z}{\sqrt{1-z^2}}\). Compute \(v'(z)\):</p> \[\begin{align*} v'(z) &amp;= (1-z^2)^{-\frac{1}{2}} + z\cdot\frac{1}{2}(1-z^2)^{-\frac{3}{2}}\cdot (2z) \\[2pt] &amp;= (1-z^2)^{-\frac{1}{2}} + z^2(1-z^2)^{-\frac{3}{2}} \\[2pt] &amp;= \frac{1}{(1-z^2)^{\frac{3}{2}}} \end{align*}\] <p>Apply the Koopman chain rule with \(\phi=v(\psi)\):</p> \[\mathcal K\phi \;=\; v'(\psi)\,\mathcal K\psi \;=\; \frac{1}{(1-\psi^2)^{3/2}}\,\lambda(\psi-\psi^3) \;=\; \lambda\,\frac{\psi(1-\psi^2)}{(1-\psi^2)^{3/2}} \;=\; \lambda\,\frac{\psi}{\sqrt{1-\psi^2}} \;=\; \lambda\,\phi.\] <p>Thus \(\boxed{\;\mathcal K\phi \;=\; \lambda \phi\; }.\)</p> <hr> <h3 id="conclusion">Conclusion</h3> <p>The pointwise transforms \(\psi = u(\phi) = \frac{\phi}{\sqrt{1+\phi^2}}, \qquad \phi = v(\psi) = \frac{\psi}{\sqrt{1-\psi^2}}\)</p> <p>carry solutions of the linear Koopman eigenfunction equation to solutions of the cubic equation and back: \(\mathcal K\phi=\lambda\phi \quad\Longleftrightarrow\quad \mathcal K\psi=\lambda(\psi-\psi^3).\)</p> </details> <p>Note that this derivation is highly non-rigorous. We gloss over the square integrability of \(\psi\) and \(\phi\), and even whether they are defined everywhere in \(\mathbb R^N\). According to our sandwich of bistability, we expect \(\psi(\boldsymbol {x^*})=\pm1\) at the attractors. According to \(\eqref{eq:unsquash}\), \(\phi(\boldsymbol {x^*})=\pm\infty\),</p> <h2 id="enter-deep-neural-networks">Enter Deep Neural Networks</h2> <p>Now that we know the properties of the desired \(\psi\), it’s time to find it. So how do we solve the \(\eqref{eq:sKEF}\) for a high-dimensional nonlinear system. This is where deep neural networks (DNNs) come in…</p> <p>First we re-write \(\eqref{eq:sKEF}\) as a partial differential equation (PDE):</p> \[\begin{equation} \nabla_{\boldsymbol{x}}\psi(\boldsymbol{x}) \cdot f(\boldsymbol{x}) = \lambda[\psi(\boldsymbol{x}) - \psi(\boldsymbol{x})^3], \label{eq:sKEFPDE} \end{equation}\] <p>recognising that \(\mathcal Kg=\nabla g \cdot f\) is another way to write the Koopman generator, using the multivariate chain rule on \(\eqref{eq:koopman_generator}\). This PDE means that instead of running the ODE \(\eqref{eq:ODE}\) to get trajectories \(\boldsymbol x(t)\), we can instead leverage the ability of DNNs to solve PDEs.</p> <p>We formulate a mean squared error loss for PDE \(\eqref{eq:sKEFPDE}\):</p> \[\begin{equation} \mathcal{L}_{\text{PDE}} = \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})} \Bigg[ \nabla \psi(\boldsymbol{x}) \cdot f(\boldsymbol{x}) - \lambda \Big(\psi(\boldsymbol{x})-\psi(\boldsymbol{x})^3\Big) \Bigg]^2, \label{eq:pde_loss} \end{equation}\] <p>where \(p(\boldsymbol{x})\) is a distribution over the phase space <d-cite key="e_deep_2018,sirignano_dgm_2018"></d-cite>. We can now parameterise \(\psi\) using a DNN, and train its weights to optimise \(\eqref{eq:pde_loss}\). This gradient-based PDE formulation is particularly convenient for implementation with DNNs since we can leverage automatic differentiation to compute the gradients efficiently. DNNs are also used in this way in physics-informed neural networks <d-cite key="raissi_physics-informed_2019"></d-cite>, encouraging DNNs to satisfy known physics, e.g., Navier–Stokes PDEs.</p> <p>Naturally, this doesn’t work out of the box. There are quite a few challenges – some common to eigenvalue problems, and some unique to our setting. You can click on them to find out more about why they arise, and how we solve them.</p> <details> <summary>Trivial solutions</summary> <p>As with any eigenvalue problem, this loss admits the trivial solution \(\psi \equiv 0\). To discourage such solutions, we introduce a shuffle-normalization loss where the two terms are sampled independently from the same distribution:</p> \[\begin{equation} \mathcal{L}_{\text{shuffle}} = \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x}), \tilde{\boldsymbol{x}} \sim p(\boldsymbol{x})} \Bigg[ \nabla \psi(\boldsymbol{x}) \cdot f(\boldsymbol{x}) - \lambda \Big(\psi(\tilde{\boldsymbol{x}}) - \psi(\tilde{\boldsymbol{x}})^3\Big) \Bigg]^2, \end{equation}\] <p>and optimise the ratio:</p> \[\begin{equation}\mathcal{L}_{\text{ratio}} = \frac{\mathcal{L}_{\text{PDE}}}{\mathcal{L}_{\text{shuffle}}}. \label{eq:ratio loss} \end{equation}\] </details> <details> <summary>Degeneracy across basins</summary> <p>Koopman eigenfunctions (KEFs) have an interesting property: a product of two KEFs is also a KEF. This can be seen by applying the PDE to the product of two such functions</p> \[\begin{equation} \nabla[\phi_1(x)\phi_2(x)] \cdot f(x) = (\lambda_1 + \lambda_2) \phi_1(x)\phi_2(x). \end{equation}\] <p>We’ll soon see that this translates to squashed KEFs as well. First, consider a smooth KEF \(\phi^1\) with \(\lambda = 1\) that vanishes only on the separatrix (what we want). Now, consider a piecewise-constant function \(\phi^0\) with \(\lambda = 0\) that is equal to 1 on one basin, and zero on another basin. The product \(\phi^1 \phi^0\) remains a valid KEF with \(\lambda = 1\), but it can now be zero across entire basins—thereby destroying the separatrix structure we aim to capture. Because of the relation between KEFs and sKEFs, this problem carries over to our squashed case. To mitigate this problem, we add another regulariser that causes the average value of \(\psi\) to be zero, encouraging negative and positive values on both sides of the separatrix.</p> </details> <details> <summary>Degeneracy in high dimensions</summary> <p>If the flow itself is separable, there is a family of KEFs that can emphasise one dimension over the others. Consider a 2D system \(\dot{x} = f_1(x), \quad \dot{y} = f_2(y)\), and the KEFs \(A(x)\) and \(B(y)\). There is a family of valid solutions \(\psi(x, y) = A(x)^{\mu} B(y)^{1 - \mu}\), for \(\mu \in R\).</p> <p>If \(\mu=0\) for instance, the \(x\) dimension is ignored. To mitigate this, we choose distributions for training the DNN that emphasise different dimensions, and then combine the results.</p> </details> <h2 id="does-it-work">Does It Work?</h2> <p>Now that we know what we are looking for (PDE equation), and how to find it (DNN), let’s put it all together. We train a DNN on a bistable damped oscillator, and on a 2D GRU trained on a 1-bit flip-flop task. In both cases, the resulting \(\psi\) has a zero level set on the separatrix.</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-09-29-Separatrix/two_2D_examples_squashed.png" alt="Two 2D Examples" width="100%"> <div style="max-width: 500px; margin: 0.5rem auto; text-align: center;"> <em><strong>A</strong>: ODEs for the damped duffing oscillator. <strong>B</strong>: Kinetic energy function identifies stable and unstable fixed points. <strong>C</strong>: DNN approximation of the sKEF and it's level sets. The zero-level set (orange) aligns with the separatrix. <strong>D,E,F</strong>: Same for a 2D GRU RNN trained on a 1-bit flip flop task. </em> </div> </div> <p>Finally, we take a published \(N=668\) unit RNN trained to reproduce the activity of neurones from anterior lateral motor cortex of mice trained to respond to optogenetic stimulation of their somatosensory cortex <d-cite key="finkelstein_attractor_2021"></d-cite>. By simulating the RNN we can locate the two attractors. The separatrix is an \((N-1)\)-dimensional manifold in \(\mathbb{R}^N\). To evaluate our method, we sample this high-D space by drawing random cubic Hermite curves that connect the two attractors (Fig. <strong>A</strong>). We then run many simulations via a binary-search along each curve (parameterised by \(\alpha\in[0,1]\)) to find the true separatrix crossing, and compare with \(\psi=0\), finding close agreement (Fig. <strong>B</strong>). This also allows us to design optimal perturbations. If we want to change the network’s decision, pushing the system towards the desired attractor may not be the most efficient direction. Using \(\psi\), we design minimal perturbations that cross the separatrix. The resulting perturbation size is smaller than perturbations aimed at the target fixed point or random separatrix locations (Fig. <strong>C</strong>).</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-09-29-Separatrix/finkelstein_blog.png" alt="Two 2D Examples" width="100%"> <div style="max-width: 500px; margin: 0.5rem auto; text-align: center;"> <em><strong>A</strong>: Hermite curves connecting attractors of a data-trained RNN <d-cite key="finkelstein_attractor_2021"></d-cite> (2D projection from 668D) with true separatrix points (red). <strong>B</strong>: sKEF zeroes versus true separatrix points along each curve. <strong>C</strong>: Norm of perturbations to reach separatrix from base point $\boldsymbol{x}_\text{base}$. </em> </div> </div> <h2 id="summary-and-outlook">Summary and Outlook</h2> <p>Making sense of high-dimensional dynamical systems is not trivial. We added another tool to the toolbox – a separatrix finder. More generally, one can think of our cubic \(\eqref{eq:sKEFsimple}\) as a <a href="https://en.wikipedia.org/wiki/Normal_form_(dynamical_systems)" rel="external nofollow noopener" target="_blank">normal form</a> for bistability. This is a canonical, or simple, version of a dynamical system with the same <em>topology</em>. Our method allows to reduce the high-D dynamics into such a form. In the future, we hope to extend this to many more applications. Check out <a href="https://github.com/KabirDabholkar/separatrix_locator" rel="external nofollow noopener" target="_blank">our code</a> and apply it to your own dynamical systems. Feel free to reach out to us, we’re excited to help and learn about new applications!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/blog/assets/bibliography/2025-09-29-Separatrix-Locator.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"UniReps/unireps.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 UniReps Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>