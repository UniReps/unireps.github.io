<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> MLIR-ARX: Accelerator-Aware MLIR-to-RISC-V Compilation Integrated with an EDA Flow | UniReps Blog </title> <meta name="author" content="UniReps Blog"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/blog/assets/img/unireps_favicon.png?3a42502b41deab62714411b8479222c3"> <link rel="stylesheet" href="/blog/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unireps.org//blog/2025/mlir-arx-accelerator-aware-mlir-to-risc-v-compilation-integrated-with-an-eda-flow/"> <script src="/blog/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/blog/assets/js/distillpub/template.v2.js"></script> <script src="/blog/assets/js/distillpub/transforms.v2.js"></script> <script src="/blog/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "MLIR-ARX: Accelerator-Aware MLIR-to-RISC-V Compilation Integrated with an EDA Flow",
            "description": "",
            "published": "September 20, 2025",
            "authors": [
              
              {
                "author": "Yongin Kwon",
                "authorURL": "https://orcid.org/0000-0003-2973-246X",
                "affiliations": [
                  {
                    "name": "Electronics and Telecommunications Research Institute, Republic of Korea",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "JooHyoung Cha",
                "authorURL": "https://orcid.org/0009-0008-2123-454X",
                "affiliations": [
                  {
                    "name": "University of Science and Technology, Republic of Korea",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog//"> <span class="font-weight-bold">UniReps</span> Blog </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>MLIR-ARX: Accelerator-Aware MLIR-to-RISC-V Compilation Integrated with an EDA Flow</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#1-introduction">1. Introduction</a> </div> <div> <a href="#2-background">2. Background</a> </div> <div> <a href="#3-design-and-architecture">3. Design and Architecture</a> </div> <div> <a href="#4-implementation">4. Implementation</a> </div> <div> <a href="#5-early-evaluation">5. Early Evaluation</a> </div> <div> <a href="#6-conclusion">6. Conclusion</a> </div> <div> <a href="#7-limitations-and-outlook">7. Limitations and outlook</a> </div> <div> <a href="#8-related-work">8. Related Work</a> </div> <div> <a href="#9-vta-configuration-and-expected-performance-on-fpga">9. VTA Configuration and Expected Performance on FPGA</a> </div> <div> <a href="#10-arx-dialect">10. ARX Dialect</a> </div> <div> <a href="#11-profiling-instrumentation-and-log-schema">11. Profiling Instrumentation and Log Schema</a> </div> <div> <a href="#12-cost-model-details-and-selection-algorithm">12. Cost Model Details and Selection Algorithm</a> </div> <div> <a href="#13-mnist-model-shapes-and-mapping-notes">13. MNIST Model Shapes and Mapping Notes</a> </div> </nav> </d-contents> <p><strong>TLDR</strong></p> <p>Heterogeneous RISC-V systems challenge us to keep model representations coherent while deciding what to accelerate as hardware evolves. <code class="language-plaintext highlighter-rouge">mlir-arx</code> treats MLIR as a unifying representation layer: we introduce an <code class="language-plaintext highlighter-rouge">arx</code> dialect that encodes accelerator capabilities, inject lightweight profiling ops, and use a two-stage (analytic + profile-guided) cost model to form maximal offload regions under resource and dependency constraints. By aligning model computations across CPU and accelerators in a single IR space, our compile–-measure loop reliably selects profitable regions and delivers substantial end-to-end speedups on a MNIST CNN with a configurable VTA overlay on a Genesis FPGA prototype.</p> <p>Availability. Source code and artifacts are available at: <a href="https://gitlab.com/ones-ai/mlir-arx" rel="external nofollow noopener" target="_blank">Repository in Our Gitlab</a></p> <hr> <h1 id="1-introduction"><strong>1. Introduction</strong></h1> <p>Edge and embedded AI systems increasingly pair general-purpose RISC-V cores with domain accelerators (from NPUs to lightweight tensor engines) to meet stringent latency and energy targets<d-cite key="jouppi2017tpu, eyeriss2016, nvdla2019"></d-cite>. In such settings, the practical challenge is not only to partition a model across CPU and accelerator boundaries, but also to decide what to accelerate when device capabilities are unknown or evolving at project start. A sensible path is to begin from a CPU-only baseline, profile real executions, and then offload the most profitable regions subject to resource and orchestration constraints. In this work we cast that workflow as a problem of representation alignment: by expressing computations in a common intermediate representation (MLIR), we keep the model’s computational structure coherent across heterogeneous RISC-V–attached devices while letting profiling and cost models determine which aligned regions should migrate to accelerators.</p> <p>MLIR’s multi-dialect design and progressive lowering are a natural fit for this workflow<d-cite key="mlir"></d-cite>. However, turning it into an end-to-end solution for RISC-V SoCs requires additional pieces:</p> <h3 id="11-requires">1.1 Requires</h3> <ol> <li>IR-level capability modeling to express an accelerator’s constraints</li> <li>lightweight profiling instrumentation that can be injected/stripped by passes</li> <li>a cost model that joins analytic estimates with profile-derived efficiencies</li> <li>packaging that integrates with an SoC/EDA flow.</li> </ol> <p>We present <code class="language-plaintext highlighter-rouge">MLIR-ARX</code>, a profile-guided compiler built on MLIR that introduces an <code class="language-plaintext highlighter-rouge">arx</code> dialect for accelerator capabilities, selects profitable offload regions via a two-stage cost model, and provides a retargetable backend integrated with RISC-V eXpress<d-footnote>We thank collaborators who contributed to the RVX integration and platform bring-up.</d-footnote> (<code class="language-plaintext highlighter-rouge">RVX</code>) for FPGA bring-up<d-cite key="rvx-etrij"></d-cite>. For initial experiments we use VTA as a configurable accelerator to test profile-driven selection and resource-aware partitioning, while the CPU-only path remains the numerically correct fallback<d-cite key="iree22tiny,tvm2018"></d-cite>.</p> <h3 id="12-contributions">1.2 Contributions</h3> <ul> <li> <strong>Accelerator-aware IR</strong>. An <code class="language-plaintext highlighter-rouge">arx</code> dialect that captures accelerator capabilities (constraints, tiling, resource usage) and enables principled lifting of standard tensor ops.</li> <li> <strong>Cost-guided partitioning</strong>. A two-stage (analytic + profile-guided) model that forms offload regions with explicit DMA orchestration and safe CPU fallbacks.</li> <li> <strong>Retargetable backend + RVX integration</strong>. Code generation for RISC-V plus accelerator stubs and a device manifest that RVX uses for automatic integration.</li> <li> <strong>Early evaluation</strong>. A small CNN on MNIST running on RVX-based FPGA prototypes with VTA, validating the compile–measure loop and selection mechanism.</li> </ul> <hr> <h1 id="2-background"><strong>2. Background</strong></h1> <p><strong>MLIR Foundations and Target Platform</strong></p> <h3 id="21-mlir-in-brief">2.1 MLIR in brief</h3> <p>MLIR is a multi-level compiler infrastructure that represents computations at various abstraction levels and supports progressive lowering through dialects and pattern-based rewrites<d-cite key="mlir"></d-cite>. Dialects capture operations, types, and constraints; key ones for <code class="language-plaintext highlighter-rouge">MLIR-ARX</code> include <code class="language-plaintext highlighter-rouge">mhlo/StableHLO</code> for tensor semantics<d-cite key="stablehlo"></d-cite>, <code class="language-plaintext highlighter-rouge">linalg</code> for structured kernels, and <code class="language-plaintext highlighter-rouge">memref</code> for explicit memory.</p> <p>Two features are central to our setting: bufferization, which separates algorithmic transformations from storage decisions by converting tensors to <code class="language-plaintext highlighter-rouge">memref</code>s with explicit lifetimes, and dialect interfaces/converters, which allow capability-aware lifting or lowering between dialects. These make it possible to express accelerator constraints in IR and offload only the supported regions. Prior systems, from TVM’s multi-backend flow<d-cite key="tvm2018"></d-cite> to IREE’s embedded pipelines<d-cite key="iree22tiny"></d-cite>, demonstrate the viability of such end-to-end MLIR-based compilation.</p> <h3 id="22-why-mlir-for-risc-v--accelerators">2.2 Why MLIR for RISC-V + Accelerators</h3> <p>RISC-V-based edge platforms often combine a control processor (with or without a vector extension) and one or more domain accelerators attached via a memory-mapped interconnect.</p> <p>This creates three immediate needs:</p> <ol> <li>partitioning of the model into CPU and accelerator regions.</li> <li>explicit orchestration of DMA, synchronization, and address spaces.</li> <li>graceful fallback when constraints are violated. MLIR’s dialect modularity lets us.</li> </ol> <p>(a) express accelerator capabilities as IR-level contracts, (b) form and legalize offload regions under those contracts, and (c) lower both sides—CPU and accelerator stubs—within a single pass manager, sharing analyses (shape, alias, dependence) across the boundary. Compared with ad-hoc code generators, the benefits are: reuse of upstream transformations, uniform debuggability, and a single IR for both accelerated and non-accelerated builds.</p> <h3 id="23-rvx-overview">2.3 RVX overview</h3> <p>RVX is an EDA environment to assemble RISC-V SoCs (single-/multi-core, memory subsystem, interconnect, peripheral IP), validate them on FPGA, and produce handoff artifacts for silicon<d-cite key="rvx-etrij"></d-cite>. On the software side, RVX provides toolchain integration points (boot/firmware layout, MMIO address map, interrupt lines) and profiling hooks. <code class="language-plaintext highlighter-rouge">MLIR-ARX</code> targets this boundary: it emits CPU binaries, an accelerator runtime and driver stubs, and a device manifest that RVX consumes to wire up interconnect ports and firmware tables. By aligning compiler outputs with RVX’s manifests, we avoid manual bring-up steps when retargeting cores or adding/removing accelerators.</p> <blockquote> <p>We primarily target edge-style RISC-V SoCs where accelerators are memory-mapped with local SRAM and DMA engines. Our current prototype assumes statically known shapes in offload regions; dynamic-shape support is under active development.</p> </blockquote> <hr> <h1 id="3-design-and-architecture"><strong>3. Design and Architecture</strong></h1> <p><strong>Profile-Guided MLIR-to-RISC-V Offload</strong></p> <h3 id="31-pipeline-overview">3.1 Pipeline Overview</h3> <p><a href="#flow">Figure 1</a> shows the end-to-end flow: models import into MLIR, run once on a CPU baseline to profile, mine offload candidates, select them under FPGA budgets, form offload regions, and lower to CPU and accelerator backends. A machine-readable accelerator description generates the <code class="language-plaintext highlighter-rouge">arx</code> dialect and converters; RVX profiling closes the compile–measure loop.</p> <h3 id="32-baseline-execution-and-profiling">3.2 Baseline Execution and Profiling</h3> <p><code class="language-plaintext highlighter-rouge">MLIR-ARX</code> assumes no accelerator a priori. Every model first executes end-to-end on RISC-V, producing per-op/region profiles (cycles, bytes moved, stalls, shapes/dtypes/layouts) keyed by stable IR handles. This path is also the correctness fallback for any region that proves illegal or unprofitable to offload.</p> <h3 id="33-candidate-discovery-and-cost-modeling">3.3 Candidate Discovery and Cost Modeling</h3> <p>Hot single ops or short fusable patterns are grouped by semantics/constraints as offload candidates. For a region $R$, we use an inline estimate $\Delta T(R)=T_{\mathrm{cpu}}(R)-T_{\mathrm{off}}(R)$ with $T_{\mathrm{off}}\approx T_{\mathrm{setup}}+\max(T_{\mathrm{dma}},T_{\mathrm{cmp}})+T_{\mathrm{sync}}$ where analytic terms (op counts, tiling, bandwidth/latency) are corrected by profile-derived efficiencies.</p> <h3 id="34-resource-aware-selection-and-partitioning">3.4 Resource-Aware Selection and Partitioning</h3> <p>On FPGA targets, selection respects LUT/FF/DSP/BRAM, local SRAM, DMA lanes, and clock budgets. Given per-candidate resource costs $\mathcal{R}(c)$ and budget $B$, we maximize $\sum\Delta T(c)$ subject to $\sum\mathcal{R}(c)\le B$, preferring high benefit density when tight. Selected ops become maximal \emph{offload regions} under dependency/memory constraints; the compiler inserts explicit host–device copies/fences and schedules DMA to overlap with compute.</p> <h3 id="35-lowering-and-runtime">3.5 Lowering and Runtime</h3> <p>Both CPU and accelerator paths first lower into the <code class="language-plaintext highlighter-rouge">arx</code> dialect. CPU ops then follow <code class="language-plaintext highlighter-rouge">linalg</code> $\to$ <code class="language-plaintext highlighter-rouge">scf/affine</code> $\to$ <code class="language-plaintext highlighter-rouge">LLVM</code> to produce RISC-V ELFs, while accelerator ops lower directly to library calls with a thin runtime and DMA descriptors. A device manifest (MMIO ranges, IRQ lines) is also emitted for RVX integration.</p> <h3 id="36-retargeting-and-feedback">3.6 Retargeting and feedback</h3> <p>A YAML/JSON accelerator description regenerates the <code class="language-plaintext highlighter-rouge">arx</code> capability model and converters, enabling recompilation without model changes. Deployed binaries feed fresh RVX profiles back into the cost model, which updates efficiencies and re-ranks candidates for the next iteration.</p> <p><a id="flow"></a> </p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-09-29-mlir-arx-accelerator-aware-mlir-to-risc-v-compilation-integrated-with-an-eda-flow/01_system_overview.png" alt="My Image" width="700"> </div> <hr> <h1 id="4-implementation"><strong>4. Implementation</strong></h1> <p><strong>ONNX-MLIR Base, ARX Dialect, and RVX/VTA Bring-up</strong></p> <h3 id="41-codebase-and-mlir-integration">4.1 Codebase and MLIR Integration</h3> <p>We implement <code class="language-plaintext highlighter-rouge">MLIR-ARX</code> by extending the open-source ONNX-MLIR<d-cite key="onnxmlir2020"></d-cite> stack. Models are imported and legalized through ONNX-MLIR into MLIR’s tensor/structured dialects, upon which we add our <code class="language-plaintext highlighter-rouge">ARX</code> components. The output code runs unmodified on RVX-synthesized RISC-V platforms; when no accelerator is present, the CPU path serves as the numerically correct fallback.</p> <h3 id="42-arx-dialect-and-lowering-paths">4.2 ARX Dialect and Lowering Paths</h3> <p>The <code class="language-plaintext highlighter-rouge">arx</code> dialect lifts eligible tensor and <code class="language-plaintext highlighter-rouge">linalg</code> ops into accelerator-aware form, annotated with capability and tiling metadata. Both CPU and accelerator ops lower through this dialect: CPU paths continue via <code class="language-plaintext highlighter-rouge">linalg</code> $\to$ <code class="language-plaintext highlighter-rouge">scf/affine</code> $\to$ <code class="language-plaintext highlighter-rouge">LLVM</code> for RISC-V binaries, while accelerator ops become driver/library calls with explicit host–device transfers. Because both paths share the same pass pipeline, analyses such as shape, aliasing, and dependence are reused across CPU and accelerator lowering.</p> <h3 id="43-profiling-instrumentation">4.3 Profiling Instrumentation</h3> <p>To support the profile-driven flow, we define lightweight profiling ops (begin/end counters, byte/traffic counters, DMA/compute timestamps). When the compiler is invoked with a profiling option, an <em>instrumentation pass</em> inserts these ops around selected regions/ops during canonicalization and bufferization. The pass is designed to be idempotent and can be stripped in a late “de-instrumentation” pass for production builds. Profiles are keyed by stable IR handles to survive recompilation unless shapes change.</p> <h3 id="44-accelerator-target-vta">4.4 Accelerator Target: VTA</h3> <p>For an initial hardware target we use Apache TVM’s VTA soft accelerator. VTA implements a small set of tensor primitives with parameterizable compute parallelism (e.g., PE width), local SRAM sizes, and instruction buffer depth. This configurability makes it a good vehicle to test <code class="language-plaintext highlighter-rouge">MLIR-ARX</code>’s candidate selection and resource-aware partitioning: The capability model is regenerated from the chosen VTA configuration, while the cost model estimates tile fit, DMA overlap, and achievable throughput under the given SRAM and bandwidth constraints.</p> <hr> <h1 id="5-early-evaluation"><strong>5. Early Evaluation</strong></h1> <h3 id="51-setup">5.1 Setup</h3> <p>Our prototype system follows the dual-ORCA configuration in prior TIP work, with two ORCA RISC-V RV32IM cores synthesized at 100 MHz on a Genesis FPGA and connected via the RVX-generated $\mu$NoC and AXI-based DRAM subsystem. A configurable VTA overlay is attached as an MMIO+DMA device, using 8-bit inputs/weights, 32-bit accumulators, and a $16{\times}16{\times}16$ GEMM block. All components are generated by RVX from a manifest and deployed to FPGA.</p> <h3 id="52-benchmark">5.2 Benchmark</h3> <p>For evaluation, we use a small CNN for MNIST consisting of two conv+ReLU blocks with $2{\times}2$ pooling, followed by a fully connected layer (input $1{\times}28{\times}28$). Profiles are first collected from the CPU-only execution to guide candidate discovery. Operators selected for offload are <em>conv2d(+bn)+relu</em> blocks (Conv1, Conv2) and the final fully connected layer; control flow, quantize/dequantize, and maxpool remain on the CPU.</p> <h3 id="53-cpu-only-baseline">5.3 CPU-only Baseline</h3> <p><a href="#inference-time-arx">Table 1</a> reports the measured per-layer latency on the dual ORCA cores. The two convolution layers dominate the runtime (over 95 % of total latency), making them the primary candidates for acceleration.</p> <h3 id="54-projected-accelerator-performance">5.4 Projected Accelerator Performance</h3> <p>Since the FPGA prototype is still under integration, we estimate accelerator-side performance using a simple throughput model assuming three VTA configurations (A/B/C) with 256/512/1024 MAC/cycle. <a href="#inference-time-arx">Table 1</a> shows the projected latency when Conv1, Conv2, and the fully connected layer are offloaded to VTA, while other layers remain on the CPU. The results indicate a potential end-to-end speedup of $50 \times$–$90\times$ compared to the CPU-only baseline, with hardware resource utilization scaling from $\sim$ 15 % to $\sim$ 60 % of available DSPs on the target FPGA.</p> <p><a id="inference-time-arx"></a></p> <table> <caption> Table 1. Predicted latency of MNIST CNN layers with CPU-only vs. three VTA configurations (A/B/C). VTA throughput assumes 256/512/1024 MAC/cycle at 75% utilization. Hardware utilization is estimated on XC7K325T FPGA. </caption> <thead> <tr> <th style="text-align:left;">Layer</th> <th style="text-align:right;">Ops</th> <th style="text-align:right;">CPU-only (µs)</th> <th style="text-align:right;">VTA-A (µs)</th> <th style="text-align:right;">VTA-B (µs)</th> <th style="text-align:right;">VTA-C (µs)</th> </tr> </thead> <tbody> <tr> <td style="text-align:left;">1. Quantize</td> <td style="text-align:right;">—</td> <td style="text-align:right;">5.73</td> <td style="text-align:right;">—</td> <td style="text-align:right;">—</td> <td style="text-align:right;">—</td> </tr> <tr> <td style="text-align:left;">2. Conv1</td> <td style="text-align:right;">97,344</td> <td style="text-align:right;">884.20</td> <td style="text-align:right;">5.07</td> <td style="text-align:right;">2.54</td> <td style="text-align:right;">1.27</td> </tr> <tr> <td style="text-align:left;">3. MaxPool1</td> <td style="text-align:right;">10,816</td> <td style="text-align:right;">28.96</td> <td style="text-align:right;">—</td> <td style="text-align:right;">—</td> <td style="text-align:right;">—</td> </tr> <tr> <td style="text-align:left;">4. Conv2</td> <td style="text-align:right;">557,568</td> <td style="text-align:right;">2949.15</td> <td style="text-align:right;">29.04</td> <td style="text-align:right;">14.52</td> <td style="text-align:right;">7.26</td> </tr> <tr> <td style="text-align:left;">5. FullyConnected</td> <td style="text-align:right;">5,120</td> <td style="text-align:right;">4.76</td> <td style="text-align:right;">0.27</td> <td style="text-align:right;">0.13</td> <td style="text-align:right;">0.07</td> </tr> <tr> <td style="text-align:left;">6. Dequantize</td> <td style="text-align:right;">—</td> <td style="text-align:right;">0.50</td> <td style="text-align:right;">—</td> <td style="text-align:right;">—</td> <td style="text-align:right;">—</td> </tr> <tr> <th style="text-align:left;">Total</th> <th style="text-align:right;">670,848</th> <th style="text-align:right;">3873.30</th> <th style="text-align:right;">69.57</th> <th style="text-align:right;">52.38</th> <th style="text-align:right;">43.78</th> </tr> <tr> <th scope="row" colspan="2" style="text-align:left;">HW Util. (DSP / BRAM / LUT %)</th> <td style="text-align:right;">—</td> <td style="text-align:right;">15 / 44 / 12</td> <td style="text-align:right;">31 / 44 / 21</td> <td style="text-align:right;">61 / 46 / 36</td> </tr> </tbody> </table> <hr> <h1 id="6-conclusion"><strong>6. Conclusion</strong></h1> <p>We presented <code class="language-plaintext highlighter-rouge">MLIR-ARX</code>, a profile-driven MLIR compiler that begins from a CPU-only RISC-V baseline, identifies profitable regions, and offloads them to accelerators under resource and orchestration constraints. <code class="language-plaintext highlighter-rouge">MLIR-ARX</code> introduces the <code class="language-plaintext highlighter-rouge">arx</code> dialect for capability-aware lifting, lightweight profiling instrumentation, and a retargetable backend integrated with RVX. Early evaluation on a small MNIST CNN with a configurable VTA overlay shows that offloading Conv1/Conv2/FC achieves order-of-magnitude latency reductions over the dual-ORCA CPU baseline, consistent with our cost-model predictions.</p> <hr> <h1 id="7-limitations-and-outlook"><strong>7. Limitations and outlook</strong></h1> <p>Our cost model is analytic with profile-derived corrections; learned models may better capture controller effects and DMA/compute overlap. Offload regions currently assume static shapes; extending legality/bufferization for dynamic-shape cases is ongoing. Finally, while the CPU path can exploit vector intrinsics, full scheduling for attention-like blocks and multi-accelerator concurrency remains future work. We expect these extensions—alongside broader accelerator backends—to further tighten the compile–measure loop and reduce manual retargeting on RVX platforms.</p> <hr> <h1 id="8-related-work"><strong>8. Related Work</strong></h1> <p><strong>RISC-V CPUs, Vector Extensions, AI Accelerators, and Design Space Exploration</strong></p> <h3 id="81-risc-v-as-the-control-plane-for-heterogeneous-ml-socs">8.1 RISC-V as the Control Plane for Heterogeneous ML SoCs</h3> <p>Open RISC-V implementations range from tiny in-order microcontrollers to out-of-order Linux-class cores, making them a natural control plane for accelerator-rich SoCs. Representative open cores include Rocket (in-order) and BOOM (out-of-order) from the Berkeley stack<d-cite key="rocket,boom"></d-cite>, CVA6/Ariane<d-cite key="cva6"></d-cite>, and the PULP family for ultra-low-power microcontrollers with DSP-like packed-SIMD extensions<d-cite key="pulp"></d-cite>. SoC generators such as Chipyard<d-cite key="chipyard"></d-cite> offer standard interconnects (TileLink/AXI) and co-processor attachment points (e.g., RoCC), reducing the cost of integrating DMA-capable accelerators alongside a RISC-V host.</p> <h3 id="82-risc-v-vector-and-packed-simd-for-ml">8.2 RISC-V Vector and Packed-SIMD for ML</h3> <p>The RISC-V Vector extension (RVV)<d-cite key="rvv"></d-cite> adopts a vector-length-agnostic model that decouples vector width from the ISA, enabling portability across microarchitectures. Implementations can choose lane count and microarchitectural details, as explored in Hwacha and Ara<d-cite key="hwacha"></d-cite>. For MCU-class devices, the packed-SIMD “P” extensions from PULP<d-cite key="pulp"></d-cite> target fixed-point and dot-product primitives. In practice, RVV or packed SIMD is well-suited for control and medium-granularity tensor compute, while large GEMMs/convolutions are often offloaded to dedicated accelerators.</p> <h3 id="83-attachment-patterns-for-ai-accelerators">8.3 Attachment Patterns for AI Accelerators</h3> <p>Three patterns dominate:</p> <ol> <li> <p><strong>Memory-mapped DMA engines</strong>: accelerators with local SRAM and DMA, controlled as MMIO devices; the most common in embedded contexts.</p> </li> <li> <p><strong>Coprocessors (e.g., RoCC)</strong>: accelerators invoked via custom instructions or queues, reducing software overhead but tying the ABI to a core design<d-cite key="rocket,chipyard,gemmini"></d-cite>.</p> </li> <li> <p><strong>Streaming/NoC-attached engines</strong>: connected via on-chip networks with stream interfaces; the host sets up dataflow graphs and dispatches jobs.</p> </li> </ol> <p><code class="language-plaintext highlighter-rouge">MLIR-ARX</code> assumes the first pattern (MMIO+DMA) but its IR contracts generalize to other attachments.</p> <h3 id="84-tensor-accelerators-and-dataflows">8.4 Tensor Accelerators and Dataflows</h3> <p>A wide body of work covers compute/dataflow design for DNNs: systolic arrays (e.g., TPU<d-cite key="jouppi2017tpu"></d-cite>), spatial dataflows (row/output/weight-stationary, e.g., Eyeriss<d-cite key="eyeriss2016"></d-cite>), and precision-specialized engines (e.g., NVDLA<d-cite key="nvdla2019"></d-cite>). Open-source generators like Gemmini<d-cite key="gemmini"></d-cite> produce RISC-V-attached systolic arrays with tunable parameters. These designs show that with aggressive on-chip reuse and explicit DMA/compute overlap, accelerators can deliver order-of-magnitude energy savings if the compiler/runtime manages packing, tiling, and synchronization.</p> <h3 id="85-fpga-overlays-and-soft-accelerators">8.5 FPGA Overlays and Soft Accelerators</h3> <p>FPGAs serve as prototyping and deployment platforms for edge ML accelerators. Overlay designs, including VTA<d-cite key="vta"></d-cite>, FINN<d-cite key="finn"></d-cite>, and hls4ml<d-cite key="hls4ml"></d-cite>, expose parameter spaces (PE array size, SRAM depth, DMA width) suitable for compiler-driven design space exploration. Compared to fixed-function ASICs, overlays trade some efficiency for rapid iteration and portability.</p> <h3 id="86-memory-systems-and-data-movement">8.6 Memory Systems and Data Movement</h3> <p>Memory hierarchy decisions strongly influence performance and energy. Designs like Eyeriss exploit row-stationary mapping to minimize off-chip traffic; others (e.g., MAERI, SCNN) adapt to sparsity and flexible reductions. For RISC-V-attached accelerators, key challenges are software-visible packing/tiling to match SRAM/DMA configurations and overlapping DMA with compute. <code class="language-plaintext highlighter-rouge">MLIR-ARX</code>’s ARX dialect makes these constraints explicit in IR.</p> <h3 id="87-design-space-exploration-dse-for-ai-accelerators">8.7 Design Space Exploration (DSE) for AI Accelerators</h3> <p>DSE methods co-optimize accelerator architecture and mapping. Frameworks like ZigZag<d-cite key="zigzag"></d-cite> generate and evaluate architecture–mapping pairs with cost models for area, energy, and performance, achieving significant energy gains over baseline mappings. Tools such as Timeloop<d-cite key="timeloop"></d-cite> and Accelergy<d-cite key="accelergy"></d-cite> focus on loop mapping and energy estimation.</p> <p>These DSE strategies are applicable to RISC-V–integrated accelerators, where FPGA/SoC constraints require balancing performance and resource use. In <code class="language-plaintext highlighter-rouge">MLIR-ARX</code>, the cost model and candidate selection can be extended to search over accelerator microarchitectures and RISC-V/accelerator interface options, further tightening the compile–measure loop.</p> <h3 id="88-positioning-of-mlir-arx">8.8 Positioning of MLIR-ARX</h3> <p><code class="language-plaintext highlighter-rouge">MLIR-ARX</code> complements the hardware and DSE work above by embedding accelerator capability models in IR, using profile-guided cost modeling to identify profitable offloads, inserting legal DMA/synchronization with overlap, and retargeting automatically to new accelerators or configurations without model changes. Hardware advances supply the building blocks; <code class="language-plaintext highlighter-rouge">MLIR-ARX</code> provides the IR-level integration and automation in a RISC-V EDA flow.</p> <h3 id="89-mlir-centric-compiler-stacks-and-hlsrtl-generation">8.9 MLIR-centric compiler stacks and HLS/RTL generation</h3> <p>MLIR has increasingly been used not only as a software-oriented IR but also as a hardware-construction and HLS coordination layer. The CIRCT project extends MLIR with hardware-facing dialects (e.g., <code class="language-plaintext highlighter-rouge">hw</code>, <code class="language-plaintext highlighter-rouge">comb</code>, <code class="language-plaintext highlighter-rouge">sv</code>, <code class="language-plaintext highlighter-rouge">fsm</code>) and export passes to synthesizable SystemVerilog, enabling end-to-end generation of RTL directly from MLIR programs <d-cite key="circt"></d-cite>. For dataflow-style accelerators, the <code class="language-plaintext highlighter-rouge">handshake</code> and <code class="language-plaintext highlighter-rouge">staticlogic</code> dialects capture fine- and coarse-grain control and enable automated scheduling/retiming before \emph{ExportVerilog}. These flows complement software-oriented tensor dialects by giving a path to hardware under the same abstraction umbrella.</p> <p>A second line of work connects MLIR to C/C++ code generation as an HLS front end. The <code class="language-plaintext highlighter-rouge">EmitC</code> path lowers structured MLIR to portable C++ suitable for downstream toolchains, including HLS compilers, while preserving shape and buffer semantics. Building on this idea, ScaleHLS uses MLIR to drive loop transformations (tiling, unrolling, pipelining) and memory partitioning so that the emitted C/C++ attains predictable quality-of-results when synthesized by commercial HLS tools <d-cite key="scalehls"></d-cite>. This separation—high-level legality and transformation in MLIR, hardware construction in HLS—aligns with our design where algorithmic and storage decisions are expressed in IR.</p> <p>Dynamic and elastic dataflow HLS has also been explored atop MLIR. Dynamatic integrates with MLIR’s <code class="language-plaintext highlighter-rouge">handshake</code> pipeline to generate elastic circuits that tolerate variable-latency operators and memory, bringing modulo scheduling and token-based control to the HLS space <d-cite key="dynamatic"></d-cite>. At the coarse-grain end, MLIR-based AIE flows target CGRA-like AI engines (e.g., Xilinx/AMD AI Engine), using MLIR dialects to express tile-local compute, DMA, and interconnect routing before producing device-ready binaries <d-cite key="mliraie"></d-cite>. Finally, the Calyx project exposes a hardware-centric intermediate representation and an MLIR dialect that make resource sharing, banking, and control explicit, providing another route from MLIR programs to verifiable RTL generators <d-cite key="calyx"></d-cite>.</p> <p>Position relative to our system. These efforts show two practical integration patterns for MLIR: (i) MLIR $\to$ RTL via CIRCT-style dialects, and (ii) MLIR $\to$ C/C++ via EmitC/ScaleHLS for HLS back ends. <code class="language-plaintext highlighter-rouge">MLIR-ARX</code> currently focuses on partitioning and orchestrating offload regions under a unified IR for CPU and accelerator execution, but its capability model and region formation are compatible with both patterns: the same <code class="language-plaintext highlighter-rouge">arx</code> ops can be lowered either to MMIO-driven stubs (our VTA/RVX path) or, in a future backend, to HLS-friendly C++ or directly to RTL through CIRCT. This suggests a path to automatically synthesize specialized accelerators for hot regions discovered by the profile-guided flow, while preserving the CPU fallback and the EDA integration boundary already in place.</p> <hr> <h1 id="9-vta-configuration-and-expected-performance-on-fpga"><strong>9. VTA Configuration and Expected Performance on FPGA</strong></h1> <h3 id="91-hardware-overview">9.1 Hardware overview</h3> <p>VTA is a soft deep-learning accelerator intended for FPGAs. It implements a decoupled three-stage pipeline (load, compute, store) with dedicated on-chip buffers and task queues, enabling overlap of DMA and compute when tiling admits double buffering<d-cite key="vta,ml2tuner25"></d-cite>. The architecture exposes configuration knobs for arithmetic precision, on-chip buffer sizes, and the inner matrix-multiply shape that together determine tile fit, bandwidth demand, and achievable utilization.</p> <h3 id="92-configuration-reported-on-zcu102">9.2 Configuration reported on ZCU102</h3> <p>As background, we refer to the ZCU102-oriented VTA configuration summarized in <a href="#hardware-vta">Table 2</a>, reported by prior work<d-cite key="ml2tuner25"></d-cite>. Values are expressed in log2 form in TVM/VTA’s JSON; for clarity we additionally list the corresponding bit-widths and buffer capacities. The reported buffer sizes are one step larger than the common defaults, chosen to better utilize Zynq UltraScale+ resources.</p> <p><a id="hardware-vta"></a></p> <table> <caption> Table 2. VTA configuration on Xilinx ZCU102 (derived from the configuration used in EVTA<d-cite key="ml2tuner25"></d-cite>). </caption> <thead> <tr> <th style="text-align:left;">Attribute</th> <th style="text-align:right;">JSON (log2)</th> <th style="text-align:left;">Interpreted value</th> <th style="text-align:left;">Effect</th> </tr> </thead> <tbody> <tr> <td><code>LOG_INP_WIDTH</code></td> <td style="text-align:right;">3</td> <td>8-bit int</td> <td>input precision</td> </tr> <tr> <td><code>LOG_WGT_WIDTH</code></td> <td style="text-align:right;">3</td> <td>8-bit int</td> <td>weight precision</td> </tr> <tr> <td><code>LOG_ACC_WIDTH</code></td> <td style="text-align:right;">5</td> <td>32-bit int</td> <td>accumulator precision</td> </tr> <tr> <td><code>LOG_BATCH</code></td> <td style="text-align:right;">0</td> <td>1</td> <td>batch factor in intrinsic</td> </tr> <tr> <td><code>LOG_BLOCK</code></td> <td style="text-align:right;">4</td> <td>16</td> <td>inner GEMM tile (PE block)</td> </tr> <tr> <td><code>LOG_UOP_BUFF_SIZE</code></td> <td style="text-align:right;">16</td> <td>64 KiB</td> <td>micro-op buffer</td> </tr> <tr> <td><code>LOG_INP_BUFF_SIZE</code></td> <td style="text-align:right;">16</td> <td>64 KiB</td> <td>input buffer</td> </tr> <tr> <td><code>LOG_WGT_BUFF_SIZE</code></td> <td style="text-align:right;">19</td> <td>512 KiB</td> <td>weight buffer</td> </tr> <tr> <td><code>LOG_ACC_BUFF_SIZE</code></td> <td style="text-align:right;">18</td> <td>256 KiB</td> <td>accumulator buffer</td> </tr> </tbody> </table> <h3 id="93-implications-for-tiling-and-overlap">9.3 Implications for tiling and overlap</h3> <p>Given <code class="language-plaintext highlighter-rouge">LOG_BLOCK</code>=4, the intrinsic compute block is $16{\times}16{\times}16$.</p> <p>Legal tiles must satisfy buffer capacity and alignment constraints for the three-stage pipeline:</p> <p>(i) an input/weight sub-tile that fits {64 KiB, 512 KiB} with layout-specific padding. (ii) an accumulator tile that fits 256 KiB. (iii) DMA chunking that aligns with the memory interface.</p> <p>When these constraints are met, double buffering allows:</p> \[T_{\mathrm{off}}\approx T_{\mathrm{setup}}+\max(T_{\mathrm{dma}},T_{\mathrm{cmp}})+T_{\mathrm{sync}},\] <p>and hides the smaller of DMA/compute times.</p> <h3 id="94-mapping-to-mlir-arx">9.4 Mapping to MLIR-ARX</h3> <p>In <code class="language-plaintext highlighter-rouge">mlir-arx</code>’s YAML capability description, the configuration in <a href="#hardware-vta">Table 2</a> becomes the static part of the accelerator model (precision, intrinsic block, buffer capacities). The partitioner only lifts regions whose tiles provably fit, and the cost model accounts for (i) the $\max(T_{\mathrm{dma}},T_{\mathrm{cmp}})$ overlap enabled by double buffering, (ii) setup/synchronization, and (iii) memory-traffic inflation from packing and padding. Under multi-VTA, the resource model exposes the number of instances and the shared DRAM bandwidth so that selection can avoid overcommitting the memory system.</p> <hr> <h1 id="10-arx-dialect"><strong>10. ARX Dialect</strong></h1> <p><strong>Selected Operations and Capability Schema</strong></p> <p>This section sketches the subset of ARX operations and attributes that our prototype uses to plan and legalize offload regions. The design mirrors MLIR’s convention of making capabilities explicit at IR boundaries so that legality and code generation are mechanically checkable.</p> <h3 id="101-core-ops-and-attributes">10.1 Core ops and attributes</h3> <p><a href="#arx-ops">Table 3</a> summarizes representative ops and their key attributes. The attributes are chosen so that (i) legality checks are local, (ii) tiling constraints can be statically validated, and (iii) DMA and compute costs can be derived from sizes and layouts.</p> <p><a id="arx-ops"></a></p> <table> <caption>Table 3. Selected ARX ops and attributes used in the prototype.</caption> <thead> <tr> <th style="text-align:left;">Op</th> <th style="text-align:left; width:5cm;">Key attributes</th> <th style="text-align:left; width:6cm;">Notes</th> </tr> </thead> <tbody> <tr> <td><code>arx.conv2d</code></td> <td><code>dtype, strides, dilations, padding, tile_h, tile_w, ic_blk, oc_blk</code></td> <td>Convolution lifted from <code>linalg.conv_*</code>. Tiling attributes reflect scratchpad fit and inner GEMM blocking.</td> </tr> <tr> <td><code>arx.gemm</code></td> <td><code>dtype, M_blk, N_blk, K_blk</code></td> <td>Canonicalized matmul; blocks must be multiples of the accelerator's intrinsic block.</td> </tr> <tr> <td><code>arx.pool</code></td> <td><code>mode, kH, kW, strides</code></td> <td>Optional; emitted only when the accelerator implements pooling.</td> </tr> <tr> <td><code>arx.copy</code></td> <td><code>src_space, dst_space, bytes, align</code></td> <td>Logical copies across host/device address spaces. Lowered to DMA descriptors when possible.</td> </tr> <tr> <td><code>arx.region</code></td> <td><code>reads, writes, sram_bytes</code></td> <td>Opaque offload region container; captures side conditions (aliasing, fences).</td> </tr> </tbody> </table> <h3 id="102-example-lifting">10.2 Example lifting</h3> <p>A legal <code class="language-plaintext highlighter-rouge">linalg.matmul</code> with shapes that fit the intrinsic block becomes:</p> <pre style="color:#111827; background:#f3f4f6">
%y = arx.gemm  %a, %b
     { dtype = i8, M_blk = 16, N_blk = 16, K_blk = 16 } : ...
</pre> <hr> <h1 id="11-profiling-instrumentation-and-log-schema"><strong>11. Profiling Instrumentation and Log Schema</strong></h1> <h3 id="111-instrumentation-ops">11.1 Instrumentation ops</h3> <p>Profiling is injected by a dedicated pass when a command-line flag is set. The ops are intentionally minimal so that they can be stripped late in the pipeline.</p> <ul> <li><code class="language-plaintext highlighter-rouge">arx.prof.begin handle: i64 {counters = [cycles, bytes_rd, bytes_wr]}</code></li> <li><code class="language-plaintext highlighter-rouge">arx.prof.end handle: i64</code></li> </ul> <p>The pass places <code class="language-plaintext highlighter-rouge">begin/end</code> around candidate ops and region boundaries after bufferization, ensuring that the measured bytes reflect concrete <code class="language-plaintext highlighter-rouge">memref</code> layouts and copies.</p> <h3 id="112-runtime-counters-and-emission">11.2 Runtime counters and emission</h3> <p>On RVX, the runtime reads CPU cycle counters and DMA byte counters at <code class="language-plaintext highlighter-rouge">begin/end</code>. Each record is keyed by the IR handle (a stable 64-bit hash).</p> <pre style="color:#111827; background:#f3f4f6">
record {
  handle: 0x17a3...
  cycles:  239812
  bytes_rd:  1572864
  bytes_wr:   262144
  stalls: {dma_wait: 0.12, sram_bank: 0.03}
}
</pre> <p>In instrumented CPU-only builds, median overhead was about 2.1% on MNIST-sized graphs (illustrative; replace with measured values in <a href="#inference-time-arx">Table 1</a>).</p> <hr> <h1 id="12-cost-model-details-and-selection-algorithm"><strong>12. Cost Model Details and Selection Algorithm</strong></h1> <h3 id="121-timing-model">12.1 Timing model</h3> <p>For a region $R$ with tiled compute and double buffering:</p> \[T_{\mathrm{off}}(R) \approx T_{\mathrm{setup}} + \max\!\big(T_{\mathrm{dma}}(R), T_{\mathrm{cmp}}(R)\big) + T_{\mathrm{sync}}.\] <p>DMA time uses transferred bytes and effective bandwidth $B_{\mathrm{dma}}$, including packing/padding inflation $\rho \ge 1$:</p> \[T_{\mathrm{dma}}(R) = \frac{\rho\cdot(\mathrm{bytes}_{\mathrm{in}}+\mathrm{bytes}_{\mathrm{out}})}{B_{\mathrm{dma}}}.\] <p>Compute time is derived from MAC counts divided by peak MAC/s and corrected by a profile-derived efficiency $\eta\in(0,1]$:</p> \[T_{\mathrm{cmp}}(R) = \frac{\mathrm{MACs}(R)}{\eta\cdot P_{\mathrm{peak}}}.\] <p>The net benefit is $\Delta T(R) = T_{\mathrm{cpu}}(R) - T_{\mathrm{off}}(R)$.</p> <h3 id="122-resource-model">12.2 Resource model</h3> <p>Each candidate $c$ has a resource vector $\mathcal{R}(c)$ over {LUT, FF, DSP, BRAM, SRAM, DMA lanes}. Selection maximizes $\sum \Delta T(c)$ subject to $\sum \mathcal{R}(c) \le B$ with a benefit-density tie-breaker when budgets are tight.</p> <h3 id="123-region-formation">12.3 Region formation</h3> <p>Candidates are merged greedily into maximal regions when:</p> <ol> <li>data dependencies allow reordering or fusion,</li> <li>the merged tile still fits on-chip buffers, and</li> <li>the merged cost is superadditive after accounting for fewer host–device crossings.</li> </ol> <hr> <h1 id="13-mnist-model-shapes-and-mapping-notes"><strong>13. MNIST Model Shapes and Mapping Notes</strong></h1> <p>For reproducibility and debugging, <a href="#mnist-shapes">Table 4</a> lists the operator shapes used in the early evaluation.</p> <p><a id="mnist-shapes"></a></p> <table> <caption>Table 4. MNIST operator shapes and the induced tiling on configuration B.</caption> <thead> <tr> <th style="text-align:left;">Op</th> <th style="text-align:left;">Input shape</th> <th style="text-align:left;">Weight shape</th> <th style="text-align:left;">Output shape</th> <th style="text-align:left;">Tile selection</th> </tr> </thead> <tbody> <tr> <td>conv1</td> <td>1×1×28×28</td> <td>16×1×3×3</td> <td>1×16×26×26</td> <td>M=16, N=16, K=16 blocks</td> </tr> <tr> <td>conv2</td> <td>1×16×13×13</td> <td>32×16×3×3</td> <td>1×32×11×11</td> <td>same as above</td> </tr> <tr> <td>gemm</td> <td>1×512</td> <td>512×10</td> <td>1×10</td> <td>M=16, N=16, K=16 with packing</td> </tr> </tbody> </table> <hr> <h1 id="acknowledgments">Acknowledgments</h1> <p>This work was supported by the Institute of Information \&amp; Communications Technology Planning \&amp; Evaluation (IITP)grant funded by the Korea government (MSIT) (No.RS-2024-00459797, No.RS-2023-00277060, No.RS-2025-02217404, No.RS-2025-02214497, No.RS-2025-02216517)</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/blog/assets/bibliography/2025-09-29-mlir-arx-accelerator-aware-mlir-to-risc-v-compilation-integrated-with-an-eda-flow.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 UniReps Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>