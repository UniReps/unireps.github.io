<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Failures in Perspective-Taking of Multimodal AI Systems | UniReps Blog </title> <meta name="author" content="UniReps Blog"> <meta name="description" content="An investigation into the spatial reasoning abilities of multimodal LLMs."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/blog/assets/img/unireps_favicon.png?3a42502b41deab62714411b8479222c3"> <link rel="stylesheet" href="/blog/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unireps.org//blog/2024/failures_perspectivetaking/"> <script src="/blog/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/blog/assets/js/distillpub/template.v2.js"></script> <script src="/blog/assets/js/distillpub/transforms.v2.js"></script> <script src="/blog/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Failures in Perspective-Taking of Multimodal AI Systems",
            "description": "An investigation into the spatial reasoning abilities of multimodal LLMs.",
            "published": "November 20, 2024",
            "authors": [
              
              {
                "author": "Bridget Leonard",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Psychology, University of Washington",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Kristin Woodard",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Psychology, University of Washington",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Scott O. Murray",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Psychology, University of Washington",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog//"> <span class="font-weight-bold">UniReps</span> Blog </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Failures in Perspective-Taking of Multimodal AI Systems</h1> <p>An investigation into the spatial reasoning abilities of multimodal LLMs.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <ul> <li> <a href="#definitions-and-terminology">Definitions and Terminology</a> </li> <li> <a href="#creating-a-new-benchmark">Creating a New Benchmark</a> </li> </ul> <div> <a href="#methods">Methods</a> </div> <ul> <li> <a href="#chain-of-thought-prompting">Chain of Thought Prompting</a> </li> </ul> <div> <a href="#results">Results</a> </div> <ul> <li> <a href="#level-1">Level 1</a> </li> <li> <a href="#level-2-spatial-and-visual-judgments">Level 2 Spatial and Visual Judgments</a> </li> <li> <a href="#chain-of-thought">Chain of Thought</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <div class="caption"> Listen to the AI-generated podcast based on our preprint or check out the benchmark paper and project on GitHub: </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/blog/assets/audio/2024-11-20-failures_perspectivetaking/podcast.mp3" controls=""></audio> </figure> </div> </div> <div style="display: flex; align-items: center;"> <a href="https://github.com/bridgetleonard2/perspectiveTaking" style="margin-left: 70px; margin-right: 80px; display: inline-block;" rel="external nofollow noopener" target="_blank"> <img src="assets/img/2024-11-20-failures_perspectivetaking/github-mark.png" alt="GitHub" class="logo" width="40"> </a> <a href="https://arxiv.org/abs/2409.13929" style="display: inline-block;" rel="external nofollow noopener" target="_blank"> <img src="assets/img/2024-11-20-failures_perspectivetaking/arxiv-logomark-small@2x.png" alt="arXiv" class="logo" width="30"> </a> </div> <h2 id="introduction">Introduction</h2> <p>Recent research in AI has exposed a critical limitation: the inability of current models to effectively perform spatial reasoning tasks. Despite their impressive visual perception capabilities, these models struggle to understand spatial relationships and make inferences about them. While previous research has explored aspects of spatial cognition in AI, it often lacks the specificity characteristic of human spatial cognition studies. In cognitive psychology, tasks are carefully designed to isolate distinct processes, enabling precise measurement and minimizing bias or reliance on alternative strategies. To bridge the gap between cognitive science and artificial intelligence, we focus on a fundamental aspect of human spatial reasoning: visual perspective-taking.</p> <blockquote> <p><strong>Visual perspective-taking</strong> is the ability to mentally simulate a viewpoint other than one’s own. It allows us to understand the relationship between objects and how we might have to manipulate a scene to align with our perspective, which is essential for tasks like navigation and social interaction.</p> </blockquote> <p>By leveraging established methodologies, we can rigorously evaluate AI’s spatial cognition, starting with perspective-taking. The extensive human literature on spatial reasoning offers a valuable benchmark, enabling comparisons between model performance and the human developmental trajectory. This comparison helps identify critical gaps and opportunities for enhancing AI models.</p> <p>Our aim was to create a targeted perspective-taking benchmark for multimodal AI systems, probing various levels and components of the cognitive process.</p> <h3 id="definitions-and-terminology">Definitions and Terminology</h3> <ul> <li> <p><strong>Level 1 Perspective-taking</strong> refers to knowing that a person may be able to see something another person does not</p> </li> <li> <p><strong>Level 2 Perspective-taking</strong> refers to the ability to represent how a scene would look from a different perspective</p> </li> <li> <p><strong>Mental Rotation</strong> where one imagines an object or scene rotating in space to align with a perspective</p> </li> <li> <p><strong>Spatial vs Visual Judgments</strong> responding to queries about the spatial orientations of objects or their non-spatial visual characteristics</p> </li> </ul> <details><summary>Click here to learn more about perspective-taking</summary> <p>In the human developmental literature, perspective-taking has been stratified into two levels, defined above. Based on developmental literature, level 1 perspective-taking appears fully developed by the age of two <d-cite key="moll2006level1"></d-cite>. In contrast, although success on some simple Level 2 tasks is first seen around age 4 <d-cite key="newcombe1992children"></d-cite>, Level 2 perspective-taking continues to develop into middle childhood <d-cite key="surtees2012egocentrism"></d-cite> and even into young adulthood <d-cite key="dumontheil2010online"></d-cite>. In terms of measurement, a common Level 1 task might ask if an object is viewable (or positioned to the front or back) of a person or avatar in a scene. Level 2 is often measured by having subjects assess the spatial relationship between objects.</p> <p>A more specific cognitive process, <strong>mental rotation</strong>, where one imagines an object or scene rotating in space to align with a perspective, plays an important role in perspective-taking. Surtees et al. <d-cite key="surtees2013similarities"></d-cite> experimentally manipulated Level 1 and Level 2 perspective-taking by presenting participants with tasks where they viewed numbers or blocks relative to an avatar. Different stimuli were used to elicit visual and spatial judgments, like whether the number was a “6” or a “9” from the person’s perspective, or if the block was to the person’s right or left. Level 1 tasks involved indicating whether the number/block was visible to the avatar, while Level 2 involved reporting either the number seen by the avatar or whether it was to the avatar’s left or right (Level 2). For both visual and spatial judgments, response times were longer for Level 2 tasks as the angular difference between the avatar and the participant increased, while response times remained unaffected by the angle in Level 1 tasks. This increase in response time when the participant’s view was unaligned with the avatar’s perspective is attributed to the mental rotation process, either rotating the scene or rotating one’s own reference frame to align with the avatar.</p> </details> <hr> <h3 id="creating-a-new-benchmark">Creating a New Benchmark</h3> <h4 id="limitations-of-current-benchmarks">Limitations of Current Benchmarks</h4> <p>There are two main limitations current AI spatial cognition assessment:</p> <details><summary>Reasoning with language alone can inflate performance on spatial benchmarks</summary> <p>Text-only GPT-4 achieves a score of 31.4, while multimodal GPT-4v achieves a score of 42.6 on the spatial understanding category of Meta’s openEQA episodic memory task <d-cite key="majumdar2024openeqa"></d-cite>. The strong baseline score achieved by the text-only GPT-4 suggests that many “real-world” questions based on visual scenes can be deduced linguistically. Additionally, the limited improvement when moving from a blind LLM to a multimodal one suggests that vision models do not gain a significant understanding of space beyond what can be inferred through language.</p> </details> <details><summary>Benchmark scores can be hard to interpret since models often perform poorly</summary> <p>BLINK <d-cite key="fu2024blink"></d-cite>, a benchmark more specifically focused on visual perception capabilities, contains categories related to spatial cognition, such as relative depth and multi-view reasoning. On this benchmark, GPT-4v achieved an accuracy of 51.26%, only 13.17% higher than random guessing and 44.44% lower than human performance. When benchmarks are highly focused on visuospatial tasks, the significant shortcomings of multimodal models suggest that further advancements are needed before these models can reliably perform in real-world scenarios. Even within specific categories, it is often difficult to determine <em>why</em> models fail on certain tasks while succeeding on others, as these failures cannot be easily linked to the absence of a particular cognitive process.</p> </details> <p>To target some of these issues, we apply established tasks in cognitive psychology that measure spatial cognition in a precise manner. By applying these tasks to AI systems, we gain not only improved measurement precision but also the ability to compare AI performance with human development, providing clear insights into model limitations and areas for improvement.</p> <h4 id="perspective-taking-benchmark">Perspective Taking Benchmark</h4> <p>Leveraging the distinction between Level 1 and Level 2 perspective-taking <d-cite key="surtees2013similarities"></d-cite>, we propose a small perspective-taking benchmark that assesses multimodal model capabilities across three tasks: Level 1, Level 2 with spatial judgments, and Level 2 with visual judgments. Although human performance remains stable regardless of judgment type, we include this differentiation of Level 2 stimuli to examine potential egocentric biases that may arise in multimodal models when interpreting spatial relations compared to optical character recognition (OCR). This benchmark aims to address gaps in current AI spatial cognition measures by increasing process specificity, limiting language-based solutions, and offering straightforward comparisons to human cognition.</p> <hr> <h2 id="methods">Methods</h2> <p>Our study utilized GPT-4o (“gpt-4o-2024-05-13” via OpenAI’s API) to conduct a series of perspective-taking experiments designed to capture the system’s spatial reasoning abilities. We kept <code class="language-plaintext highlighter-rouge">top_p = 0.5</code> to restrict the model from choosing from the top 50% of words that could come next in its response.</p> <p>Our experimental design was inspired by previous studies that evaluated viewpoint dependence using targets like toy photographers [2] and avatars with blocks [12]. In our study, we used an avatar as a target and different stimuli, either cubes with numbers and letters or cubes and spheres, to investigate the influence of visual and spatial judgments on model performance. Each task consisted of 16 trial types, featuring images at 8 different angles (0°, 45°, 90°, 135°, 180°, 225°, 270°, 315°) with 2 response options for each task (e.g., cube in front or behind, 6/9 or M/W on the cube, and cube left or right).</p> <p>Ten iterations of each image were passed through the model to calculate the percentage of correct responses.</p> <table> <thead> <tr> <th>Task</th> <th>Example Stimulus</th> <th>Prompt</th> </tr> </thead> <tbody> <tr> <td>Level 1</td> <td><img src="assets/img/2024-11-20-failures_perspectivetaking/infront_behind_ex.jpg" alt='Level 1: "IN FRONT" 45°' width="300"></td> <td>For the following images respond with in front or behind to indicate if the cube is in front or behind from the perspective of the person.</td> </tr> <tr> <td>Level 2: Spatial Judgment</td> <td><img src="assets/img/2024-11-20-failures_perspectivetaking/left_right_ex.jpg" alt='Level 2 Spatial: "RIGHT" 225°' width="300"></td> <td>For the following images respond with left or right to indicate if the cube is to the left or to the right from the perspective of the person.</td> </tr> <tr> <td>Level 2: Visual Judgment</td> <td><img src="assets/img/2024-11-20-failures_perspectivetaking/number_ex.jpg" alt='Level 2 Visual: "6" 90°' width="300"></td> <td>For the following images respond with 6 or 9 to indicate if the number on the cube is a 6 or a 9 from the perspective of the person.</td> </tr> <tr> <td>Level 2: Visual Judgment</td> <td><img src="assets/img/2024-11-20-failures_perspectivetaking/letter_ex.jpg" alt='Level 2 Visual: "W" 315°' width="300"></td> <td>For the following images respond with M or W to indicate if the letter on the cube is an M or a W from the perspective of the person.</td> </tr> </tbody> </table> <h3 id="chain-of-thought-prompting">Chain of Thought Prompting</h3> <p>To further examine how language might be used to solve spatial tasks, we included chain-of-thought prompting to the Level 2 spatial task with the prompt:</p> <p>“Analyze this image step by step to determine if the cube is to the person’s left or right, from the person’s perspective. First, identify the direction the person is looking relative to the camera. Second, determine if the cube is to the left or right, relative to the camera. Third, if the person is facing the camera, then from their perspective, the cube is to the inverse of the camera’s left or right. If the person is facing away from the camera, then the cube is on the same side as seen from the camera. Respond with whether the cube is to the person’s left or right.”</p> <hr> <h2 id="results">Results</h2> <h3 id="level-1">Level 1</h3> <p>GPT-4o performed with near-perfect accuracy on 6 out of the 8 image angles as seen below. Its poor performance on 0° images is likely due to an accidental viewpoint where the avatar blocked one of the shapes. However, poor performance on 315° image types is less interpretable, especially in contrast to GPT-4o’s impressive performance on 45° images, which have the same angular perspective.</p> <div class="l-page"> <iframe src="/blog/assets/plotly/2024-11-20-failures_perspectivetaking/infront_behind.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <h3 id="level-2-spatial-and-visual-judgments">Level 2 Spatial and Visual Judgments</h3> <p>As previously mentioned, human response times increase on perspective-taking tasks as the angular difference between the target and observer increases <d-cite key="surtees2013similarities"></d-cite>. We administered the task to a small number of human participants and replicated this effect with both our stimuli types, finding a bell-shaped curve in the relationship between response time and angle. Response times peaked when the target required a full mental rotation (180°), as seen in the green line in the figure below. As expected, GPT-4o struggled with the task when mental rotation was involved, beginning around a 90° angular difference. Interestingly, in both tasks, GPT-4o exhibited a response bias toward either “left” or “6” or “W” when the angular difference of the avatar is 90° or 135° in either direction. This likely reflects uncertainty from an egocentric perspective, and thus, a default to one response over another.</p> <div class="l-page"> <iframe src="/blog/assets/plotly/2024-11-20-failures_perspectivetaking/subplots.html" frameborder="0" scrolling="no" height="550px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <h3 id="chain-of-thought">Chain of Thought</h3> <p>GPT-4o performance significantly improved with chain-of-thought prompting on 180° stimuli. However, this linguistic strategy did not improve the model’s ability to handle intermediate rotations between 90° and 180°. This suggests that while language can convey some level of spatial information, it lacks the precision required for human-level spatial cognition. This demonstration of surface-level perspective-taking abilities can partially explain how multimodal models achieve high performance on certain spatial benchmarks.</p> <div class="l-page"> <iframe src="/blog/assets/plotly/2024-11-20-failures_perspectivetaking/cot.html" frameborder="0" scrolling="no" height="450px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <hr> <h2 id="conclusion">Conclusion</h2> <p>With this project, we highlight the value of applying cognitive science techniques to explore AI capabilities in spatial cognition.</p> <ol> <li> <p>We investigated GPT-4o’s perspective-taking abilities, finding it fails when there is a large difference between image-based and avatar-based perspectives</p> </li> <li> <p>We developed a targeted set of three tasks to assess multimodal model performance on Level 1 and Level 2 perspective-taking, with spatial and visual judgments</p> <ul> <li> <p>GPT-4o can do Level 1, aligning with the spatial reasoning abilities of a human infant/toddler</p> </li> <li> <p>GPT-4o fails on Level 2 tasks when mental rotation is required—the avatar’s perspective is not aligned with image perspective</p> </li> </ul> </li> <li> <p>We investigated if chain-of-thought prompting could elicit more spatial reasoning through language</p> <ul> <li>This enabled GPT-4o to succeed on 180° tasks, but it continued to fail at intermediate angles, underscoring its limitations in performing true mental rotation</li> </ul> </li> </ol> <p>While GPT-4o’s performance decreases on tasks that humans typically solve using mental rotation, this does not necessarily indicate that GPT-4o struggles with or cannot perform mental rotation. Instead, it suggests that GPT-4o likely employs a fundamentally different strategy to approach these tasks. Rather than engaging in mental rotation, GPT-4o appears to rely primarily on image-based information processing. We found more support for this when testing an open prompt for Level 2 visual images that did not specify which letters or numbers to respond with. GPT-4o often responded with “E” and “0” for images around a 90° angular difference, where from the image view, an M/W would look like an E, and a 9/6 would look like a 0.</p> <p>It could be that current multimodal models aren’t trained on the appropriate data to achieve the reasoning necessary for Level 2 perspective-taking. However, considering the developmental trajectory of humans, it becomes evident that this issue may not be solely data-related. Level 2 perspective-taking typically develops between the ages of 6 and 10 <d-cite key="frick2014picturing"></d-cite><d-cite key="frick2018measuring"></d-cite>, even after children have had exposure to extensive amounts of “data” through experience. This late development suggests that the challenge may be more computational than data-driven. Specifically, this ability likely relies on computations occurring outside of the visual and language networks, perhaps in areas responsible for cognitive processes like mental rotation or spatial transformation or even theory of mind <d-cite key="gunia2021brain"></d-cite><d-cite key="schurz2013common"></d-cite><d-cite key="surtees2013use"></d-cite><d-cite key="surtees2013similarities"></d-cite>. While the argument that better or more focused training data could improve model performance remains valid, it is possible that entirely new computational strategies are needed to mirror the complex, integrative processes that enable Level 2 reasoning in humans.</p> <p>This project demonstrates the potential of cognitive science methods to establish baselines for AI assessment. Using these well-established techniques, we achieve clear, interpretable measures that are less susceptible to bias. Additionally, these measures can be directly compared to human performance and developmental trajectories, providing a robust framework for understanding AI’s strengths and weaknesses in relation to well-researched human cognitive processes.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/blog/assets/bibliography/2024-11-20-failures_perspectivetaking/2024-11-20-failures_perspectivetaking.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"UniReps/unireps.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 UniReps Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>