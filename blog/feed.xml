<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://unireps.org//blog/feed.xml" rel="self" type="application/atom+xml"/><link href="https://unireps.org//blog/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-16T16:03:43+00:00</updated><id>https://unireps.org//blog/feed.xml</id><title type="html">blank</title><entry><title type="html">Do Sparse Autoencoders (SAEs) transfer across base and finetuned language models?</title><link href="https://unireps.org//blog/2025/do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/" rel="alternate" type="text/html" title="Do Sparse Autoencoders (SAEs) transfer across base and finetuned language models?"/><published>2025-02-06T00:00:00+00:00</published><updated>2025-02-06T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models</id><content type="html" xml:base="https://unireps.org//blog/2025/do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/"><![CDATA[<h1 id="tldr-executive-summary"><strong>TLDR</strong> (Executive Summary)</h1> <ul> <li>We explored <strong>whether Sparse Autoencoders (SAEs)</strong> can effectively transfer from base language models to their finetuned counterparts, focusing on two base models: <a href="https://huggingface.co/google/gemma-2b">Gemma-2b</a> <d-cite key="gemmateam2024gemmaopenmodelsbased"></d-cite> and <a href="https://huggingface.co/mistralai/Mistral-7B-v0.1">Mistral-7B-V0.1</a> <d-cite key="jiang2023mistral7b"></d-cite> (we tested finetuned versions for coding and mathematics respectively)</li> <li>In particular, we split our analysis into three steps: <ol> <li>We analysed the similarity (<strong>Cosine and Euclidian Distance</strong>) of the residual activations, which was <strong>highly correlated with the resulting transferability of the SAEs</strong> for the two models.</li> <li>We computed several performance metrics (L0 Loss, Reconstruction CE Loss, Variance Explained) of the base SAEs on the fine-tuned models. Almost all metrics agreed on a <strong>significant degradation of the SAE performance for the Gemma-2b</strong> model, and <strong>remained within a reasonable range for the Mistral-7B model</strong>, indicating a much better transferability.</li> <li>We took a further step by operationalizing the idea of transferability of SAE from base models to fine-tuned models by applying an <a href="https://transformer-circuits.pub/2023/monosemantic-features#phenomenology-universality">approach from Towards Monosemanticity</a><d-cite key="bricken2023monosemanticity"></d-cite> for studying feature universality through <strong>feature activation similarity</strong> and <strong>feature logit similarity</strong>. These similarity scores were mostly consistent with the results from the previous step, albeit with one caveat for the Gemma-2b model, suggesting that <strong>some SAE features may still transfer</strong> even if the overall SAE performance is poor for the finetuned model.</li> </ol> </li> <li>Overall, our results agree with <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">previous work that studied Instruct models</a><d-cite key="sae_finetuning"></d-cite>. That is, SAEs transferability seems to be model-dependent and sensitive to the finetuning process.</li> <li>We make our <a href="https://github.com/tommasomncttn/SAE-Transferability">code repository public</a> to facilitate future work in this direction.</li> </ul> <hr/> <h1 id="1-introduction-and-motivation">1. Introduction and motivation</h1> <h2 id="11-what-are-saes-and-why-do-we-care-about-them">1.1 What are SAEs and why do we care about them</h2> <p>We find ourselves in a world where we have machines that speak fluently dozens of languages, can do a wide variety of tasks like programming at a reasonable level, <strong>and we have no idea how they do it!</strong> This is a standard <strong>mechanistic interpretability</strong> (a.k.a. mech interp) pitch - a field that is trying to <strong>express neural networks’ behaviours as human-understandable algorithms</strong>, i.e. <strong>reverse engineer</strong> algorithms learned by a neural network (or a model, in short). The main motivation is that even though we know the exact form of computation being done by the model to transform the input (e.g. text prompt) to the output (e.g. text answer), we don’t know <em>why</em> this computation is doing what it’s doing, and this is a major concern from a standpoint of AI Safety. The model can perform the computation because it’s genuinely trained to perform the task well, or because it learned that doing the task well correlates with its other learned goals like gaining more power and resources. Without understanding the computation, we have no direct way of distinguishing between the two.</p> <p>The solution proposed by mechanistic interpretability is closely analogous to reverse engineering ordinary computer programs from their compiled binaries. In both cases, we have an intrinsically non-interpretable model of computation - a sequence of binary instructions performed on a string of 0s and 1s, and the (mathematical) function of the neural network’s architecture applied with its learned parameters (weights)<d-footnote>This is a pretty important analogy to understand and you can read more about it in [this Anthropic post](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)<d-cite key="Olah_2022"></d-cite> where it's explained better. </d-footnote>. Programmers know that a natural way to think about computer programs is mapping <strong><em>variables</em></strong> into other variables (or new states of existing variables), starting from some pre-initialized state. So, reverse engineering complied binaries boils down to (oversimplifying) identifying binary memory segments that correspond to variables, tracking how these segments change as the program is being executed, coming up with the explanations of the purpose of these variables, and ultimately arriving at the replication of the program source code - a sequence of human-understandable instructions.</p> <p>But what makes us think that the same is possible for neural networks, especially the ones as large as the current Large Language Models (LLMs)? In particular, why should we even expect that neural networks solve tasks similarly to humans, and thus adopt the same “variable-centered” model of computation? While the proof-of-existence for the first question appeared relatively early (see <a href="https://distill.pub/2020/circuits/zoom-in/">Circuits thread by Chris Olah et al.</a><d-cite key="olah2020zoom"></d-cite> for CNNs or a <a href="https://arxiv.org/abs/2301.05217">more recent work by Neel Nanda et al.</a><d-cite key="nanda2023progressmeasuresgrokkingmechanistic"></d-cite> for language models), the second question is a more general claim, and thus requires more general evidence. The first fundamental work that provided such evidence was the <a href="https://transformer-circuits.pub/2023/monosemantic-features">“Towards Monosemanticity” paper by Anthropic</a><d-cite key="bricken2023monosemanticity"></d-cite>, which introduced Sparse Autoencoders (SAEs) for interpreting the language models’ activations. The activations are any intermediate state of the models’ computation, such as residual stream, MLP layers etc. and can be seen as analogous to a program’s memory state. And just as the program’s memory state can be decomposed into variables, the <strong>main purpose of SAEs is to decompose models’ activations into features</strong>.</p> <p>A feature, in general, is a fuzzy term, and you can find some good attempts to define it <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=BQds7CQ8ytq2rolt7p0XQPbt">here</a><d-cite key="nanda_2022"></d-cite>. For this post we’ll use the analogy with variables and link it to a very general definition of a feature as “<em>a</em> <em>property of the input</em>”. The link is pretty natural: <strong>the types and the number of variables a programmer needs to solve a task depends on the task itself</strong> (i.e. on the problem input). So for a model it would seem reasonable if it used different kinds of variables/features depending on its input: you don’t need a feature “this line is inside a for-loop in Python” in a poetry text, or a feature “this word rhymes with ‘sunset’” in the Python code. And given that models have a finite amount of parameters (which limits a total number of variables they can use), we should expect that they will utilize this kind of input-specificity to use as many unique features as they need to perform a specific task.</p> <p>Why are sparse autoencoders called sparse? It’s actually deeply linked with the idea from the previous paragraph: if you want to use many features in a limited activation space (limited by a number of neurons), you have to exploit the fact that <strong>for any input, most of the features will not be there</strong>. So given that modern language models are trained to predict a next token in a huge variety of possible inputs, we should expect that any feature learned by the model will be <strong>sparse</strong>, i.e. it <strong>will be used by the model only for a small fraction of all possible inputs</strong>.</p> <p>But wait, how is it even possible for a model to learn input-specific features if it has a low-dimensional activations space (where dimension equals the number of neurons) but a very high-dimensional input space? The answer is <strong><em>superposition</em></strong> - an idea of exploiting feature sparsity to store more features than dimensions in the activation space. It has a rich mathematical background and we invite an unfamiliar reader to learn more about it in the <a href="https://transformer-circuits.pub/2022/toy_model/index.html">“Toy Models of Superposition” paper by Elhage et al.</a><d-cite key="elhage2022superposition"></d-cite></p> <p>Coming back to SAEs, they were introduced with all of these ideas in mind to <em>solve superposition</em>, i.e. to recover more than <em>n</em> features in an <em>n</em>-dimensional activation space of a model. How are they supposed to do it? The answer is once again in the name - <em>autoencoders</em>, which means that SAEs are neural networks with the “autoencoder” architecture, which is illustrated in a diagram below (borrowed from the great <a href="https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html">Adam Karvonen’s post</a><d-cite key="Karvonen_2024"></d-cite>):</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/sae.png" alt="My Image" width="500"/> </div> <p>So the model activations are “encoded” into a high-dimensional vector of feature activations (top right, note that it always has many more elements than the model’s input), and this high-dimensional vector (a.k.a. “code”) is “decoded” back to reconstruct the input, hence the name “autoencoder”. We advise the reader to take a quick look at the <a href="https://transformer-circuits.pub/2023/monosemantic-features#appendix-autoencoder">“Towards monosematicity” appendix</a><d-cite key="bricken2023monosemanticity"></d-cite> where this architecture is presented mathematically<d-footnote>Note that it's different from the diagram in two ways: adding biases vectors **b** and using a transposed encoder/decoder matrix compared to what is seen in the diagram.</d-footnote>, but the core point to understand is that we’re interested in the right part of the above diagram: <strong>how the reconstructed activations are decomposed into a linear combination of feature vectors</strong> from the Decoder matrix (with the weights of a linear combination equal to SAE <em>feature activations</em>, due to how matrix-vector multiplication works). Mathematically, it means that for each input \(x^j\) (which is the model’s activation vector at the place where we ‘attach’ the SAE - residual layer, hidden head activations etc.), we’re looking to express it in the following form:</p> \[\mathbf{x}^j \approx \mathbf{b} + \sum_i f_i(\mathbf{x}^j) \mathbf{d}_i\] <p>where \(f_i(\mathbf{x}) = \text{ReLU}\left( \mathbf{W}_{enc} \mathbf{x} + \mathbf{b}_{enc} \right)_i\) are the feature activations that are computed in the left (“encoder”) part of the diagram, and \(\mathbf{d}_i\) are the rows of the decoder matrix (or columns, if you take the transpose and multiply from the other side). Note that the diagram omits bias vectors \(\mathbf{b}\) for simplicity, but conceptually they don’t change much: instead of decomposing the activation space, we’re decomposing a translation of that space by a fixed vector (because this is just easier for an SAE to learn).</p> <p>If you think about it, it’s exactly what we hoped to do in an analogy with decomposing program memory into variable names! The variables are now features - <strong>vectors (directions) in the activation space</strong>. And <em>if</em> the autoencoder is doing a good job at reconstructing the input, we can expect that this decomposition (and hence the features) to make sense!</p> <p>The last part is tricky though. Unlike variables that are deliberately used by humans to write sensible algorithms, there is no reason to expect that the features we recover with an SAE will be <em>interpretable</em> in a sense that a human can understand on which inputs they activate and can predict their “roles” based on that (e.g. which tokens they help to predict). But this is where the <em>sparsity</em> condition comes in: we don’t only want an SAE to reconstruct the input from a high-dimensional feature-activation representation, <strong>but we also want this representation to be sparse</strong>, i.e. have only a handful of non-zero feature activations at a time. We already touched on the reason for this - the hope is that we’ll be able to recover the “true” features used by the model in this way<d-footnote>It's quite a slippery area to consider the logical relationship between the feature quality of being "truly used" by the model (analogously to correctly recovered variables from the compiled binary) and its interpretability. If the model came up with some genius way to solve a particular task using features no human can comprehend, would they still be considered as interpretable? The answer can vary from "no" to "kind of yes", because it can be argued that humans with their evolutionally developed problem-solving skills can eventually understand (i.e. interpret) how things work, even though it may not be obvious at a first glance. It's also discussed by Neel Nanda [here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=dzkF4Sh89hg1GUJj5h2TiGVx)<d-cite key="nanda_2022"></d-cite> </d-footnote>. And the way this is achieved is by imposing an L1-loss penalty on the feature activation vector, which intuitively incentivizes the model to not learn any features unless they are really useful in reconstructing the input<d-footnote>There's also a better justified mathematical reason for sparsity, greatly explained [here](http://ufldl.stanford.edu/tutorial/unsupervised/SparseCoding/)<d-cite key="Ng"></d-cite>. Essentially, by learning to decompose the model's activation space into feature activations, we're trying to find an overcomplete basis of feature directions (a basis with more than n vectors in an n-dimensional space), which is impossible to do without imposing some additional criteria. The ["Toy Models of Superposition"](https://transformer-circuits.pub/2022/toy_model/index.html)<d-cite key="elhage2022superposition"></d-cite> is also incredibly helpful to refine one's intuition about this. </d-footnote>.</p> <h3 id="111-sae-features-for-ai-safety">1.1.1 SAE features for AI Safety</h3> <p>The traditional view in mech interp has been that <strong>one cannot interpret the model’s weights if one cannot interpret the neurons that the weights are connecting</strong>. But due to the <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=RDddls6iedarJZiVvLWwnaYI">neurons polysemanticity</a><d-cite key="nanda_2022"></d-cite> (a consequence of superposition), interpreting individual neurons in the language model is extremely hard if at all possible. That’s where SAEs come to the rescue: by revealing the directions in the neuron activation space (i.e. features) that have a clear, interpretable meaning, they allow for a new form of <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=GeeSfnALcakOYfxQcKaAwV6x">circuits</a><d-cite key="nanda_2022"></d-cite> analysis: instead of interpreting weights between neurons, we can instead interpret weights connecting features. Thus the SAE features potentially serve as a new “basis” for circuit analysis, and some of the recent work e.g. by <a href="https://arxiv.org/abs/2403.19647">Marks et al.</a><d-cite key="marks2024sparsefeaturecircuitsdiscovering"></d-cite> and <a href="https://transformer-circuits.pub/2024/march-update/index.html#feature-heads">Batson et al.</a><d-cite key="Batson_Chen_Jones_2024"></d-cite> has already started exploring this idea and producing the first results.</p> <p>So what does this mean for AI Safety? We’ll cite the Anthropic team’s view on this topic (layed out in their <a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html#safety">“Interpretability Dreams”</a><d-cite key="Olah_2023"></d-cite> post and in the <a href="https://transformer-circuits.pub/2022/toy_model/index.html#strategic">“Strategic Picture” section</a><d-cite key="elhage2022superposition"></d-cite> of the Toy Models paper):</p> <blockquote> <p>We’d like a way to have confidence that models will never do certain behaviors such as “deliberately deceive” or “manipulate.” Today, it’s unclear how one might show this, but we believe a promising tool would be the ability to identify and enumerate over all features.</p> </blockquote> <blockquote> <p>Ultimately we want to say that a model doesn’t implement some class of behaviors. Enumerating over all features makes it easy to say a feature doesn’t exist (e.g. “there is no ‘deceptive behavior’ feature”) but that isn’t quite what we want. We expect models that need to represent the world to represent unsavory behaviors. But it may be possible to build more subtle claims such as “all ‘deceptive behavior’ features do not participate in circuits X, Y and Z.”</p> </blockquote> <p>Summarizing, the hope is to be able to prove statements of the following form:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/eq.png" alt="My Image" width="500"/> </div> <h2 id="12-finetuning-models-is-a-challenge-to-ai-safety---saes-to-the-rescue">1.2 Finetuning models is a challenge to AI safety - SAEs to the rescue?</h2> <p>After outlining the procedure behind SAE-interpretability, we can answer a more general question: why is it relevant to translate the matrix language of neural networks (not more understandable to us than binary code) into a human-readable algorithmic language? There are several reasons, but, among the others, once we are able to do so, we can understand what features of an input a model identifies before predicting an answer. This can allow us to identify when a model is learning to deploy features spuriously correlated with the actual labels (an intuitive example <a href="https://ar5iv.labs.arxiv.org/html/1712.02950#:~:text=these%20image%20domains.-,2,Hidden%20Information,-We%20begin%20with">here</a><d-cite key="DBLP:journals/corr/abs-1712-02950"></d-cite>) or when the model is even <a href="https://arxiv.org/abs/2310.06824">lying to us</a><d-cite key="marks2024geometrytruthemergentlinear"></d-cite>. In both of these cases, it is a primary safety concern that these behaviors are not occurring in our model when used in production. Moreover, SAE-interpretability allows us to gain some insight into solving these problems precisely!</p> <p>Nevertheless, reality is often rougher than abstraction, and mechanistic interpretability suffers from one big problem: once we crack the interpretation of a model, we are only able to decode what is going on inside <strong>a singular, particular model, and not all models with the same architecture and different weights</strong>. Luckily, to have a model that shows emergent abilities, <a href="https://epochai.org/blog/compute-trends">we need a lot of compute</a><d-cite key="computetrends"></d-cite>, which remarkably restricts the Pareto frontier of competitive models and therefore the number of pre-trained models that we need to interpret. Therefore, one could think that if we manage to get some good SAE-interpreters for these few, we will be done. This may not be true! While indeed there are few state-of-the-art models, there are tons of finetuned versions of them (<a href="https://twitter.com/ClementDelangue/status/1839375655688884305?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1839375655688884305%7Ctwgr%5Eb537ad0b54dfc2d9ec69e2b01a337c5b0ce9d4e9%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Freadwrite.com%2Fai-startup-hugging-face-reaches-one-million-downloadable-ai-models-thats-a-lot-you-have-never-heard-of%2F">hugging face reached 1 million of models</a>), which are quite cheap to obtain compared to pretraining. <strong>If a simple finetuning will make the model uninterpretable, then we might be in danger</strong>. This could be the case, as <a href="https://arxiv.org/abs/2310.02949">previous studies</a><d-cite key="yang2023shadowalignmenteasesubverting"></d-cite> showed that alignment can be erased with a small finetuning. Then we ask ourselves:</p> <p><em>Is the interpretability of a model as weak as alignment to finetuning?</em></p> <p>In this post, we try to answer these questions and extend the positive results derived from a similar study by <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite>, where SAEs for the residual stream have been shown to be easily transferable (at the cost of some finetuning).</p> <p>Lastly, we want to remark how this kind of study derives its importance from the weakness of outer alignment forced by some ad-hoc finetuning. Indeed, if interpretability is more resistant to being broken than alignment, the path towards AI safety could be reached via <a href="https://www.cold-takes.com/high-level-hopes-for-ai-alignment/">digital neuroscience</a><d-cite key="Karnofsky_2023"></d-cite>, rather than simply through external finetuning.</p> <hr/> <h1 id="2-problem-setup">2. Problem setup</h1> <p>In choosing finetuned models to work with, we tried to strike a balance between the potential relevance of these models (how many people will actually use similar models), and the availability of pre-trained SAEs from the <a href="https://jbloomaus.github.io/SAELens/">SAELens</a><d-cite key="bloom2024saetrainingcodebase"></d-cite> library we used. So, we arrived at the following models and their finetunes:</p> <ol> <li>Gemma-2b (v1) -&gt; <a href="https://huggingface.co/shahdishank/gemma-2b-it-finetune-python-codes">Gemma-2b-it-finetuned-python-codes</a><d-cite key="gemmateam2024gemmaopenmodelsbased"></d-cite> finetune on <strong>Python code</strong> by Dishank Shah.</li> <li>Mistral-7B (v0.1) -&gt; <a href="https://huggingface.co/meta-math/MetaMath-Mistral-7B">MetaMath-Mistral-7B</a><d-cite key="jiang2023mistral7b"></d-cite> finetune on <strong>math problems</strong> by Meta from their <a href="https://arxiv.org/abs/2309.12284">MetaMath paper</a><d-cite key="yu2024metamath"></d-cite> by Yu et al.</li> </ol> <p>We then loaded the following SAEs for these models from SAELens (SAE layer numbering starts from 0):</p> <table style="margin: auto; text-align: center;"> <thead> <tr> <th>Model</th> <th>SAE Release</th> <th>SAE Layer</th> <th>N Features</th> </tr> </thead> <tbody> <tr> <td>Gemma-2b (v1)</td> <td>gemma-2b-res-jb by Joseph Bloom</td> <td>Residual layer #6</td> <td>16384</td> </tr> <tr> <td>Mistral-7B (v0.1)</td> <td>mistral-7b-res-wg by Josh Engels</td> <td>Residual layer #8</td> <td>65536</td> </tr> </tbody> </table> <p>Two important things to note:</p> <ul> <li>Gemma-2b SAE was trained on the <em>base</em> Gemma-2b model, while our Gemma-2b finetune was obtained from the <em>instruct</em> model, so there was one more “finetuning step” compared to the Mistral-7B case.</li> <li>Both finetunes that we used are <em>full</em> finetunes (with respect to the base model), i.e. no layer was frozen during the finetuning process. This is important for our SAE study, because all SAEs would trivially generalize (in terms of their reconstruction quality) if they were applied at the layer where activations are not affected a priori by the finetuning process.</li> </ul> <h2 id="21-studying-default-transferability">2.1 Studying “default” transferability</h2> <p>Similarly to what <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite> did with the instruct models, we’ll study the SAE transferability “by default”. That is, we’ll take an SAE trained on the base model, and apply it to the finetuned model to see if it maintains its performance (operationalized below). We won’t do any additional finetuning of our SAEs (on the activations from the finetune model), but as the same results from <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite> indicate: even when SAEs do not transfer by default, they can be finetuned relatively cheaply to recover their performance.</p> <p>Prior to evaluating the SAEs’ performance, we computed different similarity metrics for residual stream activations at the specific layer our SAEs are used for. The goal was to obtain some sort of a prior probability that our SAEs will transfer to the finetune model: the more similar the activations are, the higher is the (expected) probability that our SAEs will transfer. On the one hand, this analysis can be used as a <em>first step to select a fine-tuned model</em> from the thousands available on Hugging-Face. On the other hand, further studies can try to analyze <em>whether the phenomenon of SAE transferability actually correlates with the difference between activations</em> of the base and fine-tuned models (which we treat here only as an unproven heuristic).</p> <h2 id="22-evaluating-saes-performance">2.2 Evaluating SAEs performance</h2> <p>Designing rigorous approaches to evaluate the SAEs’ performance is an open problem in mechanistic interpretability. The main complicating factor is that we’re interested not so much in the SAEs reconstructed output, but rather in <strong>the SAE feature activations and feature vectors</strong>. However, measuring whether the SAEs features are interpretable or whether the features “are truly used by the model” is not straightforward. For our work, we’ll just start with computing standard evaluation metrics proposed either in the original “Towards monosemanticity” paper, or used in the later work, <a href="https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream">e.g. this one by Joseph Bloom</a><d-cite key="jbloom_lesswrong"></d-cite>:</p> <ol> <li><strong>L0 loss</strong>, namely the number of non-zero values in the feature activations vector. If the features retain their sparsity, we should expect L0 loss to be low compared to the total number of features, with the fraction being usually less than 1% (\(\frac{L_0}{N_{\text{features}}} &lt; 0.01\))</li> <li><strong>Reconstruction Cross-Entropy (CE) loss</strong> (a.k.a. substitution loss) which is computed as follows: <ol> <li>Run the model up to the layer where we apply the SAE, get this layer’s activations</li> <li>Run the activations through the SAEs, obtaining the reconstructions</li> <li><strong>Substitute</strong> the original activations with the reconstructed activations, continue the forward pass of the model, and get the corresponding cross-entropy loss</li> </ol> </li> <li><strong>Variance explained</strong>, is one of the standard ways to measure the difference of original activations and the activations reconstructed by the SAE. Specifically, we’ll use \(R^2\) score a.k.a. <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">Coefficient of determination</a></li> <li><strong>Feature density histograms</strong>: <a href="https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream#Why_can_training_Sparse_AutoEncoders_be_difficult__">as explained by Joseph Bloom</a><d-cite key="jbloom_lesswrong"></d-cite>, ideally the features should be “within good sparsity range”: <strong>not too sparse</strong> (e.g. when the features are “dead” and never activate) and <strong>not too dense</strong> (e.g. activating in more than 10% of the inputs). In both edge cases, anecdotally the features are mostly uninterpretable. One (rather qualitative) way to check this is to plot feature histograms: <ol> <li>Run a given sample of tokens through the model, and get the SAE feature activations.</li> <li>For each feature, record the number of times (tokens) it had a non-zero activation.</li> <li>Divide by the total number of tokens to get the fraction, and take the log10 of it (adding some epsilon value to avoid log-of-zero)</li> <li>Plot the histogram of the resulting log-10 fractions (the number of histogram samples equals to the number of features)</li> </ol> </li> </ol> <p>We’ll compute these metrics first for the base model and its SAE to get a baseline, then for the finetuned model with the same SAE, and compare the resulting metrics against the baseline<d-footnote>Even though density histograms are not technically a metric, we can infer quantitative metrics from them like the number of dead features</d-footnote>. The dataset used in both cases is the original training dataset of the corresponding SAE:</p> <ol> <li><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb">Fineweb</a><d-cite key="fineweb"></d-cite> dataset for Gemma-2b.</li> <li><a href="https://huggingface.co/datasets/monology/pile-uncopyrighted">The Pile</a><d-cite key="thepile"></d-cite> dataset for Mistral-7B.</li> </ol> <p>Based on the feature density histograms, we additionally zoomed in on individual features to see how well they transfer using <a href="https://transformer-circuits.pub/2023/monosemantic-features#phenomenology-universality">feature activation similarity and logit weight similarity</a><d-cite key="bricken2023monosemanticity"></d-cite>, as elaborated in the later section of this post.</p> <hr/> <h1 id="3-how-similar-are-residual-activations-of-finetuned-models">3. How similar are residual activations of finetuned models?</h1> <p>Before analyzing the SAE metrics on the finetuned models, we will visualize some easier computations on the <strong>residual</strong> activations (at the residual stream of the layer where we apply the corresponding SAE) to get a sense of the SAE transferability. Specifically, we are interested in the similarities between the base and finetuned model activations. We consider two metrics: the Cosine Similarity and the Euclidian Distance, for the model and datasets specified above with the <a href="https://huggingface.co/shahdishank/gemma-2b-it-finetune-python-codes">Gemma-2b Python-codes</a> and <a href="https://huggingface.co/meta-math/MetaMath-Mistral-7B">Mistral-7b MetaMath</a><d-cite key="yu2024metamath"></d-cite> finetunes respectively.</p> <p>Computing the Cosine Similarities and Euclidian Distances of the activations yields a tensor of shape <code class="language-plaintext highlighter-rouge">[N_BATCH, N_CONTEXT]</code> (each token position is determined by its batch number and position in the context). A simple metric to start with is to consider the global mean of the Cosine Similarities of the activations across both batch and context dimensions, giving a single scalar representing the overall similarity. This can be seen in the following table:</p> <table> <thead> <tr> <th>Model/Finetune</th> <th>Global Mean (Cosine) Similarity</th> </tr> </thead> <tbody> <tr> <td>Gemma-2b/Gemma-2b-Python-codes</td> <td>0.6691</td> </tr> <tr> <td>Mistral-7b/Mistral-7b-MetaMath</td> <td>0.9648</td> </tr> </tbody> </table> <p>This already suggests much better transferability of the Mistral-7b SAE for its MetaMath finetune. For a more fine-grained comparison, we flatten the similarities into a <code class="language-plaintext highlighter-rouge">N_BATCH * N_CONTEXT</code> vector and plot the histogram across all tokens:</p> <p>Gemma-2b - Cosine Similarity Histogram</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.1.png" alt="My Image" width="700"/> </div> <p>Mistral-7b - Cosine Similarity Histogram</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.2.png" alt="My Image" width="700"/> </div> <p>Gemma-2b - Euclidian Distance Histogram</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.3.png" alt="My Image" width="700"/> </div> <p>Mistral-7b - Euclidian Distance Histogram</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.4.png" alt="My Image" width="700"/> </div> <p>We can see how the Cosine Similarities for Mistral-7b are concentrated around a value close to 1, whereas the Gemma-2b similarities are more spread around the mean of 0.66 (higher variance). The Euclidian Distances histogram shows a similar distinction, with the Gemma-2b distances being spread around a mean of around 120, while the bulk of Mistral-7b distances stay at a low value.</p> <p>We also visualize the per-context mean of Cosine Similarities and Euclidian Distances. We compute the mean across batches but preserve the context dimension, giving a tensor of shape <code class="language-plaintext highlighter-rouge">[N_CONTEXT]</code>, which reflects how similarity changes over the context length.</p> <p>Gemma-2b - Cosine Similarity Context Line</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.5.png" alt="My Image" width="700"/> </div> <p>Mistral-7b - Cosine Similarity Context Line</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.6.png" alt="My Image" width="700"/> </div> <p>Gemma-2b - Euclidian Distance Context Line</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.7.png" alt="My Image" width="700"/> </div> <p>Mistral-7b - Euclidian Distance Context Line</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.8.png" alt="My Image" width="700"/> </div> <p>In the above, we can see how the similarities and distances stabilise quickly after a few tokens of context, albeit around different values. Both models start with close to 1 similarity for the first token, and then stabilize after a few tokens.</p> <p>These results already anticipate a considerable difference in the transferability of the SAEs for the two models, which will be explored more in-depth in the following section.</p> <hr/> <h1 id="4-how-well-do-the-base-saes-work-on-the-finetuned-models">4. How well do the base SAEs work on the finetuned models?</h1> <h2 id="41-methodology">4.1 Methodology</h2> <p>In this section, we’ll compute a set of standard SAE metrics for base and finetuned models, using the same base SAE in both scenarios (i.e., the SAE that was trained on the base model activations):</p> <ol> <li>For the <strong>base model</strong>: <ol> <li>we sample input tokens from the <strong>original SAE training dataset</strong></li> <li>pass the tokens through the base model to get <strong>the model’s activations</strong></li> <li>pass the activations through the SAE to <strong>get the feature activations</strong></li> <li>complete the forward pass of the base model to <strong>get the final loss</strong> (used afterward for the reconstructed loss)</li> </ol> </li> <li>Then we repeat the same steps for the <strong>finetuned</strong> <strong>model</strong>, using the same tokens dataset</li> <li>Finally, we compute the metrics mentioned in the Evaluating SAEs performance section.</li> </ol> <h2 id="42-technical-details">4.2 Technical Details</h2> <p>Before delving deeper into the results, we want to point out three technical details:</p> <ol> <li>The sample size used across nearly all experiments is <strong>256K tokens</strong></li> <li> <p>Similarly to <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite> we observed a major numerical instability when computing our reconstruction loss and variance explained metrics. As the authors noted:</p> <blockquote> <p>SAEs fail to reconstruct activations from the opposite model that have outlier norms (e.g. BOS tokens). These account for less than 1% of the total activations, but cause cascading errors, so we need to filter these out in much of our analysis.</p> </blockquote> </li> <li> <p>To solve this problem we used a similar outlier filtering technique, where an outlier is defined as <em>an activation vector whose (L2) norm exceeds a given threshold</em>. We tried several ways to find a “good” threshold and arrived at values similar to those used by <em>Kissane et al</em>:</p> <ul> <li><strong>290 norm value</strong> for the Gemma-2b model</li> <li><strong>200 norm value</strong> for the Mistral-7B model</li> </ul> <p>Using these threshold values, we found that <strong>only 0.24% activations are classified as outliers in the Gemma-2b model</strong>, and <strong>0.7% in the Mistral-7B</strong>, agreeing with the Kissane et al. result that these outliers account for less than 1% of activations. It should be noticed, however, that we <em>only used this outlier filtering technique for our reconstruction loss &amp; variance explained</em> experiments to avoid numerical errors. In practice, it means that for this experiment the true sample size was a little smaller than for the other experiments, equal to \(\left( 1 - \text{outlier_fraction} \right) \times 256{,}000\) with the \(\text{outlier_fraction}\) defined above.</p> </li> </ol> <h2 id="43-results">4.3 Results</h2> <p>In the following table, we report the results for the first experiment with the <strong>Mistral</strong> model pair:</p> <table style="margin: auto; text-align: center;"> <thead> <tr> <th>Model\\Metric</th> <th>L0 Loss</th> <th>Clean CE Loss</th> <th>Reconstruction CE Loss</th> <th>Loss Delta</th> <th>$$R^2$$ Score (Variance Explained)</th> <th>Dead Features (%)</th> </tr> </thead> <tbody> <tr> <td>Mistral-7B</td> <td>83.37</td> <td>1.78</td> <td>1.93</td> <td><b>0.15</b></td> <td>0.68</td> <td>0.76%</td> </tr> <tr> <td>Mistral-7B MetaMath</td> <td>90.22</td> <td>1.94</td> <td>2.1</td> <td><b>0.16</b></td> <td>0.58</td> <td>0.64%</td> </tr> </tbody> </table> <p>As you can see, the L0-Loss of the features and variance explained increase a bit, but the reconstruction loss delta is almost the same! It suggests that our Mistral SAE may still transfer after finetuning, although with a slightly worse reconstruction quality. Let’s compare these results with the Gemma-2b and its Python finetune:</p> <table style="margin: auto; text-align: center;"> <thead> <tr> <th>Model\\Metric</th> <th>L0 Loss</th> <th>Clean CE Loss</th> <th>Reconstruction CE Loss</th> <th>Loss Delta</th> <th>$$R^2$$ Score (Variance Explained)</th> <th>Dead Features (%)</th> </tr> </thead> <tbody> <tr> <td>Gemma-2b Base</td> <td>53.59</td> <td>2.65</td> <td>3.16</td> <td>0.51</td> <td>0.97</td> <td>48.1%</td> </tr> <tr> <td>Gemma-2b Python-codes</td> <td>84.74</td> <td>3.29</td> <td><b>7.5</b></td> <td><b>4.21</b></td> <td><b>-10.27</b></td> <td>0.1%</td> </tr> </tbody> </table> <p>Now, this is what <em>bad</em> SAE transferability looks like! But actually this should come as no surprise after the <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite> result: they concluded that Gemma-2b SAEs do not transfer even between the base and the <em>instruct</em> models, so when you add an additional finetuning step on top of the instruct, it’s completely expected that the metrics will get even worse. The authors explain this behavior with an abnormal weights deviation in the instruct model:</p> <blockquote> <p>Here we show that the weights for Gemma v1 2B base vs chat models are unusually different, explaining this phenomenon (credit to Tom Lieberum for finding and sharing this result):</p> </blockquote> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.1.png" alt="My Image" width="700"/> </div> <p>But what effect does this have on the SAE features? Well, we could expect that if an SAE is no longer able to reconstruct the input activations, it will always “hallucinate” - any features it “detects” will not make any sense. Let’s see if this expectation holds in practice for the Gemma-2b model.</p> <p>We’ll start with the feature activations histogram plot. In general, this kind of histogram gives little insight since you will always have a large mode at 0 due to feature sparsity, and some kind of log-normal distribution at non-zero activations. Indeed, this is what happens in the base Gemma-2b model, when we plot its log10 feature activations histogram:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.2.png" alt="My Image" width="700"/> </div> <p>Two things to note:</p> <ul> <li>The first bar’s count value is <strong>clipped</strong> - it’s much larger than 900k, equal to more than 6 million.</li> <li>We used a smaller sample size for this experiment due to the need to store all the feature activations in memory to plot the histogram - here the sample size is equal to <strong>128K</strong>.</li> </ul> <p>With this in mind, let’s compare it with the same kind of histogram for our Gemma-2b finetune (where the features are given by the same SAE):</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.3.png" alt="My Image" width="700"/> </div> <p>If that’s not a characterization for “cursed”, we don’t know what is! Instead of a nice bell curve, we now have some sort of a 3-mode monster in the non-zero activations section. To be clear - nothing like that was present when we repeated this experiment for the Mistral-7B: we obtained the well-expected bell curves with similar mean and standard deviation for both base and finetuned models. We don’t have a good explanation for this Gemma-2b anomaly, but we’ll try to give some deeper insight into what happens with the SAE features in the next section.</p> <p>Let’s move on to the feature densities plot, which was produced as described in the Evaluating SAEs Performance section. Starting from Gemma-2b:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.4.png" alt="My Image" width="700"/> </div> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.5.png" alt="My Image" width="700"/> </div> <p>As expected from the above results, the two plots have little in common. We see that most of our dead features (in the base model) turn alive in the finetuned one! To see where exactly these dead feature densities land in the finetuned model (what are their new densities), we also made a parallel coordinate plot (below we show two versions of the same plot: with different density ranges highlighted):</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.6.png" alt="My Image" width="700"/> </div> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.7.png" alt="My Image" width="700"/> </div> <p>So it looks like the dead features spread out quite widely in the finetuned model, contributing to more probability mass before the -3 log-density. As for the dense features (-4 to -1 log density) in the base model, their density interval gets squeezed to (-3, -1) in the finetuned model, causing a sharp mode near the -2.5 log-density value.</p> <p>We’ll continue the Gemma-2b investigation in the next chapter, and conclude this section with the Mistral-7B feature density histograms:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.8.png" alt="My Image" width="700"/> </div> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.9.png" alt="My Image" width="700"/> </div> <p>We can see that for Mistral the feature densities distribution almost doesn’t change after the model finetuning! The only slight difference is in the number of dead features: the finetuned Mistral has around 80 dead features less than the base one. To zoom in closer, we also show the parallel coordinate plot:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.10.png" alt="My Image" width="700"/> </div> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.11.png" alt="My Image" width="700"/> </div> <p>So yes, a small number of features do turn alive, but also some features (even a smaller amount) turn dead in the finetuned model! Overall though, the feature densities look very similar, with the Pearson correlation of their log10 densities equal to 0.94 (versus 0.47 for the Gemma-2b case).</p> <hr/> <h1 id="5-do-the-base-sae-features-transfer-to-the-finetuned-model">5. Do the base SAE features transfer to the finetuned model?</h1> <p>We want to motivate this section with a more thoughtful consideration of the question <strong>what is the best way to operationalize SAE transferability</strong>. In the previous section, we simply checked the standard SAE evaluation metrics to see how well they reconstruct the activations. But this doesn’t necessarily reflect the main goal of using SAEs - <strong>interpreting the model.</strong></p> <p>As noted in the SAE features for AI Safety section of our post, the end goal of using SAEs for interpretability is to be able to <strong>use features as the basis for circuit analysis</strong>. And if we assume that some kind of circuit analysis has been done for the base model to prove that it doesn’t implement certain undesirable behaviors, the most ambitious operationalization of SAE transferability (for AI Safety) would be the ability to apply <strong>the same kind of circuit analysis with the same SAE</strong> (or the finetuned one) <strong>to prove or disprove that the finetuned model is safe.</strong></p> <p>In our case of studying transferability “by default”, the better way to demonstrate it is to show that our SAE features “stay relevant” in the finetuned model, so that we can expect that they still potentially serve as the basis for circuit analysis. Showing this rigorously would be a really difficult task (partly because there’s no standard way to do circuit analysis in the SAE basis yet) and it’s out of scope for this blog post. What we did instead is apply an <a href="https://transformer-circuits.pub/2023/monosemantic-features#phenomenology-universality">approach from Towards Monosemanticity</a><d-cite key="bricken2023monosemanticity"></d-cite> for studying features <strong>universality</strong>:</p> <ul> <li>Normally to study if a feature from model A is conceptually the same (has the same “role” in the model) as another feature in the model B, one can compute <ul> <li><strong>feature activation similarity</strong>: represent a feature as a vector of its activations across a given sample of tokens, obtaining a <em>feature activations vector →</em> do it for model A’s feature, model B’s feature and compute a <strong>correlation between their activations vectors</strong>.</li> <li><strong>feature logits similarity:</strong> represent a feature as a vector of its <a href="https://transformer-circuits.pub/2023/monosemantic-features#feature-arabic-effect">logit weights</a><d-cite key="bricken2023monosemanticity"></d-cite> (for each token of the vocab a logit weight is the relative probability of that token as predicted by the feature direct effect), obtaining a <em>feature logit vector→</em> do it for model A’s feature, model B’s feature and compute a <strong>correlation between their logit vectors</strong>.</li> </ul> </li> <li>So, we call model A our base model, model B - the corresponding finetune, and compute feature activation similarity and logits similarity for a given sample of the SAE features (which are the same for the base and finetuned models).</li> </ul> <p>This can be seen as a (very) rough proxy for “the feature is doing the same job in the finetuned model”, and we call it the “<strong>feature transferability test</strong>”.</p> <h2 id="51-feature-selection-procedures">5.1 Feature Selection Procedures</h2> <p>Conceptually, dead features are completely different from the ordinary features: <a href="https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream">as explained by Joseph Bloom</a><d-cite key="jbloom_lesswrong"></d-cite>, they represent permanently lost capacity in an SAE and thus are merely an artifact of the SAE training<d-footnote>Essentially, an SAE is saying “If I cannot find relevant features for reconstructing my input anymore, I’m going to learn a direction(s) in the activation space that is orthogonal to all the inputs I’ve seen, so that I get zero activations for the features I cannot learn and thus I’m no longer penalized by sparsity, at least”. If a feature was dead in the base model but is no longer dead in the finetuned one, it implies a distributional shift in the activation space (for which the SAE was not adapted, but could potentially be adapted by finetuning)</d-footnote>. So we decided to make a separate analysis of dead features and “<strong>regular</strong>” features, that we defined as <strong>features with a log10 density between -5 and -1.</strong></p> <p>By dead features, we mean features that are <strong>exclusively</strong> dead (never activating across our entire 256K sample of tokens), i.e. <strong>dead only in one of the models</strong>:</p> <ul> <li>a “dead base” feature is a feature that is dead in the base model, but not in the finetuned one</li> <li>a “dead finetune” feature is a feature that is dead in the finetuned model, but not in the base one.</li> </ul> <p>We observe that only a handful of features are dead in both models, so we think our definitions give more information on what we’re analysing.</p> <p>Then, our approach for the rest of this section looks as follows:</p> <ol> <li>We sample max 100 exclusively dead features and 1000 regular features using our density histogram values for each base model and its finetune.</li> <li>We convert these features to their activation vector and logit vector representations for both the base model and its finetune.</li> <li>For each regular feature, we compute their <strong>activation similarity</strong> and the <strong>logits similarity</strong> with respect to the corresponding finetune, and for the exclusively dead features - their <strong>activation error:</strong> <ul> <li>We cannot really compute the activation similarity as a correlation score if one of the feature’s activation vectors is constantly 0, i.e. the feature is dead. In this case we take the log10 of these activation vectors (with <code class="language-plaintext highlighter-rouge">1e-10</code> as the epsilon value to avoid a log of zero), take the <a href="https://en.wikipedia.org/wiki/Mean_absolute_error">Mean Absolute Error</a> of the resulting vectors and call it <strong>activation error</strong><d-footnote>It makes little sense to compute dead features logit similarity: if the feature never activates, it doesn’t matter what its logit effect is - it will never manifest itself in the model. </d-footnote>.</li> </ul> </li> <li>Additionally, we plot a <strong>histogram of similarities</strong> for each feature type, since we observed a significant deviation of the similarity score (mainly activation similarity) in some experiments.</li> </ol> <h2 id="52-gemma-2b-features-transferability-test">5.2 Gemma-2b features transferability test</h2> <p>One could say that in the Gemma-2b case, it’s obvious from the previous results that our SAE doesn’t transfer. But we could imagine a case where <em>some</em> (perhaps a tiny fraction) of our SAE features from the regular density interval do still transfer, so we decided to conduct this experiment anyway.</p> <p>Starting with the features that are exclusively dead in the <em>base</em> model, their mean activation error for Gemma-2b and Gemma-2b python-codes finetune is <strong>0.025</strong>. A histogram of these 100 activation errors is given below:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.1.png" alt="My Image" width="700"/> </div> <p>This made us think that “dead features turning alive” anomaly is not so much of an anomaly, because the dead features activate only (very) slightly in the finetuned model. The max activation value across all 100 dead features in the finetuned model was <strong>1.1,</strong> indicating that our “dead feature direction” is only slightly off in the finetuned model, and can be easily adjusted by SAE finetuning.</p> <p>As for the features that are exclusively dead in the <em>finetune</em> model, Gemma-2b had only two of them on our sample, with the activation error equal to 0.34 and 3.19, which is considerably higher than in the previous case.</p> <p>Moving on to the regular features, we expected to see a much more drastic dissimilarity of their activations. Indeed, the <strong>mean activation similarity for our sample of Gemma-2b regular feature is 0.39</strong>. Let’s check the histogram of these similarity scores:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.2.png" alt="My Image" width="700"/> </div> <p>Interestingly, we see that a small fraction of features (~10%) have an activation similarity above 0.8! This implies that if these features were interpretable in the base model, they will most likely stay interpretable in the finetune model<d-footnote>We didn’t try to manually interpret these features’ activations to verify this claim, and it would be interesting to see future works in this direction</d-footnote>. But we’re not sure about the significance of this result: this could just as well be noise, so we invite further research in this area.</p> <p>As for the logit similarity of these regular features, it turns out it’s much higher than our activation similarity, with a mean value of <strong>0.952.</strong> Looking at the logit similarity scores histogram, it’s also much more concentrated towards the end of the interval:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.3.png" alt="My Image" width="700"/> </div> <p>However, it’s easy to be misled by the mean logits similarity score. What it’s really saying is that our unembedding matrix (which is multiplied by the feature direction to get the logits similarity) hasn’t changed that much after finetuning (with a Frobenius norm ratio equal to 1.117 as we checked for our Gemma finetune). So <em>if the feature has still the same direction, we can indeed say that the “direct feature effect” hasn’t changed in the finetuned model, but we never checked this premise!</em> All we know is that there exist ~10% of features which have reasonably high activation similarity scores with the features from the base model. <em>The key point is that the latter is a statement about the feature’s encoder direction</em> (one that is used to project onto to get the feature’s activation, <a href="https://www.lesswrong.com/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s">explained by Neel Nanda here</a><d-cite key="Nanda_2023"></d-cite>), <em>not the decoder one -</em> which is what we mean when we talk about <em>feature directions. So it could be the case that the feature is still there but changed its direction</em> as discussed in <a href="https://www.lesswrong.com/posts/bsXPTiAhhwt5nwBW3/do-sparse-autoencoders-saes-transfer-across-base-and?commentId=pJHfoZ2GLD8neS57g">this comment,</a><d-cite key="sae_finetuning"></d-cite> it could also be that some features change their directions and the others don’t - it’s impossible to tell when the reconstruction score (e.g. variance explained) is as poor as in the Gemma-2b case.</p> <h2 id="53-mistral-7b-features-transferability-test">5.3 Mistral-7B features transferability test</h2> <p>Here we repeat all the same experiments for Mistral-7B and its MetaMath finetune, and compare the result with the Gemma-2b case.</p> <p>Let’s start with the features that are exclusively dead in the Mistral base model. Their mean activation error is 0.0003, which is almost <em>two orders of magnitude</em> lower than in the Gemma-2b case. The corresponding histogram looks like this:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.4.png" alt="My Image" width="700"/> </div> <p>Once again, the results suggest <em>that even though the dead features in the base model are no longer dead in the finetuned one</em>, they activate really weakly on average, so it should be easy to adjust them with a cheap SAE finetuning.</p> <p>The activation error for the features exclusively dead in the finetuned model tells a similar story:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.5.png" alt="My Image" width="700"/> </div> <p>Here the error is even smaller, implying that even though some features stopped activating after finetuning, their corresponding activation values in the base model were really low. And the features are often uninterpretable in the lowest activation intervals anyway, so it should have a minor overall effect on SAEs transferability.</p> <p>Let’s conclude this section with an analysis of our regular features. As expected from the results of the last section, the activation similarity of these features is quite high, with a mean value of <strong>0.958</strong>. As for the activation scores histogram:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.6.png" alt="My Image" width="700"/> </div> <p>As we can see, the distribution of the scores is strongly attracted to the 0.9-1.0 correlation interval, so we can conclude that SAE feature transferability is significantly high in this case. This is also backed up by the mean logits similarity of 0.9996, and a rather straightforward logits similarity histogram:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.7.png" alt="My Image" width="700"/> </div> <hr/> <h1 id="6-conclusions--limitations">6. Conclusions &amp; Limitations</h1> <h2 id="61-conclusions">6.1 Conclusions</h2> <p>Going back to our original question of <em>“Do SAEs trained on a base model transfer to the finetuned one?”</em>, the most obvious answer that comes to mind now is - it depends! We got drastically different results for our Gemma-2b-python-codes and Mistral-7B-MetaMath finetunes. However, <strong>it seems possible that one could estimate the “degree of transferability” in advance<em>.</em></strong> One method is to compute various weight deviation metrics, such as the one used by <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al</a><d-cite key="sae_finetuning"></d-cite> for Gemma-2b, and another method that we used - to compute activation similarities of the model that are fed into an SAE. Both of these anecdotally correlate with the results of our transferability experiments, but a more thorough study is definitely needed.</p> <p>Another takeaway we’ve had after finishing this post is that <strong>“SAE transferability” can mean different things</strong>. One can utilize the standard SAE evaluation metric to get a high-level evaluation of the SAE quality on the finetuned model, but it doesn’t always give a deeper insight into what happens with the SAE feature once we zoom in (which may be more interesting for the real SAE applications in mech interp). Our Gemma-2b results suggest that some SAE features may still be interpretable, even when finetuning has completely rendered the SAE incapable of reconstructing the input. And although the significance of this result can be rightly questioned, we still think it is interesting to investigate further.</p> <h2 id="62-limitations">6.2 Limitations</h2> <p>The main limitations we see in our work are the following:</p> <ul> <li>It’s not clear how our results will generalize to other finetunes. A more principled approach would be to use a custom finetuning setup, where one could e.g. study the relationship between the amount of compute put into finetuning and some key SAE transferability metrics like the reconstruction loss etc. <ul> <li>Our finetuned models also had almost the same dictionaries as the base model (with the exception of a single padding token), so it’s also not clear whether our results generalize to the finetuned model with significantly modified dictionaries (e.g. language finetunes for languages that were not in the original training dataset of the base model)</li> </ul> </li> <li>We only studied SAEs for a single residual layer for Gemma-2b and Mistral-7B models. A more thorough study is needed to see how these results will vary when considering different layers and different SAE activations, e.g. MLP or hidden head activations.</li> <li>All our experiments were performed on the training dataset of the base SAE, i.e. on the original training distribution of the base models. But the finetuned models are mostly used for tasks that they have been finetuned on, so we definitely need some future work here to extend these results to a more specific setting of finetuned models.</li> <li>Our analysis of SAE features transferability was somewhat superfluous, because we didn’t do a thorough investigation of the interpretability of our features after the finetuning. An even more representative study would be to replicate some kind of circuit analysis in the SAE basis to rigorously prove if (at least some) features are still involved in the same computation of the finetuned model.</li> </ul> <hr/> <h1 id="appendix">Appendix</h1> <p>All code is available on <a href="https://github.com/tommasomncttn/SAE-Transferability">github</a></p>]]></content><author><name>Taras Kutsyk</name></author><category term="sae"/><category term="mechanistic interpretability"/><category term="model diffing"/><summary type="html"><![CDATA[TLDR (Executive Summary)]]></summary></entry><entry><title type="html">UniReps 2024 Awards</title><link href="https://unireps.org//blog/2025/unireps2024awards/" rel="alternate" type="text/html" title="UniReps 2024 Awards"/><published>2025-01-11T00:00:00+00:00</published><updated>2025-01-11T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/unireps2024awards</id><content type="html" xml:base="https://unireps.org//blog/2025/unireps2024awards/"><![CDATA[<p>The second edition of the UniReps Workshop at NeurIPS 2024 was a huge success! Building on the momentum from last year, we were thrilled to welcome over 1,000 participants from academia and industry for a day packed with engaging talks, lively discussions, and an exciting poster session. We received 106 submissions this year, including 25 fantastic proceedings papers that will soon be published in the workshop volume.</p> <p>We couldn’t have done it without all of you! A big thank you to the authors, our amazing program committee of 181 members, the participants who made the event so special, and our sponsors for their incredible support.</p> <p>We were also proud to highlight some exceptional work through our awards. Here’s a look at the winners and their inspiring contributions.</p> <hr/> <h2 id="best-paper-awards">Best Paper Awards</h2> <h3 id="proceedings-track">Proceedings Track</h3> <p><strong>Authors</strong>: <a href="https://sarahharvey.github.io">Sarah Harvey</a>, <a href="https://sites.google.com/view/lipshutz/home">David Lipshutz</a>, and <a href="https://alexhwilliams.info">Alex H. Williams</a></p> <p><strong>Title</strong>: <em>“What Representational Similarity Measures Imply about Decodable Information.”</em> <d-cite key="harvey2024what"></d-cite></p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_best_proceedings_2024.jpg" alt="Best Paper Proceedings Track Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <hr/> <h3 id="extended-abstracts-track">Extended Abstracts Track</h3> <p><strong>Authors</strong>: <a href="https://scholar.google.com/citations?user=EtEVFLoAAAAJ">Richard Antonello</a> and <a href="https://chengemily1.github.io">Emily Shana Cheng</a></p> <p><strong>Title</strong>: <em>“Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models.”</em> <d-cite key="antonello2024evidence"></d-cite></p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_best_abstract_2024.jpg" alt="Best Paper Extended Abstracts Track Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <hr/> <h2 id="honorable-mentions">Honorable Mentions</h2> <h3 id="proceedings-track-1">Proceedings Track</h3> <p><strong>Author</strong>: <a href="https://alexhwilliams.info">Alex H. Williams</a></p> <p><strong>Title</strong>: <em>“Equivalence between Representational Similarity Analysis, Centered Kernel Alignment, and Canonical Correlations Analysis.”</em> <d-cite key="williams2024equivalence"></d-cite></p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_honorable_proceedings_2024.jpg" alt="Honorable Mention Proceedings Track Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <h3 id="extended-abstracts-track-1">Extended Abstracts Track</h3> <p><strong>Authors</strong>: <a href="https://chenyuwang-monica.github.io">Chenyu Wang</a>, <a href="https://www.mit.edu/~sharut/">Sharut Gupta</a>, <a href="https://scholar.google.com/citations?user=2gU9PYQAAAAJ">Xinyi Zhang</a>, <a href="https://www.cs.toronto.edu/~stonekaboni/">Sana Tonekaboni</a>, <a href="https://scholar.google.ch/citations?user=gTWUZlsAAAAJ">Stefanie Jegelka</a>, <a href="https://people.csail.mit.edu/tommi/">Tommi Jaakkola</a>, and <a href="https://www.carolineuhler.com/caroline-uhler">Caroline Uhler</a></p> <p><strong>Title</strong>: <em>“An Information Criterion for Controlled Disentanglement of Multimodal Data.”</em> <d-cite key="wang2024an"></d-cite></p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_honorable_abstract_2024.jpg" alt="Honorable Mention Extended Abstracts Track Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <hr/> <h2 id="best-reviewer-award">Best Reviewer Award</h2> <p>A special <strong>Best Reviewer Award</strong> was given to <a href="https://akamboj2.github.io">Abhi Kamboj</a> for their exceptional feedback.</p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_best_reviewer_2024.jpg" alt="Best Reviewer Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <hr/> <h2 id="acknowledgments">Acknowledgments</h2> <p>🔴 Congratulations to all the award recipients for their outstanding work, and a heartfelt thank you to everyone who participated in the workshop. The UniReps community keeps growing! We look forward to seeing you again next year! 🔵</p>]]></content><author><name>UniReps Organizing Team</name></author><category term="NeurIPS,"/><category term="awards,"/><category term="2024,"/><category term="UniReps"/><summary type="html"><![CDATA[A gallery of the UniReps 2024 Awards winners and photos]]></summary></entry><entry><title type="html">Submission Guidelines</title><link href="https://unireps.org//blog/2024/guidelines/" rel="alternate" type="text/html" title="Submission Guidelines"/><published>2024-10-09T00:00:00+00:00</published><updated>2024-10-09T00:00:00+00:00</updated><id>https://unireps.org//blog/2024/guidelines</id><content type="html" xml:base="https://unireps.org//blog/2024/guidelines/"><![CDATA[<p>Welcome to the <strong>UniReps Blogpost Submission</strong> guide! This page outlines the steps to submit your blogpost to the UniReps Workshop, where you can share your insights, spark discussions, and connect with the community. We’re excited you’re interested in contributing!</p> <p>To manage the blogpost track, we rely on <a href="https://github.com/alshedivat/al-folio">al-folio</a>, a <a href="https://jekyllrb.com">Jekyll</a> theme for academic websites. If this is all new to you, don’t worry. It’s simpler than it sounds and <strong>we are available for any questions you might have</strong>.</p> <h2 id="getting-started">Getting Started</h2> <h3 id="fork-the-repository">Fork the Repository</h3> <p>Start by <strong>forking the repository</strong>. You can do that <a href="https://github.com/UniReps/UniReps">here</a>. Forking creates your own copy of the project where you’ll be able to freely work on your post before submitting it via a pull request (PR).</p> <h3 id="clone-your-fork-locally">Clone Your Fork Locally</h3> <p>Next, clone your new repository to your local machine so you can start editing:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:UniReps/UniReps.git
<span class="nb">cd </span>UniReps
</code></pre></div></div> <h2 id="create-a-new-branch">Create a new branch</h2> <p>Create a new branch specific for each post submission. We recommend this format for the branch name:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git checkout <span class="nt">-b</span> post/&lt;your-post-title&gt;
</code></pre></div></div> <p>You could even use the GitHub web interface to directly write the blogpost, but we recommend cloning the repository for a smoother experience.</p> <h3 id="preview-the-website-locally">Preview the Website Locally</h3> <p>We’ve got a few options depending on your setup. Choose what works best for you:</p> <ul> <li> <p><strong>Option 1: Jekyll</strong><br/> If you have <a href="https://jekyllrb.com/">Jekyll</a> installed, run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle <span class="nb">install
</span>bundle <span class="nb">exec </span>jekyll serve
</code></pre></div> </div> </li> <li> <p><strong>Option 2: Docker</strong><br/> If Docker is your thing, simply run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-compose up
</code></pre></div> </div> </li> <li> <p><strong>Option 3: VS Code</strong><br/> If you are a <strong>VS Code</strong> user, we suggest using the <a href="https://code.visualstudio.com/docs/devcontainers/containers">Dev Container</a> feature, and it will take care of the environment setup for you.</p> </li> </ul> <p>All these options will start a local server, and you’ll be able to preview the website at <a href="http://127.0.0.1:8000/">http://127.0.0.1:8000/</a>.</p> <p>If you are unsure about which option to choose, we recommend starting with <strong>Option 3</strong> as it’s the easiest to set up (assuming you are already using VS Code). However, if you run into any issues, don’t hesitate to reach out to us for help!</p> <hr/> <h2 id="writing-your-post">Writing Your Post</h2> <p>Now to the exciting part—<strong>writing your blogpost</strong>!</p> <h3 id="topics-and-styles-">Topics and Styles 📝</h3> <p>We welcome blogposts of different nature:</p> <ul> <li>New or early-stage research results 🧑‍🔬</li> <li>Tutorial-style summaries of key methods and literature 🔍</li> <li>Opinion pieces on relevant topics in UniReps 🧠</li> <li>And more!</li> </ul> <p>Remember, the goal is to share your insights, spark discussions, and connect with the community. Keep it engaging and accessible to the <a href="https://unireps.org/2025/">broad workshop audience</a>!</p> <h3 id="create-a-new-post">Create a New Post</h3> <p>Head over to the <code class="language-plaintext highlighter-rouge">_posts/</code> directory and create a new file following this format:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>_posts/YYYY-MM-DD-title.md
</code></pre></div></div> <p>A great starting point is the template <code class="language-plaintext highlighter-rouge">/_posts/2024-10-09-template.md</code>, the original one from <a href="https://distill.pub">Distill</a>. Create a <strong>copy</strong> of it (it’s important, don’t directly edit it), rename it, and edit it for your post. It’s already structured for you!</p> <h3 id="add-media-optional">Add Media (Optional)</h3> <p>If your post includes media (we recommend to include them to exploit the blogpost format at its best), here’s how to add them:</p> <ul> <li><strong>Images</strong>: Place images in the folder <code class="language-plaintext highlighter-rouge">assets/img/YYYY-MM-DD-title/</code>.</li> <li><strong>Plotly Interactive Figures</strong>: Drop them into <code class="language-plaintext highlighter-rouge">assets/plotly/YYYY-MM-DD-title/</code>.</li> <li><strong>Citations</strong>: Save them as a BibTeX file (<code class="language-plaintext highlighter-rouge">.bib</code>) in <code class="language-plaintext highlighter-rouge">assets/bibliography/YYYY-MM-DD-title.bib</code>.</li> <li><strong>Other file types</strong>: follow the structure of the <code class="language-plaintext highlighter-rouge">assets/</code> folder, placing them in the appropriate subfolder.</li> </ul> <p>To display them in your post, use the following syntax:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>![Your Media](/blog/assets/complete/media/path/including.extension)
</code></pre></div></div> <p>For example, if you have an image <code class="language-plaintext highlighter-rouge">unireps_banner.jpeg</code> in the folder <code class="language-plaintext highlighter-rouge">assets/img/2024-10-09-guidelines/</code>, you would include it in your post like this:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>![My Image](/blog/assets/img/2024-10-09-guidelines/unireps_banner.jpeg)
</code></pre></div></div> <p>Resulting in: <img src="/blog/assets/img/2024-10-09-guidelines/unireps_banner.jpeg" alt="My Image"/></p> <p>For more examples on how to include media, check the <a href="https://unireps.org/blog/2024-10-09-template">Post Template</a> or the <a href="https://distill.pub/guide/">Distill Guide</a>.</p> <hr/> <h2 id="submitting-your-blogpost">Submitting Your Blogpost</h2> <p>Ready to submit? Follow these steps:</p> <h3 id="push-your-changes">Push Your Changes</h3> <p>Push the changes from your local machine to your forked repository:</p> <p>As an example:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git add <span class="nb">.</span>
git commit <span class="nt">-m</span> <span class="s2">"Blogpost submission: &lt;title&gt;"</span>
git push origin master
</code></pre></div></div> <h3 id="open-a-pull-request">Open a Pull Request</h3> <p>Now, head to your forked repository on GitHub and open a <strong>New Pull Request</strong>. Ensure the title reflects the topic of your blogpost, and double-check the description is clear and concise.</p> <hr/> <h2 id="review-and-publication">Review and Publication</h2> <p>Here’s what happens next:</p> <h3 id="automatic-checks">Automatic Checks</h3> <p>The repository is set up to run automatic checks on your submission so be sure to:</p> <ol> <li>Only create/edit new files, never edit existing ones from the original repository.</li> <li>Create one PR per blogpost submission (i.e., only one new .md file under the <code class="language-plaintext highlighter-rouge">_posts</code> directory).</li> <li>Place all the files in the appropriate directories with the submission pattern mentioned above.</li> </ol> <p>If everything looks good, the PR will be marked as “Ready for Review” and you will also get a preview link to see how your blogpost will look like on the website.</p> <h3 id="review-process">Review Process</h3> <p>Your blogpost will be reviewed based on the live content (ignoring the commit history or previous drafts). The review process will focus on the following aspects:</p> <ul> <li><strong>Content</strong>: Is the content relevant for the community, insightful, and engaging?</li> <li><strong>Structure</strong>: Is the blogpost well-structured and easy to follow?</li> <li><strong>Media</strong>: Are the images, plots, and other media elements well-integrated?</li> <li><strong>Style</strong>: Is the writing clear, concise, and correct?</li> </ul> <h3 id="post-acceptance">Post-Acceptance</h3> <p>Once your blogpost is accepted, it will be merged into the main repository and published on the website. You will be notified via email and your blogpost will be shared on the workshop’s social media channels.</p> <hr/> <h2 id="questions">Questions?</h2> <p>If you need any help or run into any issues, don’t hesitate to reach out to us:</p> <ul> <li>Open an issue on the <a href="https://github.com/UniReps/UniReps">GitHub repository</a></li> <li>Email: unireps-workshop [at] university [dot] org</li> </ul> <p>🔵 We’re here to help! 🔴</p>]]></content><author><name>Valentino Maiorca</name></author><category term="guide,"/><category term="tutorial"/><summary type="html"><![CDATA[How to submit a blogpost to the UniReps Workshop]]></summary></entry><entry><title type="html">Blog Post Template</title><link href="https://unireps.org//blog/2024/template/" rel="alternate" type="text/html" title="Blog Post Template"/><published>2024-10-09T00:00:00+00:00</published><updated>2024-10-09T00:00:00+00:00</updated><id>https://unireps.org//blog/2024/template</id><content type="html" xml:base="https://unireps.org//blog/2024/template/"><![CDATA[<h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <hr/> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags. An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>. For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p> <d-code block="" language="javascript"> var x = 25; function(x) { return x * x; } </d-code> <p><strong>❗️ Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look good in the dark mode. ❗️</p> <p>You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p> <figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
<span class="k">return</span> <span class="nx">x</span> <span class="err">\</span><span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="interactive-plots">Interactive Plots</h2> <p>You can add interative plots using plotly + iframes :framed_picture:</p> <div class="l-page"> <iframe src="/blog/assets/plotly/demo.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <p>The plot must be generated separately and saved into an HTML file. To generate the plot that you see above, you can use the following code snippet:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span>
<span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span>
<span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
<span class="n">df</span><span class="p">,</span>
<span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span>
<span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">assets/plotly/demo.html</span><span class="sh">'</span><span class="p">)</span></code></pre></figure> <hr/> <h2 id="details-boxes">Details boxes</h2> <p>Details boxes are collapsible boxes which hide additional information from the user. They can be added with the <code class="language-plaintext highlighter-rouge">details</code> liquid tag:</p> <details><summary>Click here to know more</summary> <p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p> </details> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p> <ul> <li> <p>Unordered list can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[An example of a Distill-style blog post and main elements]]></summary></entry><entry><title type="html">Learning Embedding Spaces with Metrics via Contrastive Learning |</title><link href="https://unireps.org//blog/2024/learning-embedding-spaces-with-metrics-via-contrastive-learning/" rel="alternate" type="text/html" title="Learning Embedding Spaces with Metrics via Contrastive Learning |"/><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://unireps.org//blog/2024/learning-embedding-spaces-with-metrics-via-contrastive-learning-</id><content type="html" xml:base="https://unireps.org//blog/2024/learning-embedding-spaces-with-metrics-via-contrastive-learning/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[GRaM Workshop]]></summary></entry></feed>