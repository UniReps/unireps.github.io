<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://unireps.org//blog/feed.xml" rel="self" type="application/atom+xml"/><link href="https://unireps.org//blog/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-20T10:14:36+00:00</updated><id>https://unireps.org//blog/feed.xml</id><title type="html">blank</title><entry><title type="html">Boomerang Distillation Enables Zero-Shot Model Size Interpolation</title><link href="https://unireps.org//blog/2025/boomerang-distillation/" rel="alternate" type="text/html" title="Boomerang Distillation Enables Zero-Shot Model Size Interpolation"/><published>2025-10-31T00:00:00+00:00</published><updated>2025-10-31T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/boomerang-distillation</id><content type="html" xml:base="https://unireps.org//blog/2025/boomerang-distillation/"><![CDATA[<p>Today’s large language models power everything from chatbots on your phone to massive AI systems running in data centers. But not all devices can handle the same model sizes: a model that’s fast on a GPU cluster might be unusable on a laptop. AI model developers try to solve this by releasing “model families” consisting of different-sized versions of the same model. For example, Llama3 models come in sizes ranging from 1 billion to 70 billion parameters. Yet training each size from scratch is expensive, leaving big gaps between available models. What if we could fill those gaps <em>without any extra training</em>? That’s exactly what we explore with <strong>Boomerang Distillation</strong>, a new way to “recombine” parts of large and small models to create many intermediate sizes – all from a single training run.</p> <hr/> <h2 id="current-approaches-for-training-llm-families-are-computationally-expensive">Current Approaches for Training LLM Families are Computationally Expensive</h2> <p>As training each model size from scratch is very computationally intensive, many modern LLM families start with one large pretrained model (the teacher) and distill it into smaller ones (the students). This procedure is called knowledge distillation. Typically, the student models learn with the usual next-token prediction objective, plus extra losses that make them imitate the teacher’s behavior (e.g. KL divergence and cosine distance). Distillation is more compute-friendly than training every model without a teacher, but it still requires training each model independently on up to a trillion tokens. This expensive process limits how many models developers can release, so we typically end up with a small set tuned for common GPU setups. Meanwhile, practitioners need models tailored to <em>their</em> hardware and compute budgets. Unless they train a new model themselves, they’re limited to a few prebuilt options, leaving large gaps in the trade-off between model compute and performance (Figure 1).</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/model_sizes_new.jpeg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/model_sizes_new.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 1: The landscape of pretrained LLM families. There are large gaps in size between available LLMs. Figure credit to <a href="https://qwen.ai/research">Qwen research</a></p> <hr/> <h2 id="boomerang-distillation-creating-multiple-models-for-the-price-of-one">Boomerang distillation: creating multiple models for the price of one</h2> <p>Given the limitations of current approaches, how can we efficiently create models of different sizes? We show that surprisingly, distillation is not just useful for training good student models – with the right setup, we can mix and match parts of the teacher and student models to build intermediate models that smoothly trade off size and performance! We call this phenomenon <em>boomerang distillation</em>: starting with a large teacher, we distill a single smaller student, and then “boomerang” back toward the teacher by selectively swapping in teacher components, creating many models of intermediate size <strong>without any additional training</strong>.</p> <p>Intuitively, boomerang distillation works because we encourage each student layer in the distilled model to approximate the function of some block (contiguous set) of teacher layers. In this setup, each student layer can be thought of as a compact summary of one or more teacher layers. Then, swapping out a layer in the student and replacing it with its corresponding block of teacher layers produces a larger model with a <em>better approximation</em> of what the student is trying to compute. Each swap increases model size and improves similarity to the teacher, producing a new, usable model with no extra training required.</p> <p>Boomerang distillation consists of three key steps: (1) student initialization, (2) knowledge distillation, and (3) student patching (Figure 2). We explain each of these steps in detail below.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/size_interpolation_v4.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/size_interpolation_v4.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 2: Overview of boomerang distillation. ➀ The student model is initialized by copying layers from the pretrained teacher model. ➁ The teacher model is distilled into the student model with cross-entropy loss, knowledge distillation loss, and cosine distance loss. ➂ After training the student model, a block of teacher layers corresponding to a student layer is inserted back into the model to get the interpolated intermediate model.</p> <h3 id="step-1-student-initialization">Step 1: Student initialization</h3> <p>We start aligning student layers to teacher blocks (a “block” is one or more contiguous teacher layers) by initializing the student with layers copied from the teacher (Figure 2, left). This creates a clean mapping: every student layer should stand in for a specific teacher block.</p> <p>For example, in Figure 2, we copy layers 1, 3, and 5 from the teacher to create the student. Then,</p> <ul> <li>The first student layer <strong>S1</strong> maps to teacher layers <strong>T1</strong> and <strong>T2</strong></li> <li>The second student layer <strong>S2</strong> maps to teacher layers <strong>T3</strong> and <strong>T4</strong></li> <li>The third student layer <strong>S3</strong> maps to teacher layer <strong>T5</strong></li> </ul> <p>Copying gives us a strong head start, since each student layer behaves like the first layer in its corresponding teacher block.</p> <h3 id="step-2-knowledge-distillation">Step 2: Knowledge distillation</h3> <p>Once we have the student initialized, we train the model using knowledge distillation (Figure 2 center). To improve student performance, we use cross entropy loss for next token prediction and KL divergence loss to make the student’s predicted probabilities match those of the teacher. We also add a cosine distance loss between the outputs of the student layers and their corresponding teacher blocks.</p> <p>Continuing the example from Figure 2 above, we align</p> <ul> <li><strong>S1</strong> with <strong>T2</strong>’s output (for the T1-T2 block)</li> <li><strong>S2</strong> with <strong>T4</strong> (for T3-T4)</li> <li><strong>S3</strong> with <strong>T5</strong> (for T5)</li> </ul> <p>This layer-to-block alignment trains each student layer to mimic the computations in its mapped teacher block.</p> <h3 id="step-3-student-patching">Step 3: Student patching</h3> <p>After distillation, we can assemble models of many sizes by patching student layers, i.e., replacing any student layer with its corresponding block of teacher layers (Figure 2, right). Because we enforced layer-wise alignment in Steps 1 and 2, each swap creates an intermediate model that <em>better approximates</em> the teacher.</p> <p>In our example from Figure 2, we can choose any subset of swaps:</p> <ul> <li>Replace <strong>S1</strong> with <strong>T1-T2</strong></li> <li>Replace <strong>S2</strong> with <strong>T3-T4</strong></li> <li>Replace <strong>S3</strong> with <strong>T5</strong></li> </ul> <p>Choosing different combinations of swaps yields intermediate models of different sizes, giving a smooth menu of compute/accuracy trade-offs <strong>without additional training</strong>.</p> <hr/> <h2 id="boomerang-distillation-interpolates-between-student-and-teacher-performance">Boomerang distillation interpolates between student and teacher performance</h2> <h3 id="what-are-the-necessary-conditions-for-boomerang-distillation-to-succeed">What are the necessary conditions for boomerang distillation to succeed?</h3> <p>To probe when boomerang distillation works, we run two simple stress tests. First, we study whether a student that’s randomly initialized (instead of copying layers from the teacher) could still yield useful intermediate models. Second, we try <em>naive pruning</em>: initialize the student from teacher layers, but skip the distillation step entirely.</p> <p>We find that across multiple model families and sizes, boomerang distillation creates intermediate models whose <strong>performance interpolates smoothly</strong> between the student and teacher (Figures 3 and 4). In contrast, both baselines’ performance sharply decreases with model size. This shows that <strong>both</strong> ingredients – the right initialization <em>and</em> knowledge distillation – matter. Leave out either step and the boomerang effect largely disappears.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_layer_dropping.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_layer_dropping.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 3: Boomerang distillation produces models with smooth size–performance interpolation, consistently outperforming naive layer pruning and interpolation from randomly initialized distilled models.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/all_classification_results.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/all_classification_results.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 4: Boomerang distillation emerges across model families like Qwen, Pythia, and Llama.</p> <p>We also test which parts of the distillation objective are needed for boomerang distillation. To do so, we train four students with ablated loss terms and then interpolate from them. In Figure 5, we find that for most model sizes, leaving out the per-layer cosine distance (the green and purple lines) does not meaningfully reduce interpolation performance. This suggests that the initialization in Step 1 already provides enough alignment for boomerang distillation to work reasonably well without explicitly training every intermediate layer to match the teacher. That said, students distilled with the per-layer cosine term show more stable interpolation than those without. Looking ahead, we are interested in understanding whether we can keep that stability without explicit layer-wise alignment during distillation, because removing the need to keep the teacher in memory would significantly reduce the GPU footprint of training the student.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_loss_type.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_loss_type.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 5: Per-layer loss yields stable and smoother interpolation performance.</p> <h3 id="how-good-is-boomerang-distillation">How good is boomerang distillation?</h3> <h4 id="comparison-to-standard-distilled-models">Comparison to standard distilled models</h4> <p>Now that we have interpolated models from boomerang distillation, we compare them to same-size models trained with a standard distillation objective. For an apples-to-apples test, we use the same initialization and distillation setup as the small student model.</p> <p>We find that boomerang distillation creates intermediate models with performance on par with standard distilled models of the same size, even outperforming them at larger sizes (Figure 6). In practice, this means we only need to distill a <em>single student</em> to get a full lineup of intermediate models that perform similarly to models individually distilled with the same token budget. These interpolated models also stack up well against pretrained models like Pythia-2.8B and Llama-3.2-3B, which train on far more tokens than the student.</p> <p>We also observe that knowledge distillation can hurt models like Qwen at larger sizes (toward the right of Figure 6), likely because they originate from proprietary, high-quality data; retraining on public data (of presumably lower quality) can cause a drop in performance. With boomerang distillation, we mitigate this issue because we interpolate directly between the student and the teacher.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_versus_distilled.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_versus_distilled.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 6: Interpolated models produced using boomerang distillation have comparable performance to pretrained and standard distilled models.</p> <h4 id="comparison-to-layer-pruning">Comparison to layer pruning</h4> <p>How does boomerang distillation compare to smart layer pruning methods? Layer Collapse (LaCo)<d-cite key="yang2024laco"></d-cite> and ShortGPT<d-cite key="men2024shortgpt"></d-cite> are popular approaches that look for redundancy in a transformer and drop entire layers to shrink the model without training.</p> <p>In practice, boomerang distillation exhibits much better performance. As Figure 7 shows, its interpolated models consistently outperform layer-pruned models of the same size, and the gap widens as models get smaller. This is likely because boomerang distillation blends information from <strong>both</strong> the distilled student and the original teacher, so the intermediate models can use information from both models. Pruning, by contrast, compresses only the teacher; once you shrink far below the teacher’s size, quality tends to fall off.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_pruning_method_comparison.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_pruning_method_comparison.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 7: Boomerang distillation performs significantly better than layer pruning methods.</p> <hr/> <h2 id="future-directions">Future directions</h2> <p>We introduce boomerang distillation, a simple recipe for training-free creation of intermediate-sized language models: start with a big teacher model, distill a compact student, then mix and match student and teacher layers to build models that smoothly scale in size and performance. We are excited by the potential of boomerang distillation to shift how developers create model families. Instead of training many models from scratch, they can simply <strong>distill one student and then interpolate</strong>, swapping in teacher blocks as needed to produce a full lineup that covers different accuracy-latency targets. This opens the door to finer-grained LLM customization for real-world constraints. Looking ahead, extending these ideas to pretraining-scale token budgets and into other domains (such as vision) can build model families tailored to a wide range of deployment settings.</p> <hr/> <p>This blog is adapted from our paper <a href="https://arxiv.org/abs/2510.05064">Boomerang Distillation Enables Zero-Shot Model Size Interpolation</a>. This blog post was first published on the <em>Deeper Learning</em> blog at the Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University.</p> <hr/>]]></content><author><name>Sara Kangaslahti</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[Real-world deployments of LLMs require models of different sizes to meet performance, latency, and cost targets. Yet pretraining every size is prohibitively expensive, leaving large gaps in size-performance curves. We identify a novel phenomenon, Boomerang Distillation, which occurs when distilling a large language model into a smaller one. In this blog post, we describe how Boomerang Distillation can be used to create entire families of LLMs of fine-grained sizes from a single student-teacher pair without any additional training.]]></summary></entry><entry><title type="html">Neural Babel: What Do Neural Networks Talk About?</title><link href="https://unireps.org//blog/2025/neural-babel/" rel="alternate" type="text/html" title="Neural Babel: What Do Neural Networks Talk About?"/><published>2025-10-26T00:00:00+00:00</published><updated>2025-10-26T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/neural-babel</id><content type="html" xml:base="https://unireps.org//blog/2025/neural-babel/"><![CDATA[<p>Code for this project can be found at: https://github.com/sike25/neural_syntax</p> <h2 id="1-introduction">1. Introduction</h2> <p>Imagine overhearing a conversation in a language you don’t speak. The speakers understand each other perfectly, but you have no idea what they’re saying. In this project, the speakers were neural networks, and the language emerged spontaneously when they were trained to collaboratively solve a task. We tried to build a translator for this “neuralese” and this is what we found.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/confused-math-lady.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-25-neural-babel/confused-math-lady.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Me trying to understand neuralese</p> <h2 id="2-teaching-machines-to-point-at-things">2. Teaching Machines to Point at Things</h2> <p>Take a world of objects W and a subset of this world X.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/w-and-x-example.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-25-neural-babel/w-and-x-example.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p><strong>Fig. 1.</strong> A world W of objects has a subset X decided by some rule.</p> <p>Before scrolling, how would YOU describe this selection?</p> <details><summary>Look at the rule</summary> <p>RULE: ‘blue’</p> </details> <p>A neural network called the speaker is given \(W\) and \(X\) and outputs a neuralese vector \(V\) that ideally captures this rule. Another network called the listener takes in \(V\) and an element of the world \(W_i \in W\) and predicts whether or not \(W_i\) belongs in \(X\). So the listener never sees \(X\), and it relies entirely on the speaker’s neuralese output to understand \(X\).</p> <p>Andreas and Klein (2017) <d-cite key="andreas2017"></d-cite> shows us that this “language” could be negated via linear transformation to take on the opposite meaning. Now, this project attempts to figure out whether these vectors can be directly translated.</p> <p>For training data, Andreas and Klein use labels from the GENX dataset <d-cite key="genx"></d-cite>. We forewent this dataset and generated our own. Each object had a color, shape, and outline thickness encoded row by row.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/matrix-as-object.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-25-neural-babel/matrix-as-object.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p><strong>Fig. 2.</strong> An object \(W_i\) of a world \(W\), represented as a one-hot encoded matrix.</p> <p>Our dataset had 80,730 unique worlds of 5 objects each. Subsets were created using 72 unique rules: single feature rule <code class="language-plaintext highlighter-rouge">red</code>, single feature negation <code class="language-plaintext highlighter-rouge">not red</code>, two features of different types joined by and/or <code class="language-plaintext highlighter-rouge">red and circle</code>, <code class="language-plaintext highlighter-rouge">triangle or thick-outline</code>. Skipping over world-rule combinations that resulted in empty subsets, we gathered a dataset of 1,705,833 (world \(W\), subset \(X\), rule) entries.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/dataset-entry.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-25-neural-babel/dataset-entry.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p><strong>Fig. 3.</strong> An entry from our generated dataset.</p> <p>Training separate networks to evolve languages in order to play a communication game has also been done in Gupta et al. (2021) <d-cite key="gupta2021"></d-cite>, Lazaridou et al. (2018) <d-cite key="lazaridou2018"></d-cite> and Andreas et al. (2018) <d-cite key="andreas2018"></d-cite>.</p> <h2 id="3-conference">3. Conference</h2> <h3 id="31-training-the-speaker-listener">3.1. Training the Speaker-Listener</h3> <p>The speaker-listener system achieved <strong>99.56%</strong> test accuracy on an unseen test set, with accuracy climbing from 60% to 95% by epoch 1, implying that the task was easily learned.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/speaker-listener.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-25-neural-babel/speaker-listener.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p><strong>Fig. 4.</strong> The Speaker-Listener Architecture.</p> <p>To prevent the speaker from encoding positional shortcuts (“select positions [0,2,3]”) and force it to learn semantic rules (“select purple circles”), the world objects are shuffled before being fed to the listener.</p> <h3 id="32-cross-rule-validation---the-baseline-signal-problem">3.2. Cross-Rule Validation - The Baseline Signal Problem</h3> <p>After training, we needed to verify that the speaker actually learned to encode rules meaningfully. Did “red objects” produce similar neuralese across different worlds? Did <code class="language-plaintext highlighter-rouge">red</code> neuralese differ significantly from <code class="language-plaintext highlighter-rouge">triangle</code> neuralese?</p> <p>Using the trained speaker, we generated 100 neuralese vectors each for 9 different rules (like <code class="language-plaintext highlighter-rouge">red</code>, <code class="language-plaintext highlighter-rouge">green or triangle</code>, <code class="language-plaintext highlighter-rouge">not purple</code>, etc.). Then we measured how similar these vectors were to each other using cosine similarity. We expected that neuralese for the same rule should be similar (high within-rule similarity), while neuralese for different rules should be different (low cross-rule similarity), but the similarities for both categories were high (\(0.908 \pm 0.090\) and \(0.865 \pm 0.097\) respectively).</p> <p>We guessed that the neuralese might contain a massive “baseline signal” that concealed the actual messages. So we normalized the neuralese by computing the average vector across all examples, then subtracting it from each vector. This brought the cosine similarity for same-rule neuralese down to \(0.246 \pm 0.519\) (moderate similarity) and cross-rule similarity to \(-0.069 \pm 0.500\) (negative similarity).</p> <p>This analysis showed that rule information did exist in the neuralese, just hidden beneath the baseline. We therefore inferred that we should normalize the neuralese before attempting to translate them.</p> <h2 id="4-translation">4. Translation</h2> <h3 id="41-training-the-translator">4.1. Training the Translator</h3> <p>Having established that the speaker-listener system could communicate, the central question comes up. Can this emergent neuralese be translated into natural language?</p> <p>If the neuralese vectors have encoded semantic information about the rules, then an appropriate neural network ought to be able to reverse-engineer these rules from the vector alone.</p> <p>The translator network is a 5-layer multilayer perceptron which takes a normalized 12-dimensional neuralese vector as its input and outputs a 3-token sequence which it classifies over our 13-word vocabulary (9 features + <code class="language-plaintext highlighter-rouge">and</code>, <code class="language-plaintext highlighter-rouge">not</code>, <code class="language-plaintext highlighter-rouge">or</code> + <code class="language-plaintext highlighter-rouge">&lt;blank&gt;</code>). The translator was trained on 1,364,666 training examples (40% of the dataset).</p> <p>Evaluation on an unseen test set yielded modest results. The network correctly predicted individual tokens <strong>63.76%</strong> of the time, and got the entire 3-token rule right <strong>38.17%</strong> of the time.</p> <h3 id="42-adjusted-evaluation-metrics">4.2. Adjusted Evaluation Metrics</h3> <p>Next, we considered the possibility that raw accuracy metrics could be painting an incomplete picture. Look at this example:</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/adjusted-accuracy-example.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-25-neural-babel/adjusted-accuracy-example.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p><strong>Fig. 5.</strong> Different rules can describe the same target subset.</p> <p>Notice how different rules can produce the same target subset? We then decided to compute a metric where a predicted rule is accurate if it produces the same subset of the world as the ground truth rule even if it is different from the ground truth rule. We called this the <strong>adjusted accuracy</strong> and calculated it at <strong>43.24%</strong>.</p> <p>Consider another example:</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/description-accuracy-example.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-25-neural-babel/description-accuracy-example.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p><strong>Fig. 6.</strong> Predicted rules can perfectly describe the target subset but include other elements. We can consider this phenomenon a sign of partial learning.</p> <p>Here, the predicted rule correctly describes all the objects in the target subset even though it incorrectly includes the purple triangle. Another adjusted metric tracked whether the rule correctly describes all the selected objects. We called this the <strong>description accuracy</strong> and calculated it at <strong>51.44%</strong>.</p> <p>We also found that only <strong>61% of predicted rules were even semantically valid</strong>. Here are some real examples of malformed rules from our test set:</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-25-neural-babel/malformed-rules-example.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-25-neural-babel/malformed-rules-example.jpg" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p><strong>Fig. 7.</strong> The translator can produce incoherent or malformed rules.</p> <p>This meant that some portion of the incorrectly predicted rules were not rules at all. So we wondered, if we filtered only for semantically valid predictions, what would the evaluation metrics look like? Conditioning on semantic validity raised the adjusted accuracy from <strong>43.24% to 70.81%</strong> and the description accuracy from <strong>51.44% to 84.23%</strong>.</p> <table> <thead> <tr> <th> </th> <th style="text-align: center">All Predictions</th> <th style="text-align: center">Semantically Valid Only</th> </tr> </thead> <tbody> <tr> <td>Raw Sequence Accuracy</td> <td style="text-align: center">38.17%</td> <td style="text-align: center">-</td> </tr> <tr> <td>Adjusted Sequence Accuracy</td> <td style="text-align: center">43.24%</td> <td style="text-align: center">70.81%</td> </tr> <tr> <td>Description Sequence Accuracy</td> <td style="text-align: center">51.44%</td> <td style="text-align: center">84.23%</td> </tr> </tbody> </table> <h2 id="5-conclusions">5. Conclusions</h2> <h3 id="51-what-the-cross-rule-validation-tells-us">5.1. What the Cross-Rule Validation Tells Us</h3> <p>Remember our baseline signal problem? After normalization, same-rule neuralese vectors had cosine similarities around 0.246— moderate, but nowhere near the ~1.0 we’d expect if neuralese vectors were perfect rule encodings. The concept of rules exists in neuralese, but neuralese is not equal to the rules themselves.</p> <p>If neuralese were theoretically equivalent to our selective rules—if <code class="language-plaintext highlighter-rouge">red</code> always mapped to some canonical <code class="language-plaintext highlighter-rouge">red</code> vector—we’d see cosine similarities approaching 1.0 within rule categories. So we know these vectors are contaminated with other (possibly useful) data. Perhaps information about the specific world being described. Perhaps metadata about the selection itself—how many objects are selected, their distribution across feature types, spatial patterns in the original (pre-shuffled) arrangement.</p> <p>So the speaker might learn some rule concepts while also exploiting easier statistical patterns.</p> <blockquote> <p>The concept of rules exists in neuralese, but neuralese is not equal to the rules themselves.</p> </blockquote> <h3 id="52-what-the-adjusted-metrics-tell-us">5.2. What the Adjusted Metrics Tell Us</h3> <p>The translator should not have struggled as much as it did to produce well-formed 3-token sequences from such a limited and consistent grammar. However, the strong performance on semantically valid rules suggests that the translation task itself is feasible. The bottleneck appears to be the token-by-token classification approach: the MLP architecture struggled to construct coherent rules when predicting each token independently, even though it could successfully translate neuralese when it did produce valid sequences.</p>]]></content><author><name>Sike Ogieva</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[Translating internal representations of neural networks into natural language.]]></summary></entry><entry><title type="html">Density Adaptive Attention Mechanism: Robust and Explainable Representations Across Multiple Modalities</title><link href="https://unireps.org//blog/2025/density-adaptive-attention/" rel="alternate" type="text/html" title="Density Adaptive Attention Mechanism: Robust and Explainable Representations Across Multiple Modalities"/><published>2025-10-25T00:00:00+00:00</published><updated>2025-10-25T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/density-adaptive-attention</id><content type="html" xml:base="https://unireps.org//blog/2025/density-adaptive-attention/"><![CDATA[<p style="font-size: 0.85em; font-style: italic;">* Work does not relate to position at Amazon.</p> <h2 id="introduction">Introduction</h2> <p>The Transformer architecture <d-cite key="vaswani2017attention"></d-cite> and its self-attention mechanism have revolutionized sequence modeling across natural language processing, speech processing, and computer vision. However, the ubiquitous scaled dot-product attention exhibits fundamental limitations when processing highly non-stationary data <d-cite key="wang2023rrwkv,zhuang2022long,he2023long"></d-cite>. Its tendency to produce low-entropy attention distributions and fixed-length context windows can lead to suboptimal performance, particularly in domains where feature importance varies dramatically.</p> <p>We introduce the <strong>Multi-Head Density Adaptive Attention Mechanism (DAAM)</strong> and the <strong>Density Adaptive Transformer (DAT)</strong>, a novel probabilistic attention framework that replaces correlation-based dot-product attention with learnable Gaussian modulation. Unlike traditional approaches that hard-code distribution parameters <d-cite key="You2020HardCodedGA,Guo_Zhang_Liu_2019"></d-cite> or learn only multiplicative scaling factors <d-cite key="gct,9053591"></d-cite>, or use pre-defined amplitudes <d-cite key="Luo_2023_ICCV"></d-cite>, DAAM features <strong>fully learnable mean and variance parameters</strong> within a multi-headed architecture, enabling it to collectively model any probability distribution and dynamically recalibrate feature significance based on input characteristics.</p> <p><strong>Key insight:</strong> By learning both additive (mean offset) and multiplicative (variance scaling) parameters across multiple attention heads, DAAM can approximate arbitrary probability distributions through mixtures of Gaussians. This capability proves particularly valuable for non-stationary data, where DAAM achieves performance improvements of up to approximately <strong>+20% absolute accuracy</strong> over traditional self-attention.</p> <h3 id="contributions">Contributions</h3> <p>This work makes four primary contributions:</p> <ol> <li> <p><strong>Novel attention mechanism:</strong> DAAM with fully learnable Gaussian parameters in a multi-headed, parameter-efficient framework (0.002-0.082M parameters)</p> </li> <li> <p><strong>Importance Factor metric:</strong> A new quantitative measure for model explainability that enhances interpretability in models using DAAM</p> </li> <li> <p><strong>Cross-modal validation:</strong> Comprehensive evaluation across Speech (WavLM <d-cite key="9814838"></d-cite>), Text (Llama2 <d-cite key="touvron2023llama"></d-cite>), and Vision (BEiT <d-cite key="bao2022beit"></d-cite>) modalities, demonstrating DAAM’s superiority over conventional dot-product attention in handling non-stationary data.</p> </li> <li> <p><strong>Practical integration:</strong> Compatibility with Grouped Query Attention <d-cite key="gqa"></d-cite>, enabling enhancement of existing pre-trained models with minimal parameter overhead (+2.66%)</p> </li> </ol> <hr/> <h2 id="the-attention-problem">The Attention Problem</h2> <h3 id="limitations-of-scaled-dot-product-attention">Limitations of Scaled Dot-Product Attention</h3> <p>The standard self-attention mechanism in Transformers computes attention weights through normalized dot-products:</p> \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>While this formulation has proven successful, the paper identifies several fundamental constraints:</p> <p><strong>1. Low-entropy attention distributions</strong></p> <p>The softmax operation inherently biases toward peaked distributions. For a vector of logits $z = {z_1, \ldots, z_n}$ obtained from scaled query-key products, the attention weights are:</p> \[a_i = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}\] <p>The entropy of this distribution is:</p> \[H(\text{softmax}(z)) = -\sum_{i=1}^n \left(\frac{e^{z_i}}{S}\right) \log \frac{e^{z_i}}{S}\] <p>where $S = \sum_j e^{z_j}$. As the magnitude of the largest $z_i$ increases, the distribution approaches one-hot encoding. In practice, the softmax output often heavily favors larger dot product values, resulting in concentrated attention on specific parts of the input. This leads to lower entropy, indicative of less uniform attention distribution.</p> <p><strong>2. Limited adaptability for non-stationary data</strong></p> <p>Self-attention’s fixed-length context window can lead to sub-optimal performance <d-cite key="li2023unlocking"></d-cite>, especially for long sequences where distant elements may not be relevant. Without inductive biases like locality <d-cite key="edelman2021inductive"></d-cite>, self-attention layers might require more data to learn patterns. The mechanism can struggle with capturing long-term dependencies in practice <d-cite key="10.1162/tacl_a_00306"></d-cite>, particularly as sequence length increases.</p> <p><strong>3. Interpretability challenges</strong></p> <p>The interpretability of self-attention mechanisms is challenging <d-cite key="xai"></d-cite> since we can only derive correlation-based activations, primarily focusing on pairwise similarities. This makes it difficult to understand why certain parts of the input are prioritized.</p> <h3 id="entropy-analysis-daam-vs-self-attention">Entropy Analysis: DAAM vs. Self-Attention</h3> <h4 id="complete-mathematical-framework-for-daam">Complete Mathematical Framework for DAAM</h4> <p><strong>Gaussian Transformation per Head:</strong></p> <p>Each head $h$ in DAAM processes input using Gaussian normalization controlled by learnable parameters $\mu_{i,h}$ and $\sigma_{i,h}$. The transformation is defined by:</p> \[y_{\text{norm}} = \frac{y - (\text{mean} + \text{mean_offset})}{\sqrt{\text{var} + \epsilon}}\] <p>where $\epsilon$ is a small constant ensuring numerical stability. This normalized input is then applied to a Gaussian function:</p> \[f^{(h)}(x) = \exp\left(-\frac{y_{\text{norm}}^2}{2c^2}\right)\] <p>with $c$ as a learnable parameter controlling the spread of the Gaussian function.</p> <p><strong>Product of Gaussians - Effective Distribution:</strong></p> <p>The overall transformation for each head approximates a Gaussian distribution with effective variance:</p> \[\sigma_{\text{prod}}^2 = \left( \sum_{i=1}^N \frac{1}{\sigma_{i,h}^2} \right)^{-1}\] <p>and effective mean:</p> \[\mu_{\text{prod}} = \sigma_{\text{prod}}^2 \left(\sum_{i=1}^N \frac{\mu_{i,h}}{\sigma_{i,h}^2}\right)\] <p><strong>Entropy Calculation:</strong></p> <p>The entropy for each head is calculated using:</p> \[H(X_h) = \frac{1}{2} \log(2\pi e \sigma_{\text{prod}}^2)\] <p>This reflects how data is spread, influenced by parameters such as $c$, the mean offset, and computed variance. The overall system entropy, including interactions among multiple heads, is:</p> \[H(\text{DAAM}) = \sum_{h=1}^H H(X_h) + \Delta\] <p>where $\Delta$ accounts for additional entropy arising from diversity and interactions across different heads, highlighting the ensemble effect of multi-head Gaussian transformations.</p> <p><strong>Traditional Self-Attention:</strong></p> <p>Traditional self-attention mechanisms are represented as:</p> \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>For a vector $z = {z_1, z_2, \ldots, z_n}$ derived from scaled dot products, let $S = \sum_{j=1}^n e^{z_j}$. The softmax values are $\left{\frac{e^{z_1}}{S}, \frac{e^{z_2}}{S}, \ldots, \frac{e^{z_n}}{S} \right}$, with entropy:</p> \[H(\text{softmax}(z)) = -\sum_{i=1}^n \left( \frac{e^{z_i}}{S} \log \frac{e^{z_i}}{S} \right)\] <p>This entropy is typically low unless the $z$ values are nearly identical. The exponential nature emphasizes larger dot product values, concentrating attention and leading to lower entropy.</p> <p><strong>Key Insight:</strong> Without modifications to the architecture—such as constraining weight matrices $W^Q$ and $W^K$ to produce similar outputs across different inputs—traditional self-attention mechanisms inherently produce lower entropy. This makes them less adaptable in scenarios demanding sensitivity to diverse and dynamic data elements.</p> <p><strong>DAAM’s Adaptive Advantage:</strong></p> <p>DAAM dynamically adjusts its entropy in response to input characteristics, providing both broad (high entropy) and focused (low entropy) attention distributions as needed. This is essential for effectively handling both highly non-stationary and stationary data environments.</p> <hr/> <h2 id="density-adaptive-attention-mechanism">Density Adaptive Attention Mechanism</h2> <p>DAAM replaces correlation-based attention with probabilistic feature modulation using learnable Gaussian distributions. The mechanism operates independently across multiple heads, with each head capturing distinct statistical patterns in different feature subspaces.</p> <h3 id="core-algorithm">Core Algorithm</h3> <p>For input features $\mathbf{x}$, DAAM performs the following transformation:</p> <div class="l-body"> <div class="fake-img"> <p style="font-family: monospace; font-size: 0.9em;"> <strong>Algorithm 1: Density Adaptive Attention Mechanism</strong><br/> <strong>Input:</strong> x (input tensor), normDimSize, normAxis, c, eps<br/> <strong>Output:</strong> Attention-modified tensor<br/> <br/> 1. Initialize learnable parameters:<br/> &nbsp;&nbsp;&nbsp;c ← (1, normDimSize) tensor with value c<br/> &nbsp;&nbsp;&nbsp;meanOffset ← (1, normDimSize) zeros<br/> <br/> 2. For each batch in x:<br/> &nbsp;&nbsp;&nbsp;a. Compute statistics along normAxis:<br/> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mean ← mean(x, dim=normAxis)<br/> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;var ← mean(x², dim=normAxis) - mean²<br/> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;var ← |var| + 1e-8 (ensure positivity)<br/> <br/> &nbsp;&nbsp;&nbsp;b. Normalize with learnable offset:<br/> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;adjustedMean ← mean + meanOffset<br/> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yNorm ← (x - adjustedMean) / √(var + 1e-5)<br/> <br/> &nbsp;&nbsp;&nbsp;c. Apply Gaussian transformation:<br/> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yTransform ← exp(-(yNorm² / (2·c)))<br/> <br/> &nbsp;&nbsp;&nbsp;d. Modulate features:<br/> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x ← x ⊙ yTransform<br/> <br/> 3. Return x </p> </div> </div> <h3 id="multi-head-architecture">Multi-Head Architecture</h3> <p>In practice, DAAM operates in a multi-head configuration where each head processes distinct, non-overlapping subspaces:</p> \[\text{MultiHeadDAAM}(\mathbf{x}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_H)\] <p>where each head applies the core algorithm independently. This multi-headed formulation allows each head to capture different aspects of the data distribution, making it possible to collectively mimic non-Gaussian traits.</p> <h3 id="learnable-parameters">Learnable Parameters</h3> <p>DAAM introduces two classes of learnable parameters per head:</p> <p><strong>Mean Offset ($\delta$):</strong> Additive shift to distribution center</p> <ul> <li>Allows for a dynamic shift in the Gaussian distribution’s mean</li> <li>Recalibrates the central focus of attention based on input context</li> <li>Enhances the model’s sensitivity to deviations</li> </ul> <p><strong>Scaled Variance ($\xi$):</strong> Multiplicative spread of Gaussian</p> <ul> <li>Adjusts the Gaussian curve’s spread</li> <li>Enables the model to adaptively concentrate on features with varying degrees of dispersion</li> <li>Ensures that the attention mechanism is appropriately scaled</li> </ul> <p>For $H$ heads and $d$ feature dimensions: \(P_{\text{DAAM}} = 2 \times H \times d\)</p> <p>In the experiments: $H=8$, $d \in {1024, 5120}$, yielding 0.016-0.082M parameters.</p> <h3 id="dynamic-contextual-adaptation">Dynamic Contextual Adaptation</h3> <p>DAAM’s dual learning strategy, encompassing both additive (mean offset) and multiplicative (variance-based scaling factor) Gaussian learnable parameters, offers significant advantages <d-cite key="Fluri2022"></d-cite>. The mean offset allows for a dynamic shift in the Gaussian distribution’s mean, recalibrating the central focus of attention based on input context. This shift enhances the model’s sensitivity to deviations and makes it a more contextually relevant center. Concurrently, the variance scaling factor adjusts the Gaussian curve’s spread, enabling the model to adaptively concentrate on features with varying degrees of dispersion. This multiplicative adjustment ensures that the attention mechanism is not just centered correctly but also appropriately scaled, optimizing the model’s performance for specific tasks.</p> <hr/> <h2 id="theoretical-foundations">Theoretical Foundations</h2> <h3 id="universal-approximation-capacity">Universal Approximation Capacity</h3> <p>The paper demonstrates that DAAM can approximate any continuous probability density function through Gaussian mixtures. Each head processes input using Gaussian normalization, where the input is transformed by the formula $y_{\text{norm}} = \frac{y - (\text{mean} + \text{mean_offset})}{\sqrt{\text{var} + \epsilon}}$. The Gaussian function applied is $f^{(h)}(x) = \exp\left(-\frac{y_{\text{norm}}^2}{2c^2}\right)$, with $c$ representing the spread of the Gaussian.</p> <p>The transformation in each head can be viewed as modifying the data under a Gaussian model whose effective variance $\sigma_{\text{eff}}^2$ and mean $\mu_{\text{eff}}$ are influenced by the learnable parameters:</p> <p>For $N$ Gaussian components per head $h$, the overall transformation approximates a Gaussian distribution whose variance is: \(\sigma_{\text{prod}}^2 = \left( \sum_{i=1}^N \frac{1}{\sigma_{i,h}^2} \right)^{-1}\)</p> <p>and the effective mean: \(\mu_{\text{prod}} = \sigma_{\text{prod}}^2 \left(\sum_{i=1}^N \frac{\mu_{i,h}}{\sigma_{i,h}^2}\right)\)</p> <p>The entropy for each head is: \(H(X_h) = \frac{1}{2} \log(2\pi e \sigma_{\text{eff}}^2)\)</p> <p>The overall system entropy, considering potential interactions among heads, is: \(H(\text{X}) = \sum_{h=1}^H H(X_h) + \Delta\)</p> <p>where $\Delta$ symbolizes additional entropy due to diversity and interaction across different heads.</p> <h3 id="complexity-analysis">Complexity Analysis</h3> <p><strong>Parameter complexity:</strong></p> <p>For $d$-dimensional input features, DAAM introduces the following parameters per head:</p> <ul> <li>Mean offsets: $d$ parameters</li> <li>Variance scaling factors: $d$ parameters</li> </ul> <p>For $H$ heads, the total parameter count is: \(P_{\text{DAAM}} = 2 \times H \times d\)</p> <p>In contrast, traditional self-attention (with projection matrices $W^Q$, $W^K$, $W^V$, and $W^O$) has: \(P_{\text{SA}} = 4 \times d \times d\)</p> <p><strong>Computational complexity:</strong></p> <p>The computational complexity of DAAM includes:</p> <ul> <li>Computing means and variances: $O(nd)$</li> <li>Gaussian calculation for each feature: $O(nd)$</li> <li>Normalization and weighting: $O(nd)$</li> </ul> <p>Total complexity: $O(n \cdot d)$ where $n$ is the batch size and $d$ is the dimension size.</p> <p>For multi-head: $O(h \cdot n \cdot d)$ with $h$ as numHeads, allowing for parallelization.</p> <hr/> <h2 id="architecture-and-integration">Architecture and Integration</h2> <h3 id="model-architecture">Model Architecture</h3> <div class="l-body"> <figure> <img src="/blog/assets/img/2025-10-25-density-adaptive-attention/daam/model_architecture.png" alt="Model architecture"/> <figcaption><strong>Figure 1:</strong> Proposed model architecture showcasing a pre-trained model (encoder) for feature extraction via its N transformer layers, followed by the attention module within the decoder network for selective emphasis, and concluding with probability output. The process flow is marked with trainable and frozen states.</figcaption> </figure> </div> <p>The complete architecture consists of three components:</p> <p><strong>1. Frozen Pre-Trained Encoder</strong></p> <p>The study leverages state-of-the-art pre-trained models as feature extractors:</p> <ul> <li><strong>Speech:</strong> WavLM-Large (24 layers, 1024-dim)</li> <li><strong>Text:</strong> Llama2-13B (40 layers, 5120-dim)</li> <li><strong>Vision:</strong> BEiT-Large (24 layers, 1024-dim)</li> </ul> <p>These encoders remain frozen during training, with the role of PTMs being crucial during the inference phase (post-training). The PTMs are utilized in their original pre-trained state, eschewing any further re-training during the preprocessing stage.</p> <p><strong>2. Attention Module (DAAM)</strong></p> <p>The output from each transformer layer in the encoder undergoes mean pooling across the time dimension (sequence length), followed by concatenation of these pooled outputs. These concatenated outputs serve as input embeddings for the Attention Module.</p> <p>The embeddings are represented as $X \in \mathbb{R}^{N \times d}$, where each $x_i$ is a vector in a $d$-dimensional space, with $d \in {1024, 5120}$. Here, $N$ signifies the total count of transformer layers in the encoder. The attention mechanism then produces a new, contextualized representation $C \in \mathbb{R}^{N \times d}$ for the input sequence.</p> <p><strong>3. Task-Specific Output Layers</strong></p> <p>Convolutional layers are utilized to distill features from the context matrix generated by the attention mechanism. By employing 2-dimensional convolution layers (with kernel_size=(3,3), stride=1, and padding=1), the model processes the array of context tensor outputs from each transformer layer.</p> <h3 id="grouped-query-density-adaptive-attention-gqdaam">Grouped Query Density Adaptive Attention (GQDAAM)</h3> <p>Following the integration of Multi-Head DAAM, the paper investigates its compatibility with dot-product-based attention mechanisms. The focus on Grouped Query Attention (GQA) is driven by its comparable performance to MHA and superior computational efficiency <d-cite key="gqa"></d-cite> and advantages of its hierarchical learning structure <d-cite key="hierarchy"></d-cite>. This approach is termed as Grouped Query Density Adaptive Attention Mechanism (GQDAAM).</p> <p>The objective is to showcase that DAAM can benefit PTMs across multiple modalities as a parameter-efficient fine-tuning method.</p> <p><strong>Parameter comparison:</strong></p> <div class="l-body"> <table> <thead> <tr> <th>Mechanism</th> <th>Heads</th> <th>Parameters (Millions)</th> </tr> </thead> <tbody> <tr> <td><strong>GQDAAM</strong></td> <td>g: 8, q: 8, kv: 2</td> <td>1.00 - 3.16</td> </tr> <tr> <td>GQA</td> <td>q: 8, kv: 2</td> <td>0.984 - 3.08</td> </tr> <tr> <td>LoRA (r=1, α=16)</td> <td>N/A</td> <td>0.43</td> </tr> <tr> <td>DAAMv1</td> <td>g: 8</td> <td><strong>0.016 - 0.082</strong></td> </tr> <tr> <td>DAAMv2 (Mixture)</td> <td>g: 1</td> <td><strong>0.002 - 0.010</strong></td> </tr> </tbody> </table> </div> <h3 id="training-methodology">Training Methodology</h3> <p><strong>Hyperparameters:</strong></p> <ul> <li>Optimizer: Adam <d-cite key="kingma2017adam"></d-cite></li> <li>Learning rate: 1e-4 (5e-5 for Llama2)</li> <li>Weight decay: 0.1</li> <li>Batch size: 8 (speech), 32 (text/vision)</li> <li>Epochs: 35</li> <li>Loss: Focal Loss <d-cite key="lin2018focal"></d-cite> (γ=2.5) for speech, Cross-Entropy for text/vision</li> <li>Precision: Mixed (fp16)</li> </ul> <p><strong>Initialization:</strong></p> <ul> <li>DAAM parameters: mean offset $\delta = 0$, scaled variance $\xi = 2$</li> <li>Other layers: Xavier initialization <d-cite key="xavierinit"></d-cite></li> </ul> <p><strong>Data preprocessing:</strong></p> <ul> <li><strong>Speech:</strong> 16kHz audio, maximum 5-second clips during training and evaluation</li> <li><strong>Text:</strong> Tokenized with maximum context length of 4096</li> <li><strong>Vision:</strong> Images resized to 224×224</li> </ul> <hr/> <h2 id="experimental-validation">Experimental Validation</h2> <p>We conduct comprehensive experiments across three modalities, comparing DAAM against state-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods including LoRA, LoRA+, and standard Multi-Head Attention with and without Batch Normalization.</p> <h3 id="parameter-efficiency-analysis">Parameter Efficiency Analysis</h3> <p>First, we establish the parameter counts for all methods under comparison:</p> <div class="l-body"> <table> <thead> <tr> <th>Mechanism</th> <th>Configuration</th> <th>Parameters (Millions)</th> <th>DAAM Overhead</th> </tr> </thead> <tbody> <tr> <td>GQA (baseline)</td> <td>q: 8, kv: 2</td> <td>1.19 - 3.47</td> <td>—</td> </tr> <tr style="background-color: #f0f8ff;"> <td><strong>GQDAAM</strong></td> <td>g: 8, q: 8, kv: 2</td> <td><strong>1.21 - 3.55</strong></td> <td><strong>+0.016 - 0.082M (0.016%-0.08%)</strong></td> </tr> <tr> <td>LoRA</td> <td>r={4,8}, α=16</td> <td>0.39 - 3.28</td> <td>—</td> </tr> <tr> <td>LoRA+</td> <td>r={4,8}, α=16</td> <td>0.39 - 3.28</td> <td>—</td> </tr> <tr style="background-color: #f0f8ff;"> <td><strong>DAAMv1</strong></td> <td>g: 8 (with 2 conv layers)</td> <td><strong>0.22 - 0.45</strong></td> <td><strong>0.016 - 0.082M DAAM params</strong></td> </tr> <tr style="background-color: #f0f8ff;"> <td><strong>DAAMv2</strong></td> <td>g: 1 (with 2 conv layers)</td> <td><strong>0.22 - 0.45</strong></td> <td><strong>0.002 - 0.010M DAAM params</strong></td> </tr> </tbody> </table> <figcaption><strong>Table 1:</strong> Parameter comparison across PEFT methods. GQDAAM adds only 0.016%-0.08% parameters compared to baseline GQA (80% fewer parameters than LoRA), while DAAMv1/v2 standalone decoders remain highly parameter-efficient.</figcaption> </div> <p><strong>Key Insight:</strong> DAAM achieves superior performance with minimal parameter overhead, making it ideal for resource-constrained deployment.</p> <hr/> <h3 id="speech-emotion-recognition-iemocap">Speech Emotion Recognition: IEMOCAP</h3> <p>Using WavLM-Large as the frozen encoder, we evaluate DAAM on the IEMOCAP dataset for 4-class emotion recognition (neutral, happiness, anger, sadness) with 5-fold cross-validation.</p> <p><strong>Complete 5-fold results with all baselines:</strong></p> <div class="l-body"> <table> <thead> <tr> <th>Method</th> <th>F1</th> <th>F2</th> <th>F3</th> <th>F4</th> <th>F5</th> <th>Mean ± Std</th> </tr> </thead> <tbody> <tr> <td>LoRA+ (r=4)</td> <td>27.6</td> <td>25.7</td> <td>31.7</td> <td>25.1</td> <td>16.8</td> <td>25.4 ± 4.87</td> </tr> <tr> <td>LoRA+ (r=8)</td> <td>27.6</td> <td>28.3</td> <td>20.5</td> <td>20.6</td> <td>24.6</td> <td>24.3 ± 3.32</td> </tr> <tr> <td>LoRA (r=4)</td> <td>49.9</td> <td>51.5</td> <td>58.2</td> <td>52.6</td> <td>52.7</td> <td>53.0 ± 2.79</td> </tr> <tr> <td>LoRA (r=8)</td> <td>49.4</td> <td>51.8</td> <td>61.5</td> <td>48.7</td> <td>55.1</td> <td>53.3 ± 4.66</td> </tr> <tr> <td>MHA (baseline)</td> <td>62.7</td> <td>59.9</td> <td>61.7</td> <td>61.3</td> <td>65.7</td> <td>62.3 ± 2.00</td> </tr> <tr> <td>MHA → BN</td> <td>62.7</td> <td>59.9</td> <td>62.9</td> <td>64.8</td> <td>66.6</td> <td>63.4 ± 2.50</td> </tr> <tr style="background-color: #e8f5e9;"> <td><strong>DAAMv2</strong></td> <td><strong>66.1</strong></td> <td><strong>60.0</strong></td> <td><strong>66.3</strong></td> <td><strong>65.2</strong></td> <td><strong>65.4</strong></td> <td><strong>64.6 ± 2.47</strong></td> </tr> <tr style="background-color: #e8f5e9;"> <td><strong>GQDAAM</strong></td> <td><strong>66.5</strong></td> <td><strong>65.4</strong></td> <td><strong>68.7</strong></td> <td><strong>65.9</strong></td> <td><strong>66.8</strong></td> <td><strong>66.7 ± 1.18</strong></td> </tr> <tr style="background-color: #e8f5e9;"> <td><strong>DAAMv1</strong></td> <td><strong>67.2</strong></td> <td><strong>64.6</strong></td> <td><strong>68.1</strong></td> <td><strong>67.9</strong></td> <td><strong>69.0</strong></td> <td><strong>67.4 ± 1.49</strong></td> </tr> </tbody> </table> <figcaption><strong>Table 2:</strong> IEMOCAP 5-fold cross-validation results using WavLM-Large. DAAM variants significantly outperform all baselines including LoRA methods.</figcaption> </div> <p><strong>Key Findings:</strong></p> <ul> <li><strong>DAAMv1 achieves 67.4%</strong> accuracy, a <strong>+5.1% absolute improvement</strong> over MHA baseline (62.3%)</li> <li><strong>Dramatically outperforms LoRA methods:</strong> +14.1% over best LoRA (r=8: 53.3%)</li> <li><strong>Superior stability:</strong> DAAMv1 shows σ=1.49 vs LoRA’s σ=4.66</li> <li><strong>LoRA+ completely fails</strong> on this task (24-25% accuracy), demonstrating the importance of attention-based PEFT for non-stationary speech data</li> </ul> <hr/> <h3 id="image-classification-cifar-100">Image Classification: CIFAR-100</h3> <p>Using BEiT-Large as the frozen encoder on CIFAR-100 (100 classes, 50K train / 10K validation):</p> <p><strong>Complete 5-run results with all baselines:</strong></p> <div class="l-body"> <table> <thead> <tr> <th>Method</th> <th>R1</th> <th>R2</th> <th>R3</th> <th>R4</th> <th>R5</th> <th>Mean ± Std</th> </tr> </thead> <tbody> <tr> <td>LoRA+ (r=4)</td> <td>20.2</td> <td>21.1</td> <td>26.8</td> <td>17.9</td> <td>24.5</td> <td>22.1 ± 3.17</td> </tr> <tr> <td>LoRA+ (r=8)</td> <td>25.0</td> <td>32.9</td> <td>22.9</td> <td>29.1</td> <td>27.5</td> <td>27.5 ± 3.44</td> </tr> <tr> <td>LoRA (r=4)</td> <td>35.7</td> <td>32.3</td> <td>31.5</td> <td>36.2</td> <td>40.1</td> <td>35.2 ± 3.08</td> </tr> <tr> <td>LoRA (r=8)</td> <td>38.1</td> <td>40.0</td> <td>42.3</td> <td>41.6</td> <td>39.6</td> <td>40.3 ± 1.49</td> </tr> <tr> <td>MHA (baseline)</td> <td>60.4</td> <td>61.9</td> <td>62.1</td> <td>62.0</td> <td>62.1</td> <td>61.7 ± 0.75</td> </tr> <tr> <td>MHA → BN</td> <td>63.0</td> <td>67.1</td> <td>69.5</td> <td>63.9</td> <td>67.0</td> <td>66.1 ± 2.25</td> </tr> <tr style="background-color: #e8f5e9;"> <td><strong>GQDAAM</strong></td> <td><strong>80.0</strong></td> <td><strong>80.1</strong></td> <td><strong>80.1</strong></td> <td><strong>80.6</strong></td> <td><strong>80.0</strong></td> <td><strong>80.1 ± 0.24</strong></td> </tr> <tr style="background-color: #e8f5e9;"> <td><strong>DAAMv1</strong></td> <td><strong>79.9</strong></td> <td><strong>80.2</strong></td> <td><strong>80.2</strong></td> <td><strong>80.7</strong></td> <td><strong>80.7</strong></td> <td><strong>80.3 ± 0.32</strong></td> </tr> <tr style="background-color: #e8f5e9;"> <td><strong>DAAMv2</strong></td> <td><strong>80.2</strong></td> <td><strong>80.4</strong></td> <td><strong>81.0</strong></td> <td><strong>80.3</strong></td> <td><strong>81.0</strong></td> <td><strong>80.6 ± 0.36</strong></td> </tr> </tbody> </table> <figcaption><strong>Table 3:</strong> CIFAR-100 5-run validation results using BEiT-Large. DAAM achieves dramatic improvements over all baseline methods.</figcaption> </div> <p><strong>Key Findings:</strong></p> <ul> <li><strong>DAAMv2 achieves 80.6%</strong> accuracy, a <strong>+18.9% absolute improvement</strong> over MHA baseline (61.7%)</li> <li><strong>Massive improvement over LoRA:</strong> +40.3% over best LoRA (r=8: 40.3%)</li> <li><strong>Most dramatic gains across all modalities</strong>, demonstrating DAAM’s strength on complex visual classification tasks</li> <li><strong>Exceptional stability:</strong> σ=0.24-0.36 for DAAM vs σ=3.44 for LoRA+</li> <li>Batch Normalization helps MHA (+4.4%) but DAAM still outperforms by +14.5%</li> </ul> <hr/> <h3 id="text-classification-ag-news">Text Classification: AG News</h3> <p>Using Llama2-13B as the frozen encoder on AG News (4-class news categorization, 120K train / 7.6K validation):</p> <p><strong>Complete 3-run results with all baselines:</strong></p> <div class="l-body"> <table> <thead> <tr> <th>Method</th> <th>R1</th> <th>R2</th> <th>R3</th> <th>Mean ± Std</th> </tr> </thead> <tbody> <tr> <td>LoRA+ (r=4)</td> <td>93.4</td> <td>65.9</td> <td>92.8</td> <td>84.0 ± 12.8</td> </tr> <tr> <td>LoRA+ (r=8)</td> <td>95.0</td> <td>69.8</td> <td>94.6</td> <td>86.5 ± 11.8</td> </tr> <tr style="background-color: #e8f5e9;"> <td><strong>DAAMv2</strong></td> <td><strong>94.4</strong></td> <td><strong>94.5</strong></td> <td><strong>94.6</strong></td> <td><strong>94.5 ± 0.08</strong></td> </tr> <tr> <td>MHA → BN</td> <td>94.5</td> <td>94.5</td> <td>94.7</td> <td>94.6 ± 0.11</td> </tr> <tr> <td>MHA (baseline)</td> <td>94.4</td> <td>94.5</td> <td>94.8</td> <td>94.6 ± 0.16</td> </tr> <tr style="background-color: #e8f5e9;"> <td><strong>DAAMv1</strong></td> <td><strong>94.5</strong></td> <td><strong>94.5</strong></td> <td><strong>94.7</strong></td> <td><strong>94.6 ± 0.11</strong></td> </tr> <tr> <td>LoRA (r=8)</td> <td>94.9</td> <td>94.6</td> <td>94.9</td> <td>94.8 ± 0.14</td> </tr> <tr style="background-color: #e8f5e9;"> <td><strong>GQDAAM</strong></td> <td><strong>94.8</strong></td> <td><strong>94.9</strong></td> <td><strong>94.9</strong></td> <td><strong>94.9 ± 0.06</strong></td> </tr> <tr> <td>LoRA (r=4)</td> <td>95.1</td> <td>94.5</td> <td>95.3</td> <td>95.0 ± 0.3</td> </tr> </tbody> </table> <figcaption><strong>Table 4:</strong> AG News 3-run validation results using Llama2-13B. DAAM performs competitively while maintaining superior stability.</figcaption> </div> <p><strong>Key Findings:</strong></p> <ul> <li><strong>GQDAAM achieves 94.9%</strong>, matching best LoRA performance (95.0%) while using 80% fewer parameters</li> <li><strong>LoRA (r=4) performs best</strong> at 95.0%, but only marginally (+0.1% over GQDAAM)</li> <li><strong>LoRA+ shows catastrophic instability:</strong> σ=11.8-12.8 with R2 dropping to 65.9-69.8%</li> <li><strong>DAAM methods show exceptional stability:</strong> σ=0.06-0.11 vs σ=0.3 for LoRA</li> <li>Text data is relatively stationary, so gains are modest but DAAM provides reliability</li> </ul> <hr/> <h3 id="cross-modal-performance-summary">Cross-Modal Performance Summary</h3> <div class="l-body"> <table> <thead> <tr> <th>Modality</th> <th>Dataset</th> <th>MHA Baseline</th> <th>Best LoRA</th> <th>Best DAAM</th> <th>Improvement vs MHA</th> <th>Improvement vs LoRA</th> </tr> </thead> <tbody> <tr> <td><strong>Speech</strong></td> <td>IEMOCAP</td> <td>62.3%</td> <td>53.3%</td> <td><strong>67.4%</strong></td> <td><strong>+5.1%</strong></td> <td><strong>+14.1%</strong></td> </tr> <tr> <td><strong>Vision</strong></td> <td>CIFAR-100</td> <td>61.7%</td> <td>40.3%</td> <td><strong>80.6%</strong></td> <td><strong>+18.9%</strong></td> <td><strong>+40.3%</strong></td> </tr> <tr> <td><strong>Text</strong></td> <td>AG News</td> <td>94.6%</td> <td>95.0%</td> <td><strong>94.9%</strong></td> <td><strong>+0.3%</strong></td> <td><strong>-0.1%</strong></td> </tr> </tbody> </table> <figcaption><strong>Table 5:</strong> Summary of best results across all modalities. DAAM excels on non-stationary data (speech, vision) and remains competitive on stationary data (text).</figcaption> </div> <p><strong>Critical Insights:</strong></p> <ol> <li><strong>Non-stationary data advantage:</strong> DAAM shows largest gains on speech (+5.1%) and vision (+18.9%) where data variability is high</li> <li><strong>LoRA limitations:</strong> LoRA methods struggle significantly with non-stationary data, failing catastrophically on IEMOCAP and CIFAR-100</li> <li><strong>Stability advantage:</strong> DAAM maintains low variance across all tasks while LoRA+ shows high instability</li> <li><strong>Parameter efficiency:</strong> DAAM achieves these results with 80% fewer parameters than LoRA</li> <li><strong>Stationary data performance:</strong> On well-structured text data (AG News), performance converges across methods, but DAAM maintains best stability</li> </ol> <p><strong>Why DAAM Outperforms LoRA:</strong></p> <ul> <li><strong>LoRA adapts weight matrices</strong> uniformly, which doesn’t handle feature-level variability well</li> <li><strong>DAAM adapts feature importance</strong> dynamically through learnable Gaussian distributions</li> <li><strong>LoRA+ instability</strong> suggests the differential learning rates harm performance on complex, non-stationary tasks</li> <li><strong>Batch Normalization limitations:</strong> While BN helps MHA (+4.4% on CIFAR-100), it assumes i.i.d. data across mini-batches, which DAAM doesn’t require</li> </ul> <hr/> <hr/> <h2 id="understanding-the-density-adaptive-mechanism-learned-parameters-analysis">Understanding the Density Adaptive Mechanism, Learned Parameters Analysis</h2> <p>To validate that DAAM truly adapts to different data characteristics, we analyze the learned Gaussian parameters (mean offsets and scaled variances) across all three modalities after training.</p> <h3 id="learned-parameter-ranges-by-modality">Learned Parameter Ranges by Modality</h3> <div class="l-body"> <table> <thead> <tr> <th>Modality</th> <th>Mean Offset Range</th> <th>Scaled Variance Range</th> <th>Total Variability</th> </tr> </thead> <tbody> <tr> <td><strong>Speech (IEMOCAP)</strong></td> <td>[-0.06, 0.10]</td> <td>[1.88, 2.06]</td> <td><strong>High</strong></td> </tr> <tr> <td><strong>Text (AG News)</strong></td> <td>[-0.05, 0.07]</td> <td>[1.94, 2.02]</td> <td><strong>Moderate</strong></td> </tr> <tr> <td><strong>Vision (CIFAR-100)</strong></td> <td>[-0.02, 0.02]</td> <td>[1.98, 2.03]</td> <td><strong>Low</strong></td> </tr> </tbody> </table> <figcaption><strong>Table:</strong> Range of learned Gaussian parameters for normalized features across modalities (best performing DAAM models with g=8 heads). Mean offset controls the center of attention, while scaled variance controls the spread.</figcaption> </div> <h3 id="what-these-parameters-reveal">What These Parameters Reveal</h3> <p>The learned parameter ranges provide crucial insights into why DAAM achieves different performance gains across modalities:</p> <p><strong>1. Speech Processing (High Variability → Largest Need for Adaptation)</strong></p> <p>Speech data exhibits <strong>high variability in both mean offset (μ) and scaled variance (σ²)</strong>:</p> <ul> <li><strong>Mean offset range: 0.16</strong> (from -0.06 to 0.10)</li> <li><strong>Variance range: 0.18</strong> (from 1.88 to 2.06)</li> </ul> <p><strong>Why this matters:</strong></p> <ul> <li>Speech is <strong>highly non-stationary</strong> <d-cite key="9440639"></d-cite> — emotional states, speaking rates, and acoustic properties change rapidly within and across utterances</li> <li>DAAM must dynamically adjust <strong>both the center (μ)</strong> of attention to track shifting emotional cues and <strong>the spread (σ)</strong> to handle varying temporal scales</li> <li>This high adaptability requirement explains the <strong>+5.1% improvement</strong> over MHA on IEMOCAP</li> <li>The wide parameter ranges show DAAM learned to focus on different acoustic features at different scales</li> </ul> <p><strong>2. Text Processing (Moderate Variability → Structured Adaptation)</strong></p> <p>Text data shows <strong>high mean variation but stable variance</strong>:</p> <ul> <li><strong>Mean offset range: 0.12</strong> (from -0.05 to 0.07)</li> <li><strong>Variance range: 0.08</strong> (from 1.94 to 2.02)</li> </ul> <p><strong>Why this matters:</strong></p> <ul> <li>Text has <strong>structured but shifting semantic contexts</strong> — topic changes, but sentence structure remains relatively consistent</li> <li>DAAM primarily adapts the <strong>attention center (μ)</strong> to track changing semantic focal points (topic shifts, key entities)</li> <li>The <strong>variance remains stable</strong> because the “window” of relevant context is relatively constant in language</li> <li>This moderate adaptation need explains the <strong>+0.3% improvement</strong> — text is already relatively stationary, so adaptive mechanisms provide smaller gains</li> <li>Aligns with the structured nature of language where focal points shift but spread remains consistent</li> </ul> <p><strong>3. Vision Processing (Low Variability → Stable Features, But Still Benefits)</strong></p> <p>Vision data demonstrates <strong>low variation in both parameters</strong>:</p> <ul> <li><strong>Mean offset range: 0.04</strong> (from -0.02 to 0.02)</li> <li><strong>Variance range: 0.05</strong> (from 1.98 to 2.03)</li> </ul> <p><strong>Why this matters:</strong></p> <ul> <li>Visual features are <strong>relatively stable and consistent</strong> in their spatial locations and spreads</li> <li>Edge detectors, texture patterns, and object parts don’t shift dramatically in their statistical properties</li> <li><strong>Yet DAAM still achieves +18.9% improvement!</strong> This reveals something important: <ul> <li>Even with low parameter variability, the <strong>precise fine-tuning</strong> DAAM provides is crucial</li> <li>The ability to model <strong>subtle variations in feature distributions</strong> matters enormously for complex visual recognition</li> <li>Small adjustments in attention can have large impacts on classification accuracy</li> </ul> </li> <li>Suggests that while features are stable, the <strong>optimal attention distribution</strong> for classification is non-trivial</li> </ul> <h3 id="why-these-insights-matter">Why These Insights Matter</h3> <p><strong>Empirical Validation of Theoretical Claims:</strong></p> <ol> <li><strong>Non-stationary data benefits most:</strong> The correlation between parameter variability and performance gains validates DAAM’s core motivation <ul> <li>High variability (speech) → +5.1% gain</li> <li>Moderate variability (text) → +0.3% gain</li> <li>Low variability but high complexity (vision) → +18.9% gain</li> </ul> </li> <li><strong>DAAM adapts to data characteristics:</strong> The learned parameters directly reflect the statistical properties of each modality <ul> <li>Speech: needs both center and spread adaptation</li> <li>Text: primarily needs center adaptation</li> <li>Vision: needs precise fine-tuning</li> </ul> </li> <li><strong>Contrast with static attention:</strong> Traditional MHA uses <strong>fixed dot-product operations</strong> that cannot adapt parameters to data characteristics <ul> <li>MHA attention weights are computed from fixed $W^Q$ and $W^K$ matrices</li> <li>DAAM learns <strong>data-specific distributions</strong> through its Gaussian parameters</li> </ul> </li> <li><strong>Design implications:</strong> These results suggest: <ul> <li>For highly non-stationary data: use full DAAM with multiple heads</li> <li>For moderately stationary data: simpler attention may suffice</li> <li>For complex visual tasks: even subtle adaptation provides massive gains</li> </ul> </li> </ol> <h3 id="connection-to-multiple-gaussian-components">Connection to Multiple Gaussian Components</h3> <p>Recall that each DAAM head can model multiple Gaussian components. The parameter ranges above show:</p> <ul> <li><strong>Speech:</strong> Needs diverse Gaussians (wide ranges) to capture emotional variability</li> <li><strong>Text:</strong> Needs focused Gaussians (moderate ranges) to track semantic shifts</li> <li><strong>Vision:</strong> Needs precise Gaussians (tight ranges) for fine-grained discrimination</li> </ul> <p>This empirically demonstrates that <strong>DAAM’s multi-head, multi-Gaussian architecture</strong> is essential for approximating the complex, non-Gaussian distributions present in real-world multimodal data.</p> <hr/> <h2 id="explainability-through-importance-factors">Explainability Through Importance Factors</h2> <p>Traditional self-attention provides correlation matrices between sequence elements. DAAM introduces the <strong>Importance Factor (IF)</strong>, a new learning-based metric that enhances the explainability of models trained with DAAM-based methods.</p> <h3 id="definition">Definition</h3> <p>For density attention weights $\text{DA}$ produced by DAAM:</p> \[\text{IF} = \frac{\text{DA} - \min(\text{DA})}{\max(\text{DA}) - \min(\text{DA})} \in [0, 1]\] <p>Higher IF values indicate features that DAAM emphasizes during attention, quantitatively assessing feature significance for improved interpretability.</p> <h3 id="multi-modal-analysis">Multi-Modal Analysis</h3> <p>IF-based heatmaps are created by taking the arithmetic average of the generated Density Attention maps during validation and then applying the IF formula. They visually depict feature importance.</p> <div class="l-body"> <figure> <img src="/blog/assets/img/2025-10-25-density-adaptive-attention/daam/IF_WavLM_Large.png" alt="IF heatmap for speech"/> <figcaption><strong>Figure 2a:</strong> IF values for Speech Processing with WavLM-Large using DAAM. Output feature number on the X-axis and layer number on the Y-axis. The dense population of higher IF values at the lower layers suggests these layers' active role in modulating the input sequence.</figcaption> </figure> </div> <p><strong>Speech interpretation:</strong> This observation implies that fundamental speech features are likely captured initially, while upper layers refine these for more abstract representations.</p> <div class="l-body"> <figure> <img src="/blog/assets/img/2025-10-25-density-adaptive-attention/daam/IF_Llama2.png" alt="IF heatmap for text"/> <figcaption><strong>Figure 2b:</strong> IF values for Text Processing with Llama2-13B using GQDAAM. The figure exhibits a more uniform IF distribution across all layers with a slight concentration at the earlier layers.</figcaption> </figure> </div> <p><strong>Text interpretation:</strong> This pattern indicates a balanced hierarchical feature extraction approach, with both lower and higher-level features playing a significant role, particularly those extracted by the early to middle layers.</p> <div class="l-body"> <figure> <img src="/blog/assets/img/2025-10-25-density-adaptive-attention/daam/IF_BEiT_Large.png" alt="IF heatmap for vision"/> <figcaption><strong>Figure 2c:</strong> IF values for Digital Image Processing with BEiT-Large using GQDAAM. The figure emphasizes lower layer features.</figcaption> </figure> </div> <p><strong>Vision interpretation:</strong> This reflects the necessity of early-stage feature extraction in visual tasks, such as identifying edges and textures.</p> <p>These variations in IF value distribution underscore the distinct information processing requirements of each modality. Speech and image processing appear to rely on primary feature extraction, while text processing demands both fundamental and complex feature identification. The insights provided by IF analysis enhance the explainability of the models, offering a quantifiable measure of feature significance.</p> <h3 id="layer-contribution-analysis">Layer Contribution Analysis</h3> <p>Analysis of the layer contribution indicates earlier layers exhibit more meaningful features and contribute more to model performance, suggesting potential overparameterization in later layers <d-cite key="zhang2022platon"></d-cite>.</p> <div class="l-body"> <figure> <img src="/blog/assets/img/2025-10-25-density-adaptive-attention/daam/wavlm_contribution.png" alt="Layer contribution for speech"/> <figcaption><strong>Figure 3a:</strong> Percentage contribution of each layer to attention weights in Speech processing with WavLM-Large using DAAM.</figcaption> </figure> </div> <div class="l-body"> <figure> <img src="/blog/assets/img/2025-10-25-density-adaptive-attention/daam/llama_contribution.png" alt="Layer contribution for text"/> <figcaption><strong>Figure 3b:</strong> Percentage contribution of each layer to attention weights in Text processing with Llama2-13B using GQDAAM.</figcaption> </figure> </div> <div class="l-body"> <figure> <img src="/blog/assets/img/2025-10-25-density-adaptive-attention/daam/BEiT_contribution.png" alt="Layer contribution for vision"/> <figcaption><strong>Figure 3c:</strong> Percentage contribution of each layer to attention weights in Image processing with BEiT-Large using GQDAAM.</figcaption> </figure> </div> <hr/> <h3 id="validating-the-importance-factor-ablation-studies">Validating the Importance Factor: Ablation Studies</h3> <p>To rigorously validate that IF scores from DAAM accurately identify key feature extraction regions, we conduct systematic ablation experiments. We retrain models using <strong>only</strong> layers with high IF scores versus <strong>only</strong> layers with low IF scores, then compare performance.</p> <p><strong>Hypothesis:</strong> If IF truly measures layer importance, then high-IF layers should significantly outperform low-IF layers.</p> <h4 id="speech-iemocap-layer-ablation">Speech: IEMOCAP Layer Ablation</h4> <div class="l-body"> <table> <thead> <tr> <th>Layer Selection</th> <th>F1</th> <th>F2</th> <th>F3</th> <th>F4</th> <th>F5</th> <th>Average</th> <th>Std Dev</th> </tr> </thead> <tbody> <tr style="background-color: #e8f5e9;"> <td><strong>Layer 9 (High IF)</strong></td> <td><strong>65.9</strong></td> <td><strong>60.1</strong></td> <td><strong>64.4</strong></td> <td><strong>62.7</strong></td> <td><strong>67.0</strong></td> <td><strong>64.0</strong></td> <td><strong>2.40</strong></td> </tr> <tr style="background-color: #ffebee;"> <td>Layer 23 (Low IF)</td> <td>62.8</td> <td>58.9</td> <td>63.2</td> <td>62.0</td> <td>64.5</td> <td>62.3</td> <td>1.89</td> </tr> <tr> <td colspan="7"><strong>Performance Difference</strong></td> <td><strong>+1.7%</strong></td> </tr> </tbody> </table> <figcaption><strong>Table:</strong> Ablation study on IEMOCAP using WavLM-Large. Models trained using only single layers identified by IF scores. High-IF Layer 9 consistently outperforms low-IF Layer 23 across all folds.</figcaption> </div> <p><strong>Key Finding:</strong> Layer 9 (high IF score) achieves <strong>+1.7% absolute improvement</strong> over Layer 23 (low IF score), validating that IF scores correlate with actual layer importance for the downstream task.</p> <h4 id="vision-and-text-multi-layer-ablation">Vision and Text: Multi-Layer Ablation</h4> <div class="l-body"> <table> <thead> <tr> <th>Dataset</th> <th>Model</th> <th>High IF Layers</th> <th>Accuracy</th> <th>Low IF Layers</th> <th>Accuracy</th> <th>Difference</th> </tr> </thead> <tbody> <tr> <td><strong>AG News</strong></td> <td>Llama2-13B</td> <td>Layers 19-21</td> <td style="background-color: #e8f5e9;"><strong>94.9%</strong></td> <td>Layers 37-39</td> <td style="background-color: #ffebee;">94.7%</td> <td><strong>+0.2%</strong></td> </tr> <tr> <td><strong>CIFAR-100</strong></td> <td>BEiT-Large</td> <td>Layers 10-12</td> <td style="background-color: #e8f5e9;"><strong>72.6%</strong></td> <td>Layers 22-24</td> <td style="background-color: #ffebee;">64.7%</td> <td><strong>+7.9%</strong></td> </tr> </tbody> </table> <figcaption><strong>Table:</strong> Ablation results for text and vision tasks. Models trained using only 3-layer groups identified by IF scores. High-IF layers substantially outperform low-IF layers, especially for vision.</figcaption> </div> <p><strong>Key Findings:</strong></p> <ul> <li><strong>Text processing:</strong> Modest but consistent gain (+0.2%), reflecting the distributed nature of linguistic feature extraction</li> <li><strong>Vision processing:</strong> Dramatic gain (+7.9%), demonstrating that early layers (10-12) capturing low-level features are critical for image classification</li> </ul> <h3 id="why-if-is-superior-to-traditional-attention-weights">Why IF is Superior to Traditional Attention Weights</h3> <p><strong>Critical Distinction: Correlation vs. Importance</strong></p> <p>In standard Multi-Head Attention (MHA), attention weights indicate the level of <strong>correlation</strong> <d-cite key="Lovisotto2022GiveMY"></d-cite> between different parts of the input sequence. Each element’s weight reflects its relevance to every other element within the same sequence. <strong>However, this does not directly translate to performance on downstream tasks.</strong></p> <p><strong>Case Study: The Layer 23 Paradox</strong></p> <p>Previous work <d-cite key="ioannides23_interspeech"></d-cite> using MHA for speech emotion recognition on IEMOCAP with WavLM-Large identified <strong>Layer 23 as “pivotal”</strong> based on normalized self-attention weights. This layer showed high attention weights, suggesting it was important.</p> <p><strong>But our ablation study reveals:</strong></p> <ul> <li>Layer 23 (identified as “important” by MHA): <strong>62.3% accuracy</strong></li> <li>Layer 9 (identified as “important” by DAAM IF): <strong>64.0% accuracy</strong></li> </ul> <p><strong>Why the discrepancy?</strong></p> <ol> <li><strong>MHA attention weights measure inter-layer correlation:</strong> <ul> <li>High attention weights → strong relationships between tokens</li> <li>Does NOT mean these relationships help the downstream task</li> <li>Can identify layers that are “active” but not “useful”</li> </ul> </li> <li><strong>DAAM’s IF measures task-aligned importance:</strong> <ul> <li>High IF scores → features that improve classification</li> <li>Directly optimized for the end goal during training</li> <li>Identifies layers whose features actually matter for performance</li> </ul> </li> </ol> <p><strong>Implications:</strong></p> <ul> <li><strong>For model interpretability:</strong> IF provides more actionable insights than MHA attention weights</li> <li><strong>For architecture optimization:</strong> IF can guide layer pruning and model compression</li> <li><strong>For transfer learning:</strong> IF helps identify which layers to fine-tune vs. freeze</li> </ul> <h3 id="cross-modal-patterns-in-layer-importance">Cross-Modal Patterns in Layer Importance</h3> <p>Across all three modalities, we observe consistent patterns:</p> <p><strong>Speech (WavLM):</strong></p> <ul> <li>High IF concentration in <strong>lower layers (especially layer 9)</strong></li> <li>Suggests fundamental acoustic features captured early are most predictive</li> <li>Upper layers refine but don’t add as much discriminative power</li> </ul> <p><strong>Text (Llama2):</strong></p> <ul> <li>More <strong>uniform IF distribution</strong> across layers with slight emphasis on early-to-middle (19-21)</li> <li>Reflects hierarchical feature extraction: both low-level (syntax) and high-level (semantics) matter</li> <li>Balanced importance across the network</li> </ul> <p><strong>Vision (BEiT):</strong></p> <ul> <li>Strong emphasis on <strong>lower layers (10-12)</strong></li> <li>Early-stage features (edges, textures, colors) are critical for classification</li> <li>Aligns with visual processing theory: low-level features compose high-level representations</li> </ul> <h3 id="practical-applications-of-if">Practical Applications of IF</h3> <p>The validated IF metric enables several practical applications:</p> <ol> <li><strong>Efficient Fine-Tuning:</strong> Focus adaptation on high-IF layers only</li> <li><strong>Interpretability:</strong> Understand which encoder layers contribute to decisions</li> </ol> <p><strong>Example:</strong> Analysis of Figure 3a-c (layer contribution) indicates earlier layers contribute more to model performance, suggesting <strong>potential overparameterization in later layers</strong> <d-cite key="zhang2022platon"></d-cite>. This insight could guide future architecture designs.</p> <hr/> <h2 id="advanced-applications">Advanced Applications</h2> <h3 id="vector-quantized-variational-autoencoder-vq-vae">Vector Quantized Variational Autoencoder (VQ-VAE)</h3> <p>To demonstrate the applicability of DAAM in more complex architectures, the paper integrates the proposed attention mechanism into a Vector Quantized Variational Autoencoder (VQ-VAE). This application represents a significantly more challenging use case than the classification tasks presented in the main paper.</p> <p><strong>Architecture:</strong></p> <p>The DAAM-enhanced VQ-VAE architecture consists of:</p> <p><strong>Encoder:</strong> Initial convolution followed by a series of downsampling blocks. Each DownSampleBlock consists of strided convolution (stride=2) for downsampling, Group normalization, ReLU activation, and a DAAM-enhanced residual block that applies Multi-Head Density Adaptive Attention on the channel dimension.</p> <p><strong>Vector Quantizer:</strong> Maps continuous encodings to nearest vectors in a learned discrete codebook containing num_embeddings vectors of embedding_dim dimensions.</p> <p><strong>Decoder:</strong> Initial convolution followed by a series of upsampling blocks. Each UpSampleBlock consists of transposed convolution (stride=2) for upsampling, Group normalization, ReLU activation, and a DAAM-enhanced residual block with Multi-Head Density Adaptive Attention.</p> <div class="l-body"> <figure> <img src="/blog/assets/img/2025-10-25-density-adaptive-attention/daam/vqvae.png" alt="VQ-VAE Model Architecture"/> <figcaption><strong>Figure 4:</strong>Vector-Quantization Variational Auto-Encoder with Density Adaptive Attention Mechanism.</figcaption> </figure> </div> <p>This architecture applies DAAM along the channel dimension (norm_axis=1), which is particularly effective for enhancing feature representation in the bottleneck of the VQ-VAE.</p> <p><strong>Training details:</strong></p> <ul> <li>Hidden dimensions: [128, 256, 512]</li> <li>Latent dimension: 256</li> <li>Codebook size: 1024 embeddings</li> <li>Image size: 256×256</li> <li>DAAM configuration: 4 heads with 3 Gaussians per head</li> <li>Optimizer: Adam <d-cite key="kingma2017adam"></d-cite>, lr=3e-4</li> <li>Epochs: 176</li> <li>Batch size: 32</li> <li>Dataset: COCO-2017 <d-cite key="lin2017coco"></d-cite></li> </ul> <div class="l-body"> <figure> <img src="/blog/assets/img/2025-10-25-density-adaptive-attention/daam/reconstruction_epoch_176.png" alt="VQ-VAE reconstructions"/> <figcaption><strong>Figure 5:</strong> Reconstruction examples from the DAAM-enhanced VQ-VAE model. Top: original images; Bottom: reconstructions. The DAAM mechanism improves reconstruction quality, particularly for complex textures, fine details, and high-frequency components. Captures text and facial features in fine detail.</figcaption> </figure> </div> <p>The integration of DAAM substantially improves reconstruction quality, particularly for fine details and textures that are challenging for standard VQ-VAE models. The DAAM mechanism proves particularly effective at addressing common VQ-VAE failure modes such as blurriness and loss of texture details. By dynamically adjusting attention across channels based on input content, the model preserves more perceptually important features.</p> <p>This application demonstrates DAAM’s versatility beyond classification tasks, showcasing its effectiveness in generative modeling contexts where adaptive feature selection is crucial for high-quality outputs.</p> <h3 id="mixture-of-densities-extension-daamv2">Mixture of Densities Extension (DAAMv2)</h3> <p>The paper presents an extension of the Multi-Head Density Adaptive Attention Mechanism (DAAM), focusing on enhancing the stability of the training process and the model’s efficiency by significantly reducing the number of learnable parameters even further.</p> <p><strong>Algorithm:</strong></p> <div class="l-body"> <div class="fake-img"> <p style="font-family: monospace; font-size: 0.9em;"> <strong>Algorithm: Mixture of Densities Adaptive Attention</strong><br/> <strong>Input:</strong> x (input tensor), normAxis, N Gaussians, eps<br/> <strong>Output:</strong> Attention-modified x<br/> <br/> 1. Initialize m, c of size N<br/> &nbsp;&nbsp;&nbsp;μ ← mean(x, axis=normAxis)<br/> &nbsp;&nbsp;&nbsp;σ² ← var(x, axis=normAxis) + eps<br/> &nbsp;&nbsp;&nbsp;mixture ← 1<br/> <br/> 2. For i = 0 to N-1:<br/> &nbsp;&nbsp;&nbsp;μᵢᵃᵈʲ ← μ + m[i]<br/> &nbsp;&nbsp;&nbsp;yᵢ ← (x - μᵢᵃᵈʲ) / √(σ²)<br/> &nbsp;&nbsp;&nbsp;gᵢ ← exp(-yᵢ²/(2·c[i]²)) / √(2π·c[i]²)<br/> &nbsp;&nbsp;&nbsp;mixture ← mixture · gᵢ<br/> <br/> 3. Normalize mixture across normAxis<br/> 4. x' ← x · mixture<br/> 5. Return x' </p> </div> </div> <p>The extended DAAM incorporates multiple attention heads, each with its Gaussian mixture model, to process different segments of the input tensor in parallel. Additionally, the algorithm adds the original input features to the augmented one for enhanced stability during training (i.e., X’ ← X’ + X).</p> <p><strong>Extended Results:</strong></p> <div class="l-body"> <table> <thead> <tr> <th>Dataset</th> <th>Method</th> <th>Results</th> </tr> </thead> <tbody> <tr> <td>IEMOCAP</td> <td>Mixture of DAAM</td> <td>67.9 ± 1.35</td> </tr> <tr> <td>CIFAR-100</td> <td>Mixture of DAAM</td> <td>80.3 ± 0.30</td> </tr> <tr> <td>AG News</td> <td>Mixture of DAAM</td> <td>94.6 ± 0.05</td> </tr> </tbody> </table> </div> <p>It is evident that Mixture of DAAM not only outperforms DAAM but it also reduces its overall trainable parameter count significantly. With only 64 parameters (8 heads × 4 Gaussians × 2 params), this achieves substantial parameter reduction.</p> <hr/> <h2 id="limitations-and-future-work">Limitations and Future Work</h2> <p>The paper acknowledges the following limitations:</p> <p><strong>Fixed number of Gaussians:</strong> The Density Adaptive Attention mechanism’s fixed number of Gaussians can limit its adaptability across different datasets and tasks.</p> <p><strong>Proposed improvements:</strong></p> <ol> <li>Implementing Bayesian approaches with Bayesian Information Criterion for dynamic Gaussian selection</li> <li>Exploring DAAM in more diverse tasks, datasets, and grounding experiments</li> <li>Model compression via attention weights during training—particularly valuable for resource-constrained applications</li> </ol> <hr/> <h2 id="conclusion">Conclusion</h2> <p>This work introduces the Multi-Head Density Adaptive Attention Mechanism and the Density Adaptive Transformer, demonstrating their effectiveness in enhancing model performance, particularly with highly non-stationary data. Results show that combining learnable mean and variance for multiple Gaussian Distributions enables dynamic feature significance recalibration and approximation of any Probability Distribution across multiple modalities.</p> <p><strong>Key contributions:</strong></p> <ol> <li>DAAM with fully learnable Gaussian parameters enabling dynamic recalibration of feature importance</li> <li>Introduction of the Importance Factor for improved model explainability</li> <li>Comprehensive validation across Speech, Text, and Vision modalities</li> <li>Integration with Grouped Query Attention with minimal parameter increase (0.016%-0.08% compared to GQA models) and 80% fewer parameters than LoRA</li> </ol> <p><strong>Results summary:</strong></p> <table> <thead> <tr> <th>Modality</th> <th>Dataset</th> <th>Baseline</th> <th>Best DAAM</th> <th>Improvement</th> </tr> </thead> <tbody> <tr> <td>Speech</td> <td>IEMOCAP</td> <td>62.3%</td> <td>67.4%</td> <td>+5.1%</td> </tr> <tr> <td>Vision</td> <td>CIFAR-100</td> <td>61.7%</td> <td>80.6%</td> <td>+18.9%</td> </tr> <tr> <td>Text</td> <td>AG News</td> <td>94.6%</td> <td>94.9%</td> <td>+0.3%</td> </tr> </tbody> </table> <p>Overall, DAAM represents an advancement towards development of better performing and more explainable attention models across multiple modalities.</p> <hr/> <h2 id="acknowledgments">Acknowledgments</h2> <p>We thank the creators of WavLM, Llama, and BEiT for releasing their pre-trained models. We are grateful for the IEMOCAP, Librilight, AG News, CIFAR-100, and COCO datasets that enabled this research.</p> <hr/> <h2 id="code-availability">Code Availability</h2> <p>The complete implementation is available on GitHub: <a href="https://github.com/gioannides/DAAM-paper-code">https://github.com/gioannides/DAAM-paper-code</a></p> <p>Source code has also been uploaded in the supplementary material section.</p> <hr/>]]></content><author><name>Georgios Ioannides</name></author><category term="gaussian"/><category term="representation"/><category term="learning,"/><category term="attention-mechanisms"/><category term="transformers"/><category term="parameter-efficient,"/><category term="multi-modal"/><category term="deep-learning"/><summary type="html"><![CDATA[A novel probabilistic attention framework that dynamically recalibrates feature significance through learnable Gaussian distributions]]></summary></entry><entry><title type="html">Shared Coordinates for Cross-Subject Brain Dynamics: Universal Latents and Directly Comparable Phase Diagrams</title><link href="https://unireps.org//blog/2025/phase-diagram-playbook/" rel="alternate" type="text/html" title="Shared Coordinates for Cross-Subject Brain Dynamics: Universal Latents and Directly Comparable Phase Diagrams"/><published>2025-10-25T00:00:00+00:00</published><updated>2025-10-25T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/phase-diagram-playbook</id><content type="html" xml:base="https://unireps.org//blog/2025/phase-diagram-playbook/"><![CDATA[<h2 id="overview-tldr">Overview (TL;DR)</h2> <p>We present a modular, modality-agnostic workflow that turns heterogeneous whole-brain time series into cohort-comparable, interpretable coordinates on a shared phase diagram, together with energy-landscape descriptors such as attractors, barriers, and kinetics.</p> <p>Key steps: 1) population-universal latent spaces (Shared Response Model - SRM, Multiset Canonical Correlation Analysis - MCCA, Group PCA or Group ICA with consensus and automated dimensionality selection) <d-cite key="NIPS2015_b3967a0e,60b14841-3c7e-3751-88f2-15308f78bf55,decheveigne2019mcca,correa2010mcca,SMITH2014738,calhoun2001groupica"></d-cite> 2) per-latent binarisation to the +/-1 format 3) Pairwise Maximum Entropy Model (PMEM) or Ising fitting: exact for small N, pseudo-likelihood, or variational Bayes <d-cite key="jaynes1957maxent,schneidman2006nature,besag1975pl,ravikumar2010ising,opper2001advancedmf"></d-cite> 4) energy landscape analysis (ELA): minima, disconnectivity, barriers, occupancies, kinetics <d-cite key="watanabe2014ela,becker1997disconnectivity,wales2006jpcb"></d-cite> 5) phase diagram analysis (PDA): novel multi-observable placement on a shared reference surface with uncertainty <d-cite key="edwards1975ea,sherrington1975sk,ezaki2020critical"></d-cite></p> <p>Outputs include uncertainty, quality control, and interactive visuals. Methods are user-tweakable, reliable, and reproducible.</p> <hr/> <h2 id="introduction">Introduction</h2> <h4 id="1-bigpicture-overview---why-shared-latents--pmem--ela--pda">1. Big‑picture overview - why shared latents + PMEM → ELA → PDA?</h4> <p>Modern whole‑brain recordings are heterogeneous across subjects, sessions, tasks and modalities. If we analyse each participant in their own idiosyncratic space, descriptors of “brain state” are not directly comparable. Our pipeline solves this in two moves:</p> <ol> <li>Population‑universal shared latents: We align subjects into a common, low‑dimensional space (SRM / MCCA / Group PCA‑ICA with consensus). Variables have stable meaning across participants and runs, so everything downstream is comparable and reproducible.</li> <li> <p>Physics‑grounded descriptors: On the binarised latents we fit a pairwise maximum‑entropy model (PMEM/Ising), then read out two complementary summaries:</p> <ul> <li>Energy‑Landscape Analysis (ELA) - an attractor‑and‑barrier view of the fitted Ising energy. It yields minima/basins, disconnectivity graphs, barrier spectra, and kinetic descriptors (basin dwell times, mean first-passage times (MFPTs), committors, relaxation times). This is the mechanistic, state‑space view.</li> <li>Phase‑Diagram Analysis (PDA) - a macroscopic view that places each subject on the <strong>(\(\mu\), \(\sigma\))</strong> plane of a disordered (via parametric perturbations) Ising model (SK‑like). In broad outline, it uses multiple observables at once to locate individuals relative to critical boundaries, providing cohort‑comparable coordinates and uncertainty.</li> </ul> </li> </ol> <h4 id="2-where-the-methods-come-from-intuitive-recap">2. Where the methods come from (intuitive recap)</h4> <ul> <li><strong>PMEM/Ising:</strong> Among all binary models that match the empirical means and pairwise correlations, PMEM has maximum entropy. It is equivalent to the zero‑temperature Ising family used throughout statistical physics. Minimal assumptions; parameters are interpretable as fields \(h_i\) and couplings \(J_{ij}\).</li> <li> <p><strong>ELA:</strong> Treat the fitted Ising as an energy landscape:</p> <p>\(E(\mathbf{s}) = -\sum_i h_i s_i - \tfrac{1}{2}\sum_{i\neq j} J_{ij} s_i s_j\),</p> <p>over binary states \(\mathbf{s}\in\{-1,+1\}^N\).</p> <p>Local minima = attractors; energy differences = barriers; transition graphs + Markov kinetics = how the brain moves between them.</p> </li> <li><strong>PDA:</strong> In spin‑glass models, the distribution of couplings matters. If the off‑diagonal \(J_{ij}\) have mean \(\mu\) and standard deviation \(\sigma\) with \(h_i\approx 0\), the system sits in regions (paramagnetic / spin‑glass / ferromagnetic) that govern ordering, glassiness and susceptibility. PDA maps each subject onto this phase surface so cohorts can be compared at a glance.</li> </ul> <h4 id="3-why-shared-latents-are-necessary-and-useful">3. Why shared latents are necessary (and useful)</h4> <ul> <li><strong>Stable semantics:</strong> Each latent represents the same population‑level pattern across participants, which makes ELA basins and PDA coordinates directly comparable.</li> <li><strong>Tractability:</strong> PMEM scales as \(\mathcal{O}(N^2)\) parameters; a well‑chosen latent space puts us in the sweet spot between information retention and robust estimation.</li> <li><strong>Downstream identifiability:</strong> Binarisation → PMEM → ELA/PDA relies on on/off switching. Our alignment preserves this structure and gives us comparable switching rasters across the cohort.</li> <li><strong>Utility‑forward:</strong> with one aligned space we can publish shared phase diagrams and landscape reports that are re‑usable across datasets and modalities, enabling baseline‑independent comparisons and cross‑study synthesis.</li> </ul> <p><strong>Select practical advantages of our framework:</strong></p> <p>The workflow is largely standalone and implemented locally, from scratch - allowing researchers/analysts to adapt its workings to their exact needs - with numerical-stability and computational-efficiency improvements (relative to analogous implementations in the domain), novel algorithmic extensions (i.a. for multi-subject dimensionality reduction, comparative phase diagram analysis of heterogeneous brain dynamics), informative metrics and rich visualisations, and emphasis on <strong>a)</strong> automated parameter optimisation - not requiring domain-expertise or significant prior experience with the pipeline from the user, <strong>b)</strong> data-driven model selection, <strong>c)</strong> data-modality universality and independence of heuristics/meta-knowledge throughout the entire design process, and <strong>d)</strong> availability of alternative methods and hyperparameters for key processing/modelling stages, so as to best fit the needs of the user</p> <p><strong>Limitations worth remembering:</strong></p> <ul> <li>Binarisation coarsens signals (whereas more granular discretisation becomes computationally prohibitive almost instantly for real-life data/problems)</li> <li>Results depend on the selection of binarisation thresholds, dimensionality-reduction models and target counts of obtained latent features</li> <li>For exact modelling methods, the set of possible states doubles in size with every additional feature/node/brain region</li> <li>PDA assumes \(h\approx 0\) and an SK‑like (Sherrington-Kirkpatrick) parametrisation</li> <li>ELA explores single‑spin‑flip pathways, which is a limited and simplified assumption</li> </ul> <p>To counteract the influence of initial choice of (hyper)parameters, we quantify uncertainty in the workflow, track convergence wherever applicable, offer truly data-driven and bias-free optimisation of pipeline parameters, and expose diagnostics so these choices remain transparent and testable.</p> <hr/> <h2 id="motivation-and-contribution">Motivation and contribution</h2> <p>Comparing whole-brain dynamics across individuals is hard without a common reference that preserves interpretability and quantifies uncertainty. This challenge becomes even more apparent in studies of complex brain processes spanning cognition, sensory integration, and perception; when the data are limited; or when comparing brain dynamics driven by systematically different sub-types of various neurodevelopmental, psychiatric, or neurodegenerative conditions. Aiming to address these challenges and reflecting the UniReps community’s interest in representational alignment and comparability across subjects, datasets, and models, this post demonstrates:</p> <ul> <li>a subject-alignment front-end that produces shared latent representations with stable semantics across a population, offering several alternative approaches</li> <li>a stitched, physics-grounded back-end (PMEM to ELA to PDA) that yields mechanistically interpretable descriptors and shared phase-diagram coordinates derived with our original methodology</li> <li>a robustness-first toolkit that includes custom-built consensus alignment, automated parameter selection, uncertainty quantification, diagnostics, and review-ready artefacts</li> </ul> <hr/> <h2 id="pipeline-overview">Pipeline overview</h2> <ol> <li>Preprocess and engineer: ELA-secure detrending, conservative handling of missing data, safe within-subject run concatenation, per-region standardisation.</li> <li>Population-aware alignment and dimensionality reduction: shared latents via SRM, MCCA, or Group PCA or Group ICA with consensus; automatic dimensionality selection.</li> <li>Binarisation: per-latent threshold (usually median or mean, unless domain-expertise justifies, e.g., percentile-based thresholding), yielding +/-1 time series.</li> <li>PMEM or Ising fitting: exact (small N), safeguarded pseudo-likelihood, or variational Bayes - each enriched with its adequate set of solution-stability and significance/quality assessments.</li> <li>Energy-landscape analysis: attractors, barriers, disconnectivity graph, occupancies, kinetics, and many more descriptors providing mechanistic, biologically meaningful insight into brain dynamics, as well as facilitating direct and intuitive comparisons between subjects/cohorts.</li> <li>Phase-diagram analysis: multi-observable placement on a shared reference surface with our custom cost function, reports on confidence intervals, and near-criticality indices.</li> </ol> <p><strong>Data summary</strong> (used for the development and testing of the pipeline; details not central to current discussion in themselves):</p> <p>Resting-state fUS from N=8 mice (7 Cre-lox ASD models spanning 4 subtypes; 1 control with no symptomatic manifestation modelled); 54 bilateral regions (27 L/R pairs; whole-brain collection) - unified across all the subjects; two runs per mouse (1494 frames for each recording session; TR ≈ 0.6 s); runs concatenated per subject.</p> <style>figure.plotblock{clear:both;display:block;margin:2rem 0 3rem}figure.plotblock .holder{width:100%;max-width:1100px;margin:0 auto;height:clamp(520px,72vh,900px);overflow:hidden}figure.plotblock figcaption{margin-top:.75rem;text-align:center;font-size:.95rem;color:var(--theme-text,#eaeaea)}</style> <script src="https://cdn.plot.ly/plotly-2.32.0.min.js"></script> <figure class="plotblock l-page"> <div id="louvain-3d-fig" class="holder" aria-label="3D Louvain graph of brain regions"></div> <figcaption> <strong>3D Louvain graph of brain regions.</strong> Nodes are regions; node colour shows Louvain communities, edges reflect above-threshold similarity used for the graph. Hover reveals region name, degree, and cluster; drag to rotate, scroll to zoom, right-click drag to pan. </figcaption> </figure> <script>(async()=>{const o=document.getElementById("louvain-3d-fig");try{const e=await fetch("/blog/assets/plotly/2025-10-25-phase/3D-louvain.json"),a=await e.json();Plotly.newPlot(o,a.data||a,a.layout||{},Object.assign({responsive:!0,displaylogo:!1},a.config||{})),window.addEventListener("resize",()=>Plotly.Plots.resize(o))}catch(t){console.error("Plot load error: Louvain 3D",t),o.textContent="Interactive figure failed to load."}})();</script> <hr/> <h2 id="1-ela-secure-preprocessing-brief-overview">1) ELA-secure preprocessing (brief overview)</h2> <p>Aim: remove spikes, outliers, and slow global drifts while <strong>preserving the on/off switching structure</strong> that drives binarisation, PMEM fitting, and ELA/PDA. The procedure is modality-agnostic, non-invasive, and parameterised to be reproducible.</p> <ul> <li>Adaptive per-region parameters are computed from simple statistics and Welch spectra, then re-adapted after each step if requested (robust mode). <d-cite key="welch1967psd"></d-cite></li> <li>Despiking uses derivative-based detection with local, percentile-scaled replacements in short contextual windows; consecutive spikes are handled as blocks.</li> <li>Outlier removal is IQR-based with the same local, percentile-scaled replacement; an optional second pass is available.</li> <li>Population-aware detrending uses a cohort-optimised LOESS trend (fraction selected by stationarity and autocorrelation reduction)<d-cite key="cleveland1979lowess"></d-cite>. The global trend is estimated on the mean signal and scaled per region, which corrects drift without flattening transitions.</li> <li>Optional steps: light smoothing, bandpass filtering, breaking long flat runs, temporal standardisation, and spatial normalisation.</li> <li>Outputs include per-step amplitude deltas and residual checks; we also report the concordance between pre- and post-detrending binary states to ensure switching patterns are retained.</li> </ul> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20111942.png" alt="Five-step preprocessing pipeline panels for one region"/> <figcaption style="color:#f5f5f5;background:rgba(45,45,0,.1);padding:.6rem .8rem;border-radius:8px;"> Five-step preprocessing for a representative region (Striatum dorsal, R): Original → after despiking (derivative-based detection with local replacement) → after outlier removal (IQR with local context) → after universal LOESS detrending (global trend removed, transitions preserved) → after spatial normalisation to [0, 1]. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20111451.png" alt="Bars showing global linear-trend strength and magnitude across recordings"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> Cohort-wide evidence for a global trend. Top: absolute linear-trend correlation \(|r|\) for each recording with a decision threshold (red dashed line). Bottom: relative trend magnitude with its threshold. Many runs exceed both criteria, motivating a universal detrending step. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20111514.png" alt="Global signal with linear, quadratic and LOESS trends; residuals comparison"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> Model selection for detrending on an exemplar run. Top: global signal with linear, quadratic, and LOESS fits (LOESS selected). Bottom: residuals after each method. LOESS yields the most stationary, least autocorrelated residuals, hence chosen for the universal detrending step. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20111548.png" alt="Example regions: raw vs detrended signals and binary-state rasters"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> Region-wise example showing that global detrending preserves the on/off switching used downstream. Left: raw vs detrended signals for dentate gyrus (top) and thalamus (bottom). Right: binary rasters before/after median thresholding; “Concordance” in the panel titles reports the fraction of timepoints whose binary state is unchanged by detrending. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20111524.png" alt="Table of global-trend metrics and post-detrending stationarity flags for all recordings"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> Audit table across all recordings: linear-trend correlation and \(p\)-value, relative trend magnitude, and stationarity of residuals after linear, quadratic, and LOESS detrending. Most datasets achieve stationarity only with LOESS, supporting the universal-detrending choice. </figcaption> </figure> <details><summary>Click to expand: Practical notes</summary> <ul> <li>Replacement never injects artificial structure: values are drawn from local percentiles and clamped to local ranges; neighbors can be skipped to avoid bleeding flagged samples.</li> <li>Block handling groups consecutive indices to avoid fragmentation; a de-blocking fix prevents long identical segments after replacements.</li> <li>Universal LOESS fraction is chosen across the cohort to balance residual stationarity, autocorrelation reduction, and variance explained; region-wise application only scales that global trend.</li> <li>All steps are seedable and config-driven; logs capture chosen parameters, max amplitude changes, and QC metrics for auditability.</li> </ul> <h4 id="remark">Remark:</h4> <p>For ELA, methods must enhance quality without compromising temporal structure. Safe, beneficial steps:</p> <ul> <li>Despiking: generally beneficial; percentile/MAD-based, no hardcoded amplitude thresholds.</li> <li>Outlier removal: single or repeated; statistics-driven safeguards prevent over-removal.</li> <li>Detrending: crucial for our data; prefer the universal LOESS approach for cross-subject/session/region comparability. Region-wise detrending should be used only with domain context; the cohort-wide analysis supports global detrending, with expert review welcomed.</li> <li>Flat-blocks breaking: apply if replacements create long identical runs.</li> <li>Smoothing: only very mild, primarily cosmetic; avoid aggressive windows that could distort binarisation.</li> </ul> <p>Bandpass filtering can be performed if not already applied to the data - many recording devices perform it automatically. Temporal standardisation and spatial normalisation are not required for ELA itself but are retained for general use; spatial normalisation is applied for the imputation pipeline to align signs and amplitudes. LOESS may shift some series below zero; this is expected and accounted for. All parameterisations are chosen to remain strictly ELA-compatible.</p> </details> <h3 id="preprocessing-supplement-i---biologically-plausible-imputation-of-time-series-for-entire-missing-brain-regions">Preprocessing Supplement I - biologically plausible imputation of time series for entire missing brain regions</h3> <details><summary>Click to expand: Imputation</summary> <h3 id="brain-region-imputation-why-this-sub-pipeline-works">Brain-region imputation: why this sub-pipeline works</h3> <p>This sub-pipeline fills in missing regional time series in whole-brain power-Doppler recordings while <strong>preserving temporal structure</strong> and providing <strong>auditable quality checks</strong>. It offers three complementary strategies and a common validation suite:</p> <h3 id="what-the-pipeline-does-high-level-summary">What the pipeline does (high-level summary)</h3> <ul> <li> <p><strong>Detect &amp; canonise</strong> Finds mice/runs with missing regions and re-indexes to a canonical (reference-mouse) ordering so every region has a stable label and posiion.</p> </li> <li> <p><strong>Quantify bilateral coupling</strong> For each left/right pair it computes change-based correlation, raw correlation, directional agreement, magnitude coupling, lagged cross-correlation (with optimal lag), and frequency-domain coherence. These metrics tell us whether a contralateral trace is a <strong>plausible shape donor</strong> and provide thresholds for safe use.</p> </li> <li> <p><strong>Offer three imputation routes</strong></p> <ol> <li> <p><strong>Bilateral (shape-copy with lag alignment):</strong> Mirrors the contralateral region after shifting by a <strong>median optimal lag</strong> estimated from reference mice. It <strong>does not scale amplitudes</strong> (we work in normalised units), preserving the on/off <strong>shape</strong> that matters for downstream binarisation / PMEM / ELA/PDA. Optional light noise can be added in a non-deterministic mode.</p> </li> <li> <p><strong>Temporal (population median):</strong> Builds the missing series from the <strong>median temporal pattern</strong> of the same region across other mice (optionally across both runs). This is robust to outliers and yields stable reconstructions; with MAD-based jitter it can reflect natural variability while staying anchored to the cohort’s typical dynamics.</p> </li> <li> <p><strong>Clustering / nearest-neighbours:</strong> Groups reference mice/runs for the same region using correlation or Euclidean distance and imputes from the <strong>cluster mean</strong> of the nearest group. This conditions the reconstruction on <strong>matched dynamics</strong>, often outperforming global medians when cohorts are heterogeneous. A 3-D PCA visualisation makes the neighbourhoods and the imputed point <strong>inspectable</strong>.</p> </li> </ol> </li> <li> <p><strong>Validate, don’t guess</strong> Every imputed series is compared to the population using:</p> <ul> <li>mean absolute deviation to the cross-mouse mean,</li> <li>correlation with that mean,</li> <li><strong>coverage</strong> within 1–2 s.d.,</li> <li>change-correlation, peak cross-correlation and optimal lag,</li> <li>mean coherence and peak-coherence frequency,</li> <li><strong>power-spectrum correlation</strong>. These metrics are printed and plotted, so each imputation carries its own <strong>provenance and QC</strong>.</li> </ul> </li> </ul> <h3 id="why-this-is-useful-for-downstream-physics-style-analyses">Why this is useful for downstream physics-style analyses</h3> <ul> <li><strong>Shape-faithful</strong>: methods preserve <strong>temporal switching</strong> structure (crucial for binarisation → PMEM → ELA/PDA).</li> <li><strong>Cohort-aware</strong>: temporal and clustering routes borrow information only from the <strong>same labelled region</strong> across other mice/runs.</li> <li><strong>Bilateral advantage</strong>: when hemispheric symmetry is strong, lag-aligned mirroring recovers realistic trajectories with minimal bias.</li> <li><strong>Transparent &amp; reproducible</strong>: seeds are fixed; thresholds are explicit; NaNs and edge cases are handled defensively; outputs are re-indexed to a canonical order.</li> <li><strong>Method flexibility</strong>: you can pick the route that best matches your biology (e.g., bilateral for symmetric circuits; clustering for diverse cohorts) and still get the <strong>same validation bundle</strong>.</li> </ul> <hr/> <h3 id="figures">Figures</h3> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20150710.png" alt="Imputed Postrhinal area (R) time series overlaid with reference mice and population mean; bilateral change-scatter with metrics"/> <figcaption style="color:var(--theme-text,#eaeaea)"> <strong>Imputation outcome and bilateral synchrony (Postrhinal area, R; Run&nbsp;1).</strong> Top: the imputed trace (red) is plotted against reference mice (light blue) and the population mean (green); the y-axis is the normalised power-Doppler signal. Bottom: bilateral <em>change</em> scatter (left minus right first differences) with the diagonal “perfect synchronisation” guide, a fitted trend line, and summary metrics (change-correlation, peak cross-correlation with its optimal lag, mean coherence). Together these panels show that the imputed series follows cohort-typical dynamics and remains temporally consistent with its contralateral partner. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20150758.png" alt="Cross-correlation vs lag and magnitude-squared coherence for the imputed region across two runs"/> <figcaption style="color:var(--theme-text,#eaeaea)"> <strong>Lag-structure and frequency-locking (Postrhinal area, R; Runs&nbsp;1–2).</strong> For each run, the left panel shows cross-correlation across lags (dashed zero line for reference), and the right panel shows magnitude-squared coherence versus frequency. Peaks indicate preferred temporal offsets and shared frequency content; stable peaks across runs support consistent reconstruction rather than overfit noise. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20150815.png" alt="Console-style validation summary listing deviation from population mean, coverage within 1–2 SD, temporal and spectral metrics"/> <figcaption style="color:var(--theme-text,#eaeaea)"> <strong>Validation summary.</strong> For each run the pipeline reports deviation from the population mean, correlation with the mean, coverage within 1–2 standard deviations, change-correlation, peak cross-correlation and optimal lag, mean coherence, and power-spectrum correlation. These values provide an audit trail for every imputation. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20150835.png" alt="Console log showing missing region detection, reference coupling metrics, and selected imputation method for each run"/> <figcaption style="color:var(--theme-text,#eaeaea)"> <strong>Pipeline provenance.</strong> The log records which regions are missing, cohort-level bilateral coupling benchmarks used as context, and the selected imputation route (here, clustering-based). This makes method choice and inputs explicit for later review. </figcaption> </figure> <figure class="l-page"> <div id="plot-3d-impu" style="position:relative;width:100%;max-width:980px;height:0;padding-bottom:85%;"> </div> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Nearest-neighbours imputation — 3D PCA view</strong> </figcaption> </figure> <script src="https://cdn.plot.ly/plotly-2.32.0.min.js"></script> <script>!async function(){const t=document.getElementById("plot-3d-impu"),o="/blog/assets/plotly/2025-10-25-phase/3D-impu2.json";try{const a=await fetch(o),n=await a.json(),l=n.data||n,s=n.layout||{},c=Object.assign({responsive:!0,displaylogo:!1},n.config||{});Plotly.newPlot(t,l,s,c)}catch(e){console.error("Plot load error:",e),t.textContent="Interactive figure failed to load."}}();</script> </details> <h3 id="preprocessing-supplement-ii--similarity-assessment-and-conditional-concatenation-of-recordings-from-different-sessions-for-each-mouse">Preprocessing Supplement II — similarity assessment and conditional concatenation of recordings from different sessions for each mouse</h3> <details><summary>Click to expand: Concatenation</summary> <h3 id="concatenating-two-recording-runs-per-mouse">Concatenating two recording runs per mouse</h3> <p>This section documents the methodology we use to <strong>decide whether two runs are similar enough to be concatenated</strong>, how we <strong>align</strong> them when needed, and what diagnostics we plot to make the process auditable.</p> <h3 id="what-similar-enough-to-concatenate-means">What “similar enough to concatenate” means</h3> <p>Before concatenation, we compare the two runs <strong>region-by-region</strong> and turn the result into a single pass/fail decision.</p> <h3 id="1-regionwise-similarity-time-domain-patterns">1) Regionwise similarity (time-domain patterns)</h3> <p>For each region \((r)\) present in both runs, we check a scalar <strong>similarity</strong> between its run-1 series \(\mathbf{x}_r\) and run-2 series \(\mathbf{y}_r\). Several options are available:</p> <ul> <li><strong>Pearson correlation</strong>: requires \(\operatorname{corr}\big(\mathbf{x}_r,\mathbf{y}_r\big)\ \ge\ \tau_{\mathrm{corr}}.\)</li> <li><strong>Spearman</strong> (rank correlation): compute \(\rho\in[-1,1],\) map to \([0,1]\) by \(\rho_{01}=\tfrac{1}{2}(\rho+1),\) and require \(\rho_{01}\ \ge\ \tau_{\mathrm{spearman}}.\) <em>(Good for monotone but non-linear similarity)</em></li> <li><strong>Kendall’s \(\tau\)</strong>: same mapping \(\tau_{01}=\tfrac{1}{2}(\tau+1),\) require \(\tau_{01}\ \ge\ \tau_{\mathrm{kendall}}.\)</li> <li><strong>Euclidean (shape) distance</strong>: min–max normalise both series to \([0,1]\), compute \(d=\frac{\lVert \mathbf{x}_r-\mathbf{y}_r\rVert_2}{\sqrt{T}},\) and require \(d\ \le\ \tau_{\mathrm{dist}}.\) <em>(Insensitive to absolute scale; tests waveform similarity)</em></li> <li><strong>Cosine similarity</strong>: map \(\cos\theta\in[-1,1]\) to \([0,1]\) via \(c_{01}=\tfrac{1}{2}(\cos\theta+1),\) and require \(c_{01}\ \ge\ \tau_{\mathrm{cos}}.\)</li> </ul> <p>Each region yields a boolean pass/fail; the resulting vector is our <strong>regionwise mask</strong>.</p> <h3 id="2-optional-distribution-similarity">2) (Optional) Distribution similarity</h3> <p>As a distributional check, we run a <strong>two-sample Kolmogorov–Smirnov test</strong> per region and declare a pass when the \(p-value\) exceeds a threshold, i.e. the two marginal distributions are not detectably different at the chosen level.</p> <h3 id="3-aggregating-to-a-single-gate">3) Aggregating to a single gate</h3> <p>We fuse regionwise pass/fail results into a final score using one of:</p> <ul> <li><strong>fraction</strong> (default): share of regions that pass; concatenate if \(\mathrm{fraction}\ \ge\ \tau_{\mathrm{agg}}.\)</li> <li><strong>weighted</strong>: same as above but weights each region by amplitude or variance.</li> <li><strong>median / any / all</strong>: robust/lenient/strict alternates.</li> </ul> <p>The gate is reported as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[check_run_similarity] aggregator='&lt;mode&gt;', final_score=&lt;value&gt;, pass=&lt;True|False&gt;
</code></pre></div></div> <p>Only if it <strong>passes</strong>, or if <code class="language-plaintext highlighter-rouge">force_concatenate=True</code>, do we proceed.</p> <h3 id="alignment-making-run-2-comparable-to-run-1">Alignment: making run-2 comparable to run-1</h3> <p>If concatenation proceeds, we align <strong>levels</strong> of run-2 to run-1 <strong>per region</strong> (no temporal warping):</p> <ul> <li><code class="language-plaintext highlighter-rouge">alignment_mode='match_medians'</code> (default): for each region \((r)\), \(\mathbf{y}_r \leftarrow \mathbf{y}_r + \big(\operatorname{median}(\mathbf{x}_r)-\operatorname{median}(\mathbf{y}_r)\big)\)</li> <li>Alternatively, <code class="language-plaintext highlighter-rouge">match_means</code> uses the mean instead of the median.</li> </ul> <p><strong>Why this is safe:</strong> the ELA/PDA pipeline is driven by <strong>on/off switching and relative changes</strong>. Shifting a time series by a constant to match central tendency <strong>does not</strong> distort the temporal structure we use downstream.</p> <h3 id="preprocessing-before-the-gate-optional-but-recommended">Preprocessing before the gate (optional but recommended)</h3> <p>Both runs can first pass through a light, ELA-secure preprocessing stack (despiking, robust outlier handling, LOESS detrending, mild smoothing). Parameters are seedable and adapt across regions. This improves the reliability of similarity estimates without changing the switching dynamics that matter later.</p> <h3 id="concatenation-step">Concatenation step</h3> <p>After alignment, we intersect region indices and <strong>horizontally concatenate</strong> the two runs (time dimension doubles). An optional last <strong>outlier smoothing</strong> can be applied to the concatenated trace using a robust IQR rule.</p> <h3 id="diagnostics-and-what-the-plots-show">Diagnostics and what the plots show</h3> <p>The helper <code class="language-plaintext highlighter-rouge">show_intermediate_concat_plots(...)</code> produces a <strong>3×2</strong> panel (if preprocessing is enabled):</p> <ul> <li><strong>Row 1:</strong> Run-1 RAW (left) and Run-2 RAW (right). Orange dashed line = mean; bright-green dashed line = median.</li> <li><strong>Row 2:</strong> Run-1 Preprocessed (left) and Run-2 Preprocessed (right) with the same guides.</li> <li><strong>Row 3:</strong> <strong>Aligned Run-2</strong> (left; grey dashed = pre-alignment, green = aligned) and <strong>Final Concatenated</strong> (right; with mean/median guides).</li> </ul> <p>A shared legend clarifies the mean/median guides. These views make the gating, alignment, and final result fully inspectable.</p> <hr/> <h3 id="figures">Figures</h3> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20154328.png" alt="Console readout of regionwise similarity gate for multiple mice"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Run-similarity gate (console readout).</strong> For each mouse, we compute a per-region similarity mask and aggregate it to a single decision. The line shows the chosen aggregator, the final score, and pass/fail. Only runs that pass are aligned and concatenated; failing pairs are skipped to avoid mixing incompatible sessions (although similarity checks can be manually overridden and the user could force concatenation anyway). </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20154306.png" alt="Six-panel plot illustrating raw runs, preprocessed runs, alignment of run 2 to run 1, and final concatenation with mean/median guides"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Concatenation diagnostics for a representative region.</strong> <em>Top row:</em> raw run-1 (left, blue) and raw run-2 (right, red) with orange mean and bright-green median lines. <em>Middle row:</em> the same region after preprocessing. <em>Bottom-left:</em> alignment step—run-2 before (grey dashed) and after (green) median-matching to run-1. <em>Bottom-right:</em> final concatenated trace with mean/median guides. These panels document that the two runs are comparable, that level alignment has worked as intended, and that the final time series is suitable for downstream analyses. </figcaption> </figure> <hr/> <h2 id="why-this-approach-is-robust-and-useful">Why this approach is robust and useful</h2> <ul> <li><strong>Prevents spurious mixing.</strong> The gate stops concatenation when the two runs <strong>do not</strong> tell a consistent story for most regions. This protects subsequent ELA/PDA stages from artefactual discontinuities.</li> <li><strong>Flexible similarity notions.</strong> You can choose correlation-based (linear or rank), cosine (directional), or Euclidean-shape metrics, depending on whether absolute scale or monotonicity matters most for your data The rank-based options (Spearman/Kendall) are especially <strong>stable</strong> for neural time series, where amplitude changes can be non-linear or non-stationary.</li> <li><strong>Scale-safe alignment.</strong> Median/mean level-matching fixes global offsets without altering temporal structure, keeping <strong>on/off</strong> switching intact—the key for binarisation and PMEM fitting.</li> <li><strong>Transparent diagnostics.</strong> The console summary and 3×2 figure make each decision inspectable. If a pair fails, you immediately see why; if it passes, you can verify that alignment has not introduced distortions.</li> <li><strong>Configurable strictness.</strong> Thresholds \((\tau_{\mathrm{corr}},\ \tau_{\mathrm{spearman}},\ \tau_{\mathrm{dist}},\ \dots,\ \tau_{\mathrm{agg}})\) and the aggregation rule control how strict the gate is. You can be conservative for clinical datasets and more permissive for exploratory work.</li> </ul> <hr/> <h3 id="minimal-recipe-what-the-functions-do">Minimal recipe (what the functions do)</h3> <ol> <li><strong>Preprocess</strong> each run (optional, recommended).</li> <li><strong>Compute regionwise similarity</strong> with your chosen method.</li> <li><strong>Aggregate</strong> passes into a final gate score; stop if it fails.</li> <li><strong>Align levels</strong> (<code class="language-plaintext highlighter-rouge">match_medians</code>/<code class="language-plaintext highlighter-rouge">match_means</code>) per region.</li> <li><strong>Concatenate</strong> the two runs (time axis).</li> <li><strong>(Optional) Final outlier pass</strong> on the concatenated series.</li> <li><strong>Plot diagnostics</strong> for audit and reports.</li> </ol> <p>This procedure is <strong>seeded, reproducible, and auditable</strong>, and it keeps exactly the aspects of the signal that matter for your subsequent ELA/PDA pipeline.</p> </details> <hr/> <h2 id="2-population-universal-shared-latent-space">2) Population-universal shared latent space</h2> <p>Goal: obtain shared, ELA-secure latent components whose semantics are stable across subjects and runs, so downstream binarisation → PMEM → ELA/PDA is directly comparable and computationally tractable.</p> <h4 id="novelty-factor-at-a-glance">Novelty factor at a glance:</h4> <ul> <li>Pick-and-mix reductions: (Group PCA, Group ICA, SRM, MCCA) with robust tweaks (varimax, multi-restart ICA, PCA-initialised SRM, whitened MCCA).</li> <li>A population-aware, multi-metric objective (structural + temporal + method-specific) → auto-selection of dimensionality and hyperparameters.</li> <li>Alignment toolkit (orthogonal Procrustes with column-wise sign fixes, Hungarian matching, and a neuro-Procrustes consensus).</li> <li>Synergy-weighted consensus across methods with dominance checks, stability (RV) analysis, and per-component contribution audits <d-cite key="robert1976rv"></d-cite>.</li> <li>Efficient, reproducible compute (SVD on averaged covariances, fast downsampling for temporal metrics, parallel grid search, seeded randomness).</li> </ul> <hr/> <h3 id="21-methods-population-aware-ela-secure">2.1 Methods (population-aware, ELA-secure)</h3> <p>All methods preserve temporal ordering (only linear projections or orthogonal transforms over channels), and we quantify temporal faithfulness (trustworthiness/continuity, autocorrelation preservation) <d-cite key="venna2006localmds"></d-cite>.</p> <ul> <li> <p>Group PCA (variance capture + varimax): eigenvectors of the average subject covariance; optional varimax keeps components sparse/interpretable without breaking orthogonality <d-cite key="SMITH2014738,kaiser1958varimax"></d-cite>. <em>Extras:</em> elbow detection, subject-wise EVR, reconstruction error, and post-rotation low-variance pruning.</p> </li> <li> <p>Group ICA (shared independent subspaces): subject-wise PCA → concatenation → FastICA <d-cite key="hyvarinen1999fastica"></d-cite> with multi-restart selection (best negentropy proxy via kurtosis deviation), independence verification (mean \(\lvert\mathrm{corr}\rvert\), kNN mutual information <d-cite key="kraskov2004mi"></d-cite>, squared-corr checks), sign harmonisation of subject contributions before averaging.</p> </li> <li> <p>SRM (shared timecourses with subject-specific maps): iterative orthogonal subject mappings \((W_i)\) and shared response \((S)\), PCA-based initialisation, Hungarian-matched alignment of mappings across subjects, orthogonality diagnostics, shared variance quantification <d-cite key="NIPS2015_b3967a0e"></d-cite>.</p> </li> <li> <p>MCCA (maximising cross-subject correlation): per-subject whitening → SVD on concatenated features (SUMCOR-style) → iterative refinement of subject loadings \((a_i)\) <d-cite key="60b14841-3c7e-3751-88f2-15308f78bf55,decheveigne2019mcca,correa2010mcca"></d-cite>. We report cross-subject alignment, canonical correlations to shared response, orthogonality in whitened space, and shared variance in native space.</p> </li> </ul> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20180713.png" alt="Group PCA scree: individual and cumulative explained variance with elbow and totals"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Group PCA scree.</strong> Light-blue bars show individual explained-variance ratio (EVR) per component; the black line is cumulative EVR. The dotted vertical line marks the elbow (here at the 1st component), and the title reports total variance explained (65.6%). This plot guides the initial range for dimensionality selection before our multi-metric model choice. </figcaption> </figure> <figure class="l-page"> <div style="display:flex; gap:1rem; flex-wrap:wrap;"> <img style="flex:1 1 360px; max-width:49%;" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181009.png" alt="SRM consensus mapping W: region-by-component loadings heatmap"/> <img style="flex:1 1 360px; max-width:49%;" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181039.png" alt="SRM example subject: reduced correlation between latent components"/> </div> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>SRM latent space.</strong> <em>Left:</em> consensus SRM mapping \(W\) (regions × components). Colours indicate signed loadings (arbitrary overall sign but harmonised across subjects), highlighting stable spatial patterns shared across the cohort. <em>Right:</em> example subject’s correlation matrix between SRM latent time series. Light (near-white) off-diagonals indicate low cross-component correlation, i.e. little superfluous overlap—evidence of efficient representation and dimensionality reduction. </figcaption> </figure> <figure class="l-page"> <div style="display:flex; gap:1rem; flex-wrap:wrap;"> <img style="flex:1 1 360px; max-width:49%;" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181248.png" alt="MCCA example subject: reduced correlation between latent components"/> <img style="flex:1 1 360px; max-width:49%;" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181218.png" alt="MCCA subject projection matrix a: region-by-component loadings"/> </div> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>MCCA latent space.</strong> <em>Left:</em> example subject’s correlation matrix between MCCA components—again, light off-diagonals reflect low redundancy across latents. <em>Right:</em> subject-specific MCCA projection matrix \(a\) (regions × components), showing how each brain region contributes to each shared component; structured bands point to interpretable, population-aligned patterns. </figcaption> </figure> <blockquote> <p>Why four methods? They trade off interpretability, independence, correlation sharing, and variance capture. We score them with the same unified, ELA-aware metric suite and fuse them, yielding a stable, population-universal latent space.</p> </blockquote> <hr/> <h3 id="22-alignment--sign-consistency-critical-for-comparability">2.2 Alignment &amp; sign consistency (critical for comparability)</h3> <ul> <li>Orthogonal Procrustes with column-wise sign checks (flip a component if it anticorrelates with the reference) <d-cite key="schonemann1966procrustes"></d-cite>.</li> <li>Hungarian matching aligns component order across methods/subjects by maximal absolute correlations <d-cite key="kuhn1955hungarian"></d-cite>.</li> <li>Neuro-Procrustes consensus: iterative, order-robust alignment across methods (SVD-based reference) with final sign harmonisation.</li> <li>Optional biological sign protocol (baseline-anchored flips) to stabilise polarities across datasets.</li> </ul> <hr/> <h3 id="23-metrics--selection-multi-objective-normalised-to-01">2.3 Metrics &amp; selection (multi-objective, normalised to [0,1])</h3> <p>Structural fidelity (RSA-style): correlation between original region-by-region correlation structure and that reconstructed from latents; sign preservation weighted by \(\lvert\mathrm{corr}\rvert\); Procrustes disparity; Kruskal stress <d-cite key="kruskal1964nmds1"></d-cite> on inter-region distances.</p> <p>Temporal faithfulness: trustworthiness/continuity of neighbour relations, temporal neighbourhood hits, autocorrelation preservation at task-relevant lags.</p> <p>Method-specific:</p> <ul> <li>PCA: mean subject EVR, reconstruction error.</li> <li>ICA: independence (mean \(\lvert\mathrm{corr}\rvert\)↓, Mutual Information (MI)↓), kurtosis (≠3), sparsity.</li> <li>SRM: shared alignment (Hungarian-matched), orthogonality, shared variance.</li> <li>MCCA: cross-subject alignment, shared variance, canonical correlations, orthogonality (whitened).</li> </ul> <p>Normalisation uses principled maps (e.g., \(x \mapsto \tfrac{1}{1+x}\) for “smaller-is-better”, linear \([-1,1]\to[0,1]\) for correlations). Composite score: weighted average (user- or default weights), then auto-optimise \((k)\) and hyperparameters via seeded, parallel grid search.</p> <details><summary>Click to expand: metric formulas</summary> <p>Kruskal stress (upper-triangle, variance-matched):</p> \[\mathrm{Stress}_1 =\min_{b&gt;0}\sqrt{\frac{\sum_{i&lt;j}\big(d_{ij}-b\,\hat d_{ij}\big)^2} {\sum_{i&lt;j} d_{ij}^2}},\qquad b^\star=\frac{\sum_{i&lt;j} d_{ij}\,\hat d_{ij}}{\sum_{i&lt;j} \hat d_{ij}^2}\] <p>Procrustes (disparity surrogate):</p> \[R^\star=\arg\min_{R\in O(k)}||A-BR||_F,\qquad with \quad B^\top A = U\Sigma V^\top, \quad R^\star = UV^\top, \quad A,B\in\mathbb{R}^{n\times k}\] <p>RV stability (consensus vs method): \(\mathrm{RV}(A,B)=\frac{\operatorname{tr}(A^\top B)}{\sqrt{\operatorname{tr}(A^\top A)\operatorname{tr}(B^\top B)}} \quad\)</p> <p>(illustrative RV formulation for comparing matrices of the same size: recording sites X number of observations)</p> <p>Composite (normalised metrics \(\tilde m\in[0,1]\)): \(J=\frac{\sum_m w_m\tilde m}{\sum_m w_m}\)</p> </details> <hr/> <h3 id="24-consensus-across-methods-synergy-weighted-stability-checked">2.4 Consensus across methods (synergy-weighted, stability-checked)</h3> <p>We build a weighted consensus matrix after cross-method alignment:</p> <ol> <li>Align each method’s components (Hungarian or neuro-Procrustes).</li> <li> <p>Synergy weights per method: \(\tilde w_m \propto \big(\mathrm{composite}_{\mathrm{specific},m}\big)^{\alpha}\, \big(\mathrm{composite}_{\mathrm{common},m}\big)^{1-\alpha},\qquad w_m=\frac{\tilde w_m}{\sum_j \tilde w_j}\)</p> </li> <li>Weighted sum of aligned components → column L2 re-normalisation.</li> <li>Validation: RSA metrics, Procrustes disparity, variance explained, RV stability, reconstruction consistency with each method, and a dominance index (Gini/CV/TV-distance) to ensure no method overwhelms the fusion.</li> </ol> <details><summary>Click to expand: consensus diagnostics &amp; thresholds</summary> <ul> <li>Component alignment quality: mean inter-method \(\lvert\mathrm{corr}\rvert\) per component, with permutation and bootstrap thresholds, plus a data-driven clustering criterion for bimodal quality profiles.</li> <li>Subject consistency: mean cross-subject \(\lvert\mathrm{corr}\rvert\) of per-component timecourses, with phase-randomised surrogate thresholds (preserve autocorrelation).</li> <li>Dominance (balance of synergy weights): we report normalised Gini, CV, TVD, and min/max ratio (all mapped to [0,1]).</li> </ul> </details> <figure class="l-page"> <img loading="lazy" alt="Bar chart of cross-subject component consistency with statistical thresholds" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181523.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Cross-subject consistency of shared components.</strong> Each bar is the mean absolute correlation of the same component across subjects (higher = more reproducible). Dashed/dotted lines show permutation and 95% CI thresholds. Components above both guides generalise well across mice and are kept for the consensus latent space. </figcaption> </figure> <figure class="l-page"> <img loading="lazy" alt="Bar chart of cross-method alignment quality with thresholds" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181355.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Alignment quality across methods.</strong> Bars show how well each consensus component matches its counterparts from Group PCA, Group ICA, SRM and MCCA after Procrustes + Hungarian alignment (mean |corr|). Higher values and crossing the dashed/dotted thresholds indicate robust cross-method agreement, supporting the fused consensus basis. </figcaption> </figure> <figure class="l-page"> <img loading="lazy" alt="Stacked bars showing method contributions to each consensus component" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181438.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Method contributions to the consensus components.</strong> Stacked bars report the weighted influence of each method (Group PCA / Group ICA / SRM / MCCA) on every consensus component. Balanced contributions mean the final space is not dominated by a single method and retains structure that is consistent across approaches. </figcaption> </figure> <figure class="l-page"> <img loading="lazy" alt="Heatmap of Group PCA transformation matrix after alignment" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20183547.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Example aligned loadings: Group PCA.</strong> Brain regions × components matrix (unit-normalised) after cross-subject/method alignment. Warmer/colder cells mark stronger positive/negative loadings; light colours near zero indicate sparse, non-overlapping contributions—useful for efficient representation and reduced redundancy. </figcaption> </figure> <figure class="l-page"> <img loading="lazy" alt="Triangular heatmap of cross-method component correlations after alignment" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20183612.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Cross-method component correlations (post-alignment).</strong> Each block compares components across the four methods. A sharp diagonal with light off-diagonals shows one-to-one matches and little superfluous overlap, validating that different methods recover consistent latent directions. </figcaption> </figure> <figure class="l-page"> <img loading="lazy" alt="Heatmap of the final consensus transformation matrix" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-08%20083629.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Final consensus transformation.</strong> Regions × components loadings that define the population-universal latent space used for all subjects. The mapping is sign-harmonised and unit-normalised so that latents have consistent semantics across mice and runs. </figcaption> </figure> <figure class="l-page"> <img loading="lazy" alt="Correlation matrix of consensus component time series for one subject" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20183651.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Consensus latent space — within-subject orthogonality.</strong> Correlation matrix of consensus component time courses for an example mouse. Near-zero off-diagonals (light colours) indicate low redundancy between latents, which aids binarisation and stabilises the downstream PMEM/ELA/PDA steps. </figcaption> </figure> <hr/> <details><summary>Click to expand: Mathematical underpinnings</summary> <h3 id="25-mathematical-cores-rigorous-but-compact">2.5 Mathematical cores (rigorous but compact)</h3> <p><strong>SRM (orthogonal, correlation‑maximising form)</strong></p> <p>For subjects \(X_i \in \mathbb{R}^{T \times d}\), find \(W_i \in \mathbb{R}^{d \times k}\) with \(W_i^\top W_i = I\) and a shared response \(S \in \mathbb{R}^{T \times k}\):</p> \[\min_{\{W_i\},\, S}\; \sum_i \lVert X_i W_i - S \rVert_F^2 \;\Longleftrightarrow\; \max_{\{W_i\},\, S}\; \sum_i \operatorname{tr}\!\big(S^\top X_i W_i\big) \quad \text{s.t. } W_i^\top W_i = I .\] <p>Updates:</p> \[S \leftarrow \frac{1}{n} \sum_i X_i W_i ,\] <p>then z‑score \(S\) column‑wise.</p> \[W_i \leftarrow U V^\top, \qquad \text{where } U \Sigma V^\top = \operatorname{SVD}\!\big(X_i^\top S\big).\] <p>*The above formulation reflects our shared-T assumption - since all the subjects from the dataset used for developing the pipeline had the exact same number of frames in their time series.</p> <hr/> <p><strong>MCCA (SUMCOR‑style, whitened)</strong></p> <p>Let \(X_i\) be centred and whitened to \(\tilde{X}_i\). Find \(a_i \in \mathbb{R}^{d \times k}\) maximising total cross‑correlation:</p> \[\max_{\{a_i\}} \; \sum_{i&lt;j} \operatorname{tr}\!\big((\tilde{X}_i a_i)^\top (\tilde{X}_j a_j)\big) \quad \text{s.t. } a_i^\top a_i = I .\] <p>Solution via SVD on the concatenation:</p> \[\operatorname{SVD}\!\left(\big[\, \tilde{X}_1 \ \ \tilde{X}_2 \ \ \cdots \ \ \tilde{X}_n \,\big]\right),\] <p>then map back with subject‑specific unwhitening.</p> <hr/> <p><strong>Group PCA (population‑aware)</strong></p> <p>With subject covariances \(\Sigma_i \in \mathbb{R}^{d \times d}\), form the mean covariance \(\bar{\Sigma}=\frac{1}{n}\sum_{i=1}^{n}\Sigma_i .\)</p> <p>Eigen‑decompose the mean covariance \(\bar{\Sigma} = Q \Lambda Q^\top,\) where the columns of \(Q=[q_1,\dots,q_d]\) are orthonormal eigenvectors and \(\Lambda=\operatorname{diag}(\lambda_1\ge\dots\ge\lambda_d)\) .</p> <p>Select the top‑\(k\) eigenvectors \(E := [q_1\ \cdots\ q_k] \in \mathbb{R}^{d \times k}.\)</p> <p>(Optional) apply an orthogonal rotation \(R\in\mathbb{R}^{k\times k}\) (e.g., varimax) to improve sparsity/interpretability.</p> <p>The rotated loadings are \(L = ER, \text{ with } R^\top R = I.\)</p> <p>A standard varimax objective (orthomax with parameter \(\gamma\in[0,1]\)) is: \(R^\star = \arg\max_{R^\top R=I} \sum_{j=1}^k \left( \sum_{p=1}^d L_{pj}^4 - \frac{\gamma}{d}\left(\sum_{p=1}^d L_{pj}^2\right)^2 \right), \quad \text{for loadings } L = E R .\)</p> <p>Per‑subject time‑series data \(X_i \in \mathbb{R}^{T_i \times d}\) are projected to latent timecourses via:</p> \[Z_i = X_iL \in \mathbb{R}^{T_i \times k},\] <p>or, without rotation: \(Z_i = X_iE\).</p> <p><strong>Notes:</strong></p> <ul> <li>Using the average covariance \(\bar{\Sigma}\) ensures the components reflect population structure.</li> <li>Rotation is optional and keeps components orthogonal (since \(R\) is orthogonal). If we prefer unrotated principal components, use \(L=E\).</li> </ul> <hr/> <p><strong>Group ICA (robust)</strong></p> <p>Subject PCA → concatenation → FastICA. Restart with multiple seeds; select the run with the largest negentropy proxy (mean \(\lvert\mathrm{kurtosis} - 3\rvert\)), and verify independence (low mean \(\lvert\operatorname{corr}\rvert\), low kNN‑MI, low mean squared correlation).</p> </details> <hr/> <h3 id="26-practicalities-efficiency-reproducibility">2.6 Practicalities (efficiency, reproducibility)</h3> <ul> <li>Efficient linear algebra: SVD on averaged covariances (Group PCA), batched downsampling for temporal metrics, parallel grid search with seeded restarts.</li> <li>Diagnostics at each stage: subject EVR, reconstruction error, independence checks, alignment scores, consensus stability (RV), method dominance, and retention rationale for kept/dropped components.</li> <li>ELA-secure by design: all transformations are linear in space (no temporal warping), detrending was handled upstream, and temporal metrics explicitly guard the switching dynamics used by ELA.</li> </ul> <hr/> <h3 id="27-outputs-what-to-expect">2.7 Outputs (what to expect)</h3> <ul> <li>Method-specific models (components/loadings), aligned across methods.</li> <li>Consensus model (columns = population-universal latents), with weights and stability report.</li> <li>Per-subject projections (time × components) ready for binarisation → ELA/PDA.</li> <li>QC bundle: metric table, thresholded alignment &amp; consistency plots, dominance index, and retained-component justifications.</li> </ul> <hr/> <details><summary>Click to expand: Implementation highlights &amp; safeguards</summary> <ul> <li>RSA-style structure preservation reconstructs regions from components (per-region least squares) to compare correlation matrices before/after, reporting Pearson on vectorised upper triangles, Frobenius/mean diffs, and weighted sign preservation.</li> <li>Temporal metrics (trustworthiness/continuity, neighbourhood hit) use downsampled representations for speed without losing neighbourhood signal; autocorr preservation checked at task-relevant lags.</li> <li>Robust sign handling: column-wise correlation checks after Procrustes; optional baseline-anchored flipping (percentile-based) prevents biologically implausible polarity swaps.</li> <li>Dominance index offers four normalised variants (Gini/CV/TVD/min-max) to ensure balanced consensus.</li> <li>Permutation/phase-randomisation thresholds provide statistical guardrails for declaring alignment/consistency “good enough.”</li> </ul> </details> <hr/> <h2 id="3-binarisation-of-latent-time-series">3) Binarisation of latent time series</h2> <p>After alignment and dimensionality reduction, each latent \(x_i(t)\) is thresholded per-latent using median or mean and mapped to \(s_i(t) \in \{-1,+1\}\).</p> <p>This respects component-specific baselines, keeps PMEM tractable, and standardises inputs for inter-subject comparability.</p> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20183815.png" alt="Proportion of +1 (active) binary states across latents over time with linear trend, min and max markers"/> <figcaption style="color:rgba(255, 255, 255, 0.84);padding:.6rem .8rem;border-radius:8px;"> <strong>Binarisation sanity check: activity fraction over time.</strong> The blue trace shows, at each time step, the proportion of latents in the +1 state after per-latent median thresholding. The grey dashed line marks 0.5; orange dotted lines show the observed range (min ≈ 0.20, max ≈ 0.80). The red dashed line is the fitted linear trend (+0.052 percentage points per 100 steps; total change ≈ 1.57 pp). Near-stationary behaviour centred around 0.5 indicates balanced on/off usage and supports downstream PMEM fitting; large drifts would flag thresholding issues or residual global trends. </figcaption> </figure> <hr/> <h2 id="4-pairwise-maximum-entropy-ising-fitting">4) Pairwise maximum-entropy (Ising) fitting</h2> <p>We model each time‑point’s binary latent vector \(\mathbf{s}\in\{-1,+1\}^N\) with the pairwise maximum‑entropy (PMEM/Ising) distribution: \(P(\mathbf{s}) \propto \exp\Big(\sum_{i} h_i s_i + \sum_{i&lt;j} J_{ij} s_i s_j\Big), \qquad E(\mathbf{s}) = -\sum_{i} h_i s_i - \tfrac12 \sum_{i\ne j} J_{ij} s_i s_j\)</p> <p>PMEM matches the empirical first and second moments with minimal assumptions while remaining expressive for mesoscopic neural populations (as well as on the scale of entire networks of brain regions, and extending naturally to dynamical systems beyond neuroscience) <d-cite key="jaynes1957maxent,schneidman2006nature"></d-cite>.</p> <h3 id="inference-routes-complementary-scaleaware">Inference routes (complementary, scale‑aware):</h3> <ul> <li><strong>Exact (small \(N\))</strong>: Enumerate all \(2^N\) states to obtain the exact log‑likelihood and moments for gold‑standard checks</li> <li> <p><strong>Pseudo‑likelihood (PL)</strong>: Optimise the sum of node‑wise logistic conditionals with L2 penalties and a safeguarded Armijo line‑search; enforce symmetry \(J_{ij}=J_{ji}, J_{ii}=0\) and use a relative‑norm stopping rule (scale‑free, robust).</p> <p><strong>Local field:</strong></p> \[f_i^{(t)} = h_i + \sum_{j\neq i} J_{ij}s_j^{(t)} .\] <p><strong>Node-wise conditional and log-conditional:</strong></p> \[p\left(s_i^{(t)}\mid \mathbf s_{\setminus i}^{(t)}\right) = \frac{\exp\big(s_i^{(t)} f_i^{(t)}\big)}{2\cosh f_i^{(t)}}, \qquad \log p\left(s_i^{(t)}\mid \mathbf s_{\setminus i}^{(t)}\right) = s_i^{(t)} f_i^{(t)}-\log\big(2\cosh f_i^{(t)}\big).\] <p><strong>PL objective with L2 penalties (optimise over \((h)\) and the upper-triangle \(\{J_{ij}\}_{i&lt;j}\) ):</strong></p> \[\mathcal L_{\mathrm{PL}}(h,J) = \overline{ \sum_{i=1}^N \Big[s_i f_i - \log\big(2\cosh f_i\big)\Big] } - \frac{\lambda_h}{2}||h||_2^2 - \frac{\lambda_J}{2}\sum_{i&lt;j} J_{ij}^2 , \qquad J_{ij}=J_{ji}, \qquad J_{ii}=0 .\] </li> </ul> <hr/> <details><summary>Click to expand: PL Gradients defined</summary> \[\nabla_{h_i}\mathcal L_{\mathrm{PL}} = \overline{s_i - \tanh f_i,}-\lambda_h h_i\] \[\nabla_{J_{ij}}\mathcal L_{\mathrm{PL}} = \overline{2s_i s_j - s_j \tanh f_i - s_i \tanh f_j,} -\lambda_J J_{ij}, \qquad i&lt;j\] <p>(Bars denote averages over time; gradients include L2 terms)</p> </details> <hr/> <ul> <li> <p><strong>Variational Bayes (VB):</strong> Gaussian prior on \((h,J)\) with separate precisions for fields/couplings; a quadratic bound on the log‑partition yields closed‑form majorise–minimise updates:</p> \[\begin{aligned} \Sigma^{-1} &amp;= \Lambda_0 + T\,C_\eta,\\ \mu &amp;= \theta_0 + \Sigma\,T\big(\bar{\Phi} - m_\eta\big), \end{aligned}\] <p>where \(m_\eta = \mathbb{E}_{p_\eta}[\Phi], \qquad C_\eta = \operatorname{Cov}_{p_\eta}(\Phi),\) are the model moments/curvature evaluated at the current anchor \(\eta=\mu\).</p> <p>Credible intervals come from \(\Sigma\); optional Gamma hyper‑priors give ARD‑style shrinkage.</p> </li> </ul> <hr/> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20192129.png" alt="Variational Bayes quality report with posterior standard deviations, top z-scores, and coefficient-of-variation of couplings" style="width:100%;height:auto;border:1px solid rgba(255, 255, 255, 0.84); border-radius:8px;" w=""/> <figcaption style="color:#e5e7eb"> <strong>VB-Ising diagnostics.</strong> Iteration summary and posterior uncertainties for fields/couplings. Large \(|z|=\lvert\mu\rvert/\sigma\) edges are strongly supported; high coefficient-of-variation flags potentially unstable couplings. Use these readouts to prioritise robust interactions when interpreting circuit-level dependencies. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20234938.png" alt="Variational Bayes Ising diagnostics: uncertainty vs magnitude, posterior sd(J) maps, ELBO trajectory, field uncertainties, ARD precision spectrum, data–model probability agreement, CV histogram, fit-quality indices"/> <figcaption style="color:#f5f7ff; text-shadow:0 1px 2px rgba(255, 255, 255, 0.84);"> <strong>Variational-Bayes PMEM: uncertainty and fit checks.</strong> Panels summarise one VB run: (i) posterior uncertainty vs coupling magnitude; (ii, v) heatmaps of posterior s.d. for couplings (diagonal masked; log-scale variant); (iii) (negative) ELBO trajectory decreasing across iterations (convergence); (iv) histogram of field uncertainties; (vi) ARD precision spectrum indicating data-driven shrinkage; (vii) model-vs-empirical state probabilities (log–log; closer to the diagonal is better); (viii) histogram of coupling coefficient-of-variation \( \mathrm{CV}=\sigma/|\mu| \); (ix) two fit-quality indices (moment-matching accuracy). Together these quantify parameter credibility and goodness-of-fit before ELA/PDA. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20234218.png" alt="Energy–probability diagnostic: empirical probability vs shifted energy with basin colours, histograms and slope fit"/> <figcaption style="color:#f5f7ff; text-shadow:0 1px 2px rgba(255, 255, 255, 0.84);"> <strong>Energy–probability diagnostic.</strong> Empirical pattern probabilities \( P_{\mathrm{emp}}(\sigma) \) vs shifted energies \( E(\sigma)-E_{\min} \). An approximately linear trend in log-probability vs energy (dashed fit; slope and \( R^2 \) shown) is consistent with Boltzmann structure. Points are coloured by basin; the circled marker denotes the global minimum. Marginal histograms summarise energy and count distributions. Deviations at the extremes flag rare states and help identify outliers before landscape and phase-diagram analyses. </figcaption> </figure> <h3 id="fit-quality--sanity">Fit quality &amp; sanity</h3> <p>We report (i) moment matching (means and pairwise correlations), (ii) multi‑information explained and KL‑reduction vs. independence, and (iii) empirical‑vs‑Boltzmann pattern agreement.</p> <p>For Monte‑Carlo checks we use multi‑chain sampling<d-cite key="metropolis1953mcmc"></d-cite> with <strong>R̂</strong>/effective sample size (ESS) <d-cite key="vehtari2021rhat"></d-cite> diagnostics and estimate observables (magnetisation \(m\), Edwards–Anderson \(q\), spin‑glass and uniform susceptibilities \(\chi_{\mathrm{SG}},\chi_{\mathrm{Uni}}\), specific heat \(C\)).</p> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20192518.png" alt="Console log of pseudo-likelihood fit, correlation to empirical distribution, baseline vs PL error, BM-Metropolis sampling time" style="width:100%;height:auto;border:1px solid ;border-radius:8px;"/> <figcaption style="color:#f5f7ff"> <strong>Pseudo-likelihood fit and sampling check.</strong> The I2/IN ratio and correlation \(r\) quantify global match to empirical statistics; the PL error markedly improves the independent baseline. BM–Metropolis runtimes confirm that the fitted model is tractable for sampling-based validation and energy-landscape analyses. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20192606.png" alt="Console showing null-based QC thresholds and a table of m1_error, C_error, passed_QC for each mouse" style="width:100%;height:auto;border:1px solid rgba(255, 255, 255, 0.84); border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Null-based quality control.</strong> Thresholds for first-moment error \(m_1\) and configuration-error \(C\) are estimated from nulls; all shown mice pass. This guards against over-fitting and ensures that downstream landscape metrics (barriers, relaxation) rest on statistically credible fits. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20193023.png" alt="Console listing bootstrap accuracy estimates with 95% confidence intervals for each mouse/run" style="width:100%;height:auto;border:1px solid rgba(255, 255, 255, 0.88); border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Bootstrap accuracy with 95% CIs.</strong> For each mouse we report resampled accuracies and confidence intervals, quantifying the stability of the fitted model against sampling noise — a practical measure of reliability for comparative neuroscience. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20191957.png" alt="Boltzmann rank plot of empirical probabilities by energy rank with basin colouring and fitted slope" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Boltzmann rank plot (basin-coloured).</strong> Empirical probabilities versus energy rank (log-scale) assess Boltzmann-like ordering: high-probability states sit among the lowest-energy configurations. Basin colours reveal which attractors dominate the high-probability tail; the dashed fit summarises the overall decay. </figcaption> </figure> <h3 id="implementation-highlights">Implementation highlights</h3> <p>PL uses mean‑field initialisation, symmetric updates, Armijo backtracking and a relative gradient test; VB stores posterior precisions and ELBO traces for convergence auditing. (See robustness notes in Section Robustness, uncertainty and diagnostics.)</p> <hr/> <h2 id="5-energy-landscape-analysis-ela-descriptors-and-kinetics">5) Energy-Landscape Analysis (ELA): descriptors and kinetics</h2> <p>Once \((h,J)\) are fitted, the Ising energy</p> \[E(\mathbf{s})=-\sum_i h_i s_i-\tfrac{1}{2}\sum_{i\neq j}J_{ij} s_i s_j\] <p>induces a rugged energy landscape over \(\{-1,+1\}^N\).</p> <p>We compute:</p> <ul> <li><strong>Attractors and basins:</strong> Local minima are states whose single‑spin flips all increase energy. Every state is assigned to a basin by steepest‑descent (or best‑improving) paths. We summarise basins by occupancies and a disconnectivity graph <d-cite key="becker1997disconnectivity,wales2006jpcb"></d-cite>.</li> <li><strong>Barriers and disconnectivity:</strong> The barrier between basins \((\alpha,\alpha')\) is the minimum, over all paths connecting them, of the maximum energy along the path; the disconnectivity graph visualises these heights. Denoting the (symmetrised) minimal saddle by \(\overline{E}_{\alpha\alpha'}\),</li> </ul> \[\overline{E}_{\alpha\alpha'} = \min_{\gamma: \alpha\to\alpha'}\ \max_{\mathbf{s}\in\gamma} E(\mathbf{s}).\] <ul> <li>We also estimate a depth threshold (null‑model percentile) to guard against spurious minima.</li> </ul> <figure class="l-page" style="max-width: 820px; margin: 1rem auto;"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20193305.png" alt="Histogram and KDE of pairwise basin barrier heights with median shown" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Distribution of pairwise barrier heights.</strong> Histogram of inferred barrier heights \(\overline{E}_{\alpha,\alpha'}\) between metastable basins in the fitted landscape; the dashed line marks the sample median. Mechanistically, more negative/lower barriers indicate easier inter-basin switches, whereas rarer higher barriers point to protected transitions. The spread quantifies heterogeneity of switching difficulty across the mouse’s brain-state repertoire. </figcaption> </figure> <p><strong>Kinetics (Markov view):</strong> Build a single-spin-flip Metropolis chain with proposal “flip one spin uniformly” <d-cite key="metropolis1953mcmc"></d-cite> and transition probability:</p> \[p(\mathbf{s}\to\mathbf{s}^{(i)}) = \frac{1}{N}\min\{1,\ \exp\big(E(\mathbf{s})-E(\mathbf{s}^{(i)})\big)\},\] <p>where \(\mathbf{s}^{(i)}\) is \(\mathbf{s}\) with spin \(i\) flipped. This yields a \(2^N\times 2^N\) transition matrix \(P\) (or a restriction to the visited subgraph).</p> <p><strong>From \(P\) we derive:</strong></p> <ul> <li>Stationary distribution \(\pi\), dwell-time distributions, and basin occupancy.</li> <li>Mean first-passage times (MFPT): from a set \(A\) to \(B\) via the fundamental matrix \(Z=(I-Q)^{-1}\) of the transient block <d-cite key="kemeny1960finite"></d-cite>.</li> <li>Committors \(q_{AB}\) solving \((I-Q),q=b\), where \(b\) collects transitions into \(B\) <d-cite key="e2006tpt"></d-cite>.</li> <li>Relaxation spectrum (mixing time-scales): non-unit eigenvalues \(\lambda_i(P)\) with \(\tau_i=-1/\log\lvert\lambda_i\rvert\), and the Kemeny constant (mean mixing time) \(K=\sum_{i\ge 2}\frac{1}{1-\lambda_i}\).</li> </ul> <figure class="l-page" style="margin:1.2rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20234516.png" alt="Mean first-passage time (MFPT) matrix across all discrete states, sorted by basin index" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Mean first-passage times (MFPT).</strong> Heatmap of expected steps to reach each <em>target</em> state from each <em>start</em> state (both sorted by basin index). The bright diagonal reflects near-zero self-passage; block structure and asymmetric bands reveal easy vs hard cross-basin routes. White gridlines mark basin boundaries; the colour bar is in steps. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20191024.png" alt="Per-basin dwell-time distributions (violins with overlaid points and summary markers)" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Dwell-time distributions per basin.</strong> For each attractor basin, the violin shows the full distribution of residence times (frames) from the single-spin-flip dynamics; points display individual visits. Wider violins and higher medians indicate kinetically stable basins; narrow shapes near 1–2 frames indicate transient basins. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20191326.png" alt="Time-stripe raster showing the sequence of visited basins over the recording" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Basin-visit raster over time.</strong> Colour-coded stripe plot of the visited basin label across the recording. Long same-colour blocks correspond to sustained dwell periods; frequent colour changes indicate rapid switching. This readout complements MFPT and dwell-time summaries by exposing the temporal ordering of visits. </figcaption> </figure> <figure class="l-page" style="max-width: 820px; margin: 1rem auto;"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20191607.png" alt="Log-PDF of dwell times in steps" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Dwell-time distribution (empirical).</strong> On a semi-log scale the right tail decreases approximately linearly, consistent with near-geometric escape from basins (memoryless hazard). Longer dwells reflect stabilised neural configurations (metastability), while short dwells reflect rapid exploration; the tail’s slope encodes an effective per-step leaving rate. </figcaption> </figure> <figure class="l-page" style="max-width: 820px; margin: 1rem auto;"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20191630.png" alt="Stem plot of slow relaxation times versus mode index" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Slow relaxation spectrum.</strong> Relaxation times \(tau_i\) (from the fitted Markov operator) quantify how quickly perturbations along each mode decay. A dominant \(tau_1\) and a gap to subsequent modes signal slow inter-basin exchange and long memory in the dynamics — a hallmark of metastable neural regimes. </figcaption> </figure> <figure class="l-page" style="max-width: 820px; margin: 1rem auto;"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20191557.png" alt="Histogram of empirical committor values from A to B" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Empirical committor \(q_i\) for \(A=[150]\to B=[1309]\).</strong> The committor is the forward-commitment probability that a state first hits \(B\) before \(A\). Mass near \(q_i\approx 0\) indicates most visited states lie closer to \(A\); states with \(q_i\approx 0.5\) are transition-like and highlight probable dynamical bottlenecks in the brain’s state graph. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20191723.png" alt="Console summary with accuracies, stationary entropy, relaxation times, spectral gap, Kemeny constant" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Model-fit quality and global kinetics.</strong> Reported are fit accuracies, stationary entropy \(H(\pi)\) (spread of state use), slow \(\tau_i\), spectral gap \(\lambda_1-\lambda_2\) (mixing speed), and the Kemeny constant (mean hitting time averaged over targets). Together they summarise how well the model matches the data and how swiftly the brain’s state dynamics explore the landscape. </figcaption> </figure> <p><strong>Read-outs:</strong> (i) attractor maps (patterns + labels), (ii) disconnectivity graphs, (iii) barrier distributions, (iv) transition/reachability matrices (one-step and multi-step), and (v) kinetic summaries (MFPT heatmaps, committor fields, relaxation spectra, Kemeny constants). These quantify stability, switching propensity, and heterogeneity of access between states.</p> <figure class="l-page"> <div id="ela-1" style="width:100%;max-width:1100px;height:min(80vh,820px);"></div> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Interactive 3D energy landscape for an example mouse</strong> </figcaption> </figure> <figure class="l-page"> <div id="ela-2" style="width:100%;max-width:1100px;height:min(80vh,820px);"></div> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Interactive 3D energy landscape for another example mouse</strong> </figcaption> </figure> <script src="https://cdn.plot.ly/plotly-2.32.0.min.js"></script> <script>(async()=>{const t=[{mount:document.getElementById("ela-1"),url:"/blog/assets/plotly/2025-10-25-phase/3D-ELA22.json"},{mount:document.getElementById("ela-2"),url:"/blog/assets/plotly/2025-10-25-phase/3D-ELA23.json"}];for(const e of t)try{const t=await fetch(e.url),l=await t.json(),n=l.data||l,a=l.layout||{},s=Object.assign({responsive:!0,displaylogo:!1},l.config||{});Plotly.newPlot(e.mount,n,a,s)}catch(o){console.error("Plotly JSON load error:",e.url,o),e.mount.textContent="Interactive figure failed to load."}})();</script> <p>Crucially, these mechanistic and interpretable descriptors and metrics provide an additional high-level framework for comparing brain dynamics across different individuals, or even cohorts with systematically divergent patterns of neural activity - a discrete and more intuitive alternative to classic means for unifying/juxtaposing representations in computational systems.</p> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-22%20225317.png" alt="Energy Landscape Analysis (ELA) panel with attractor patterns, 3D energy surface with basins and paths, transition matrices, disconnectivity graph, basin visit counts and basin sizes"/> <figcaption style="color:#f5f7ff; text-shadow:0 1px 2px rgba(0,0,0,.55);"> <strong>Energy-Landscape Analysis (ELA) — summary descriptors.</strong> The composite figure illustrates the standard read-outs used downstream of the fitted Ising model: <br/> <em>(A)</em> <strong>Local-minimum patterns</strong> (binary states for each attractor); <em>(B)</em> <strong>3-D energy surface</strong> with labelled minima (white dots) and most-probable transition paths (white arrows); <em>(C)</em> <strong>Direct transition counts</strong> between minima (Metropolis single-flip kernel); <em>(D)</em> <strong>Disconnectivity graph</strong> showing barrier heights that separate basins; <em>(E)</em> <strong>Basin visit frequencies</strong> (empirical occupancy); <em>(F)</em> <strong>Basin sizes</strong> (number of micro-states per basin in state-space); <em>(G)</em> <strong>Direct/indirect transition counts</strong> summarising multi-step reachability. Deeper basins and higher barriers indicate more stable, harder-to-leave states; denser transition lanes point to preferred switching routes. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-11%20161144.png" alt="Basin graph: mosaics of microstates grouped by attractor label with an energy colour bar"/> <figcaption style="color:#eaeaea;"> <strong>Basin graph (alternative A: mosaics).</strong> Each panel corresponds to one attractor (State 1–18). Circles denote individual binary microstates assigned to that basin; circle colour encodes Ising energy (cooler = lower). This compact view shows how densely each basin occupies nearby configurations and highlights within-basin heterogeneity (broader colour spread ⇒ greater internal energy variance).<br/> * Visualisation tools for direct/indirect transitions, local minimum patterns, disconnectivity graphs, and mosaic-style basin graphs adapted from: <a href="https://github.com/okumakito/elapy.git/">Oku &amp; Kimura</a> </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-11%20161034.png" alt="Directed neighbourhood graphs for selected basins with node colour=energy and arrowed transitions"/> <figcaption style="color:#eaeaea;"> <strong>Basin graph (alternative B: directed neighbourhoods).</strong> For selected attractors (States 5–8), nodes are microstates (colour = energy) and arrows indicate admissible single-spin-flip moves under the Metropolis kernel. The layered, fan-shaped structure reflects typical downhill funnels into each attractor; sparse cross-links indicate rarer exits via saddles. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-11%20160912.png" alt="3D energy surface with numbered minima and white transition skeleton overlaid; colour bar shows energy"/> <figcaption style="color:#eaeaea;"> <strong>3-D energy landscape.</strong> A continuous rendering of the fitted Ising energy with numbered minima (basins) and a white transition skeleton connecting them through low-saddle routes. Valleys (cool colours) are deep, stable basins; ridges quantify barrier heights that regulate switching. </figcaption> </figure> <figure class="l-page" style="max-width: 820px; margin: 1rem auto;"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-11%20160656.png" alt="2D contour map of energy with the same transition skeleton between minima; colour bar shows energy levels"/> <figcaption style="color:#eaeaea;"> <strong>2-D energy landscape.</strong> The same landscape as a contour map. This top-down view makes it easy to read relative heights along paths and to spot alternative routes between basins (branch points near saddles). Together with the 3-D view, it provides complementary intuition about basin depth (3-D) and path geometry (2-D). </figcaption> </figure> <hr/> <h2 id="6-phase-diagram-analysis-pda-multi-observable-placement">6) Phase-Diagram Analysis (PDA): multi-observable placement</h2> <p><strong>Goal:</strong> Place every subject on a <em>shared</em> Sherrington–Kirkpatrick-like \((\mu,\sigma)\) phase surface using <em>multiple</em> observables at once, with uncertainty, so that cohorts become directly comparable without needing a fixed “healthy baseline”. PDA sits downstream of our shared-latent → binarisation → Ising (PMEM) fit, and is designed to be robust, auditable, and reproducible from end to end. <d-cite key="edwards1975ea,sherrington1975sk,ezaki2020critical"></d-cite></p> <style>figure.plotblock{clear:both;display:block;margin:2rem 0 3rem}figure.plotblock .holder{width:100%;max-width:1100px;margin:0 auto;height:clamp(520px,72vh,900px);overflow:hidden}figure.plotblock figcaption{margin-top:.75rem;text-align:center;font-size:.95rem;color:var(--theme-text,#eaeaea)}</style> <script src="https://cdn.plot.ly/plotly-2.32.0.min.js"></script> <figure class="plotblock l-page"> <div id="pda-m-fig" class="holder"></div> <figcaption><strong>[Interactive] Magnetisation (m)</strong> — whole-brain activation bias on the σ–μ plane (pooled-reference surface).</figcaption> </figure> <script>(async()=>{const t=document.getElementById("pda-m-fig");try{const e=await fetch("/blog/assets/plotly/2025-10-25-phase/3D-PDA-m.json"),a=await e.json();Plotly.newPlot(t,a.data||a,a.layout||{},Object.assign({responsive:!0,displaylogo:!1},a.config||{}))}catch(o){console.error("Plot load error: m",o),t.textContent="Interactive figure failed to load."}})();</script> <figure class="plotblock l-page"> <div id="pda-q-fig" class="holder"></div> <figcaption><strong>[Interactive] Spin-glass order (q)</strong> — pattern stability; high q = rigid/repetitive, low q = flexible/variable (pooled-reference).</figcaption> </figure> <script>(async()=>{const t=document.getElementById("pda-q-fig");try{const e=await fetch("/blog/assets/plotly/2025-10-25-phase/3D-PDA-q.json"),a=await e.json();Plotly.newPlot(t,a.data||a,a.layout||{},Object.assign({responsive:!0,displaylogo:!1},a.config||{}))}catch(o){console.error("Plot load error: q",o),t.textContent="Interactive figure failed to load."}})();</script> <figure class="plotblock l-page"> <div id="pda-chisg-fig" class="holder"></div> <figcaption><strong>[Interactive] Spin-glass susceptibility (χ<sub>SG</sub>)</strong> — sensitivity to local perturbations; peaks indicate proximity to critical boundaries (pooled-reference).</figcaption> </figure> <script>(async()=>{const t=document.getElementById("pda-chisg-fig");try{const e=await fetch("/blog/assets/plotly/2025-10-25-phase/3D-PDA-chiSG.json"),a=await e.json();Plotly.newPlot(t,a.data||a,a.layout||{},Object.assign({responsive:!0,displaylogo:!1},a.config||{}))}catch(o){console.error("Plot load error: chiSG",o),t.textContent="Interactive figure failed to load."}})();</script> <figure class="plotblock l-page"> <div id="pda-chiuni-fig" class="holder"></div> <figcaption><strong>[Interactive] Uniform susceptibility (χ<sub>uni</sub>)</strong> — sensitivity to a global nudge; high values = strong coherent whole-brain shift (pooled-reference).</figcaption> </figure> <script>(async()=>{const t=document.getElementById("pda-chiuni-fig");try{const e=await fetch("/blog/assets/plotly/2025-10-25-phase/3D-PDA-chiUni.json"),a=await e.json();Plotly.newPlot(t,a.data||a,a.layout||{},Object.assign({responsive:!0,displaylogo:!1},a.config||{}))}catch(o){console.error("Plot load error: chiUni",o),t.textContent="Interactive figure failed to load."}})();</script> <figure class="plotblock l-page"> <div id="pda-c-fig" class="holder"></div> <figcaption><strong>[Interactive] Specific heat (C)</strong> — variance of model energy; high C = many competing states near a phase boundary (pooled-reference).</figcaption> </figure> <script>(async()=>{const t=document.getElementById("pda-c-fig");try{const e=await fetch("/blog/assets/plotly/2025-10-25-phase/3D-PDA-C.json"),a=await e.json();Plotly.newPlot(t,a.data||a,a.layout||{},Object.assign({responsive:!0,displaylogo:!1},a.config||{}))}catch(o){console.error("Plot load error: C",o),t.textContent="Interactive figure failed to load."}})();</script> <hr/> <h3 id="61--what-pda-estimates-observables-and-phase-coordinates">6.1 What PDA estimates (observables and phase coordinates)</h3> <p>For a fitted Ising model on binary latents \(\mathbf{s}\in\{-1,+1\}^N\) with fields \(h_i\) and couplings \(J_{ij}\), PDA uses macroscopic observables:</p> <ul> <li> <p><strong>Magnetisation</strong><br/> \(m=\frac{1}{N}\sum_i \langle s_i\rangle\)</p> </li> <li> <p><strong>Edwards–Anderson order</strong><br/> \(q=\frac{1}{N}\sum_i \langle s_i\rangle^2\)</p> </li> <li> <p><strong>Spin‑glass susceptibility</strong> (using the eigenvalues \(\{\lambda_k\}\) of the spin covariance)<br/> \(\chi_{\mathrm{SG}}=\frac{1}{N}\sum_{k=1}^{N}\lambda_k^2\)</p> </li> <li> <p><strong>Uniform susceptibility</strong><br/> \(\chi_{\mathrm{Uni}}=\frac{1}{N}\sum_{i\neq j}\mathrm{Cov}(s_i,s_j)\)</p> </li> <li> <p><strong>Specific heat</strong> (from energy variance)<br/> \(C=\frac{\langle E^2\rangle-\langle E\rangle^2}{N},\qquad E(\mathbf{s})=-\sum_i h_i s_i-\frac{1}{2}\sum_{i\neq j}J_{ij}s_is_j\)</p> </li> </ul> <p>In practice, we compute these from Monte‑Carlo samples of the fitted model (with automatic convergence checks). For quick diagnostics, the first four are also obtainable directly from the binarised data.</p> <hr/> <h3 id="62--the-reference-phase-surface-musigmamapstomqchi_mathrmsgchi_mathrmunic">6.2 The reference phase surface \((\mu,\sigma)\mapsto\{m,q,\chi_{\mathrm{SG}},\chi_{\mathrm{Uni}},C\}\)</h3> <p>We construct a <em>reference</em> coupling matrix \(J_{\mathrm{ref}}\) (either <em>pooled</em> across the cohort or <em>control</em>‑only), then generate a dense grid over target \((\mu,\sigma)\) by <em>affinely transforming the off‑diagonal entries</em> of \(J_{\mathrm{ref}}\). Let \(\mu_{\text{old}}\) and \(\sigma_{\text{old}}\) be the mean and std of off‑diagonal entries of \(J_{\mathrm{ref}}\). For a target \((\mu,\sigma)\):</p> \[J_{ij}^{(\mu,\sigma)}= \begin{cases} \big(J_{ij}-\mu_{\text{old}}\big)\dfrac{\sigma}{\sigma_{\text{old}}+\varepsilon}+\mu, &amp; i\neq j,\\[6pt] 0, &amp; i=j~, \end{cases}\] <p>with all <strong>fields zeroed</strong> \(h_i\equiv 0\) (diagonal remains 0) to recover the canonical spin‑glass phase structure. For each grid point we Monte‑Carlo sample the observables above and cache five surfaces \(\{\mathrm{m},\mathrm{q},\chi_{\mathrm{SG}},\chi_{\mathrm{Uni}},C\}\).</p> <p><strong>Working vs display grids:</strong> We build a high‑resolution <em>working</em> grid used for optimisation/placement and optionally a wider <em>display</em> grid for visuals. We automatically pick \((\mu,\sigma)\) limits by running quick PL fits per subject to estimate native \((\hat\mu,\hat\sigma)\), then expand by a safety factor; we avoid overly coarse grids (e.g., if \(\Delta\mu\) or \(\Delta\sigma&gt;0.01\)).</p> <p><strong>MC convergence safeguards:</strong> We run multiple chains with increasing sweep budgets until <em>all</em> observables reach \(\hat R&lt;1.05\) and an effective sample size threshold, or a maximum sweep cap is hit (in which case a warning is issued).</p> <hr/> <h3 id="63--multiobservable-placement-cost-projection-with-balanced-weights">6.3 Multi‑observable placement (cost projection with balanced weights)</h3> <p>Given a subject’s <em>observed</em> \((m_o,q_o,\chi^{o}_{\mathrm{SG}},\chi^{o}_{\mathrm{Uni}})\) from their binarised latents, we <strong>project</strong> onto the reference surface by minimising a <em>variance‑balanced</em> squared error:</p> \[\underset{\mu,\sigma}{\arg\min}\;\sum_{k\in\{\mathrm{m},\mathrm{q},\chi_{\mathrm{SG}},\chi_{\mathrm{Uni}}\}} w_k\big(O_k^{\text{obs}}-\widehat{O}_k(\mu,\sigma)\big)^2,\] <p>where \(\widehat{O}_k(\mu,\sigma)\) is obtained by regular‑grid interpolation of the precomputed surfaces and weights \(w_k\) default to the inverse <em>range</em> of each surface (less sensitive than \(1/\mathrm{var}\)). Optimisation uses L‑BFGS‑B on the working grid bounds; we also provide an <strong>iso‑curve</strong> fallback (intersect level sets of \(\chi_{\mathrm{SG}}\) and \(\chi_{\mathrm{Uni}}\) with a simple border‑safety check) and a brute‑force grid fallback.</p> <p>We return \((\hat\mu,\hat\sigma)\), the final cost value, and the method used (“cost_minimisation”, “iso_curves”, or “fallback_grid”).</p> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20235654.png" alt="3-D surface of specific heat C over sigma and mu with subject markers and a dashed ridge"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Specific-heat landscape \(C(\sigma,\mu)\) with cohort placements.</strong> The dashed ridge marks a near-critical band where responsiveness inflates. Subjects cluster on the gentler slope below the ridge; <em>the control mouse consistently occupies the lowest-\(\sigma\)</em> (least heterogeneous couplings), as expected. This panel anchors the multi-observable placement on a shared surface. </figcaption> </figure> <figure class="l-page" style="max-width: 760px; margin: 1rem auto;"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-29%20134043.png" alt="3-D observable surface with two 2-D panels for q and chiSG over sigma and mu; subject points overlaid"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Cross-checks across observables.</strong> A complementary 3-D view (top) with 2-D slices for <em>q</em> (left) and <em>\(\chi_{\mathrm{SG}}\)</em> (right). Consistent subject ordering across panels indicates that placements are not driven by a single metric. The control remains the lowest-\(\sigma\) point on the pooled reference. </figcaption> </figure> <hr/> <h3 id="64--uncertainty-robustness-and-diagnostics">6.4 Uncertainty, robustness, and diagnostics</h3> <p><strong>Bootstrap CIs:</strong> For each subject we run a <em>circular block bootstrap</em> along time, refit the Ising per resample, and recompute \((\mu,\sigma)\) or \((m,q,\chi_{\mathrm{SG}},\chi_{\mathrm{Uni}})\) as needed to report 95% intervals. Specific heat \(C\) can be bootstrapped likewise via short MC runs per resample.</p> <p><strong>Control shrinkage (optional):</strong> When a control is available, we stabilise its estimate by bootstrapping its \(J\), pooling across the cohort, and forming a convex combination \(J^{\text{ctrl,shrunk}}=(1-\lambda)J^{\text{ctrl}}+\lambda J^{\text{pooled}}\); we then summarise \((\mu_c,\sigma_c)\) from the shrunk \(J\).</p> <p><strong>Cost bowls:</strong> Around each optimum we display the <em>local cost landscape</em> (2‑D filled contour and 3‑D surface) to judge identifiability; narrow, well‑curved bowls suggest precise placement.</p> <p><strong>Group‑level tests:</strong> Small helpers allow groupwise comparisons in \(\sigma\) (e.g., Welch ANOVA and permutation tests) using the bootstrapped distributions.</p> <p><strong>Critical structure:</strong> We plot <em>critical contours</em> (e.g., a fixed fraction of the maximum of an observable) on the display surfaces; a simple near‑criticality index is the minimal Euclidean distance from \((\hat\mu,\hat\sigma)\) to the chosen contour.</p> <figure class="l-page" style="max-width: 820px; margin: 1rem auto;"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20235914.png" alt="Bootstrap means with 95% ellipses on the sigma–mu plane for each mouse"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>\(\mu\)–\(\sigma\) placements with uncertainty.</strong> Dots show bootstrap means; ellipses are 95% confidence regions from circular block-bootstrap resamples. Groupings separate along \(\sigma\) (heterogeneity), and the control consistently shows the smallest \(\sigma\) on the pooled reference — a sanity check aligned with biological expectations. </figcaption> </figure> <figure class="l-page" style="max-width: 820px; margin: 1rem auto;"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20235737.png" alt="2-D filled contour of the PDA objective around the optimum with the optimum marked"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Local 2-D cost surface (“identifiability map”).</strong> Filled contours of the variance-balanced multi-observable discrepancy around the optimum in \((\sigma,\mu)\). A tight, symmetric bowl indicates precise, well-conditioned placement; mild elongation reflects correlated trade-offs between observables. </figcaption> </figure> <figure class="l-page" style="max-width: 820px; margin: 1rem auto;"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20235742.png" alt="3-D surface view of the PDA objective around the optimum forming a convex bowl"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>3-D “cost bowl”.</strong> The same objective shown as a 3-D surface. Clear curvature corroborates convergence and local identifiability. (Together with the 2-D map above, this rules out flat minima or boundary locking.) </figcaption> </figure> <hr/> <h3 id="65--practical-recipe-what-the-code-actually-does">6.5 Practical recipe (what the code actually does)</h3> <p>1) <strong>Prepare data</strong>: shared latents → binarise per latent to \(\pm1\).<br/> 2) <strong>Rough bounds</strong>: quick PL fits to get subject‑wise \((\hat\mu,\hat\sigma)\); expand to working ranges; verify resolution is fine enough.<br/> 3) <strong>Build reference surface</strong>: choose <code class="language-plaintext highlighter-rouge">reference_mode ∈ {pooled, control}</code>, set \(h\equiv 0\), sweep \((\mu,\sigma)\) by affine off‑diagonal transforms of \(J_{\mathrm{ref}}\), run multi‑chain MC with convergence checks, and cache \(\{\mathrm{m},\mathrm{q},\chi_{\mathrm{SG}},\chi_{\mathrm{Uni}},C\}\) surfaces.<br/> 4) <strong>Place subjects</strong>: minimise the balanced multi‑observable cost (or use the iso‑curve / grid fallbacks with border guard).<br/> 5) <strong>Uncertainty</strong>: bootstrap along time to report CIs; optionally shrink the control to summarise \((\mu_c,\sigma_c)\).<br/> 6) <strong>Visuals</strong>: 2‑D contour panels and interactive 3‑D surfaces, with group‑colours and critical lines; “cost bowls” per subject for identifiability.</p> <hr/> <h3 id="66--mathematical-and-implementation-notes-exact-to-this-workflow">6.6 Mathematical and implementation notes (exact to this workflow)</h3> <ul> <li><strong>Affine mapping of \(J\) to \((\mu,\sigma)\)</strong> modifies <em>only</em> off‑diagonals and preserves \(J_{ii}=0\). This avoids artefactual self‑coupling and keeps the spin‑glass structure intact. Fields \(h\) are explicitly zeroed for the surface.</li> <li><strong>Observables from data vs model.</strong> Fast “data‑side” \(m,q,\chi\) provide immediate checks, while MC‑side estimates (including \(C\)) are used to build the surface; both routes are available.</li> <li><strong>Balanced weights</strong> \(w_k\) default to inverse <em>range</em> (normalises sensitivity without over‑penalising heavy‑tailed metrics). Equal weights are also supported.</li> <li><strong>Convergence gating</strong> uses multi‑chain Gelman-Rubin \(\hat R\) and a conservative ESS check with geometric back‑off of sweeps up to a cap; we report if the cap is reached.</li> <li><strong>Border guard</strong> prevents pathological “corner locking” when intersecting \(\chi\)‑iso‑curves on coarse grids.</li> </ul> <hr/> <h3 id="67--interpretation-tips">6.7 Interpretation tips</h3> <h3 id="metric-glossary">Metric glossary:</h3> <p><strong>\(\sigma\) — network heterogeneity (dispersion of couplings)</strong><br/> <strong>Meaning:</strong> standard deviation of off‑diagonal \(J_{ij}\) on the reference surface.<br/> <strong>High:</strong> uneven, subnetwork‑biased interactions; rugged energy landscape with many competing basins; dynamics readily reconfigured by <em>local</em> nudges.<br/> <strong>Low:</strong> broadly uniform interactions; smoother coordination; fewer competing minima (less glassy).</p> <p><strong>\(\mu\) — net coupling / co‑activation balance (mean coupling)</strong><br/> <strong>Meaning:</strong> mean of off‑diagonal \(J_{ij}\) (with \(h\approx 0\) on the surface).<br/> <strong>High (positive):</strong> stronger global ordering/co‑activation; coherent whole‑system shifts are easy.<br/> <strong>Low/negative:</strong> interactions cancel or oppose; regions act more independently/segregate.</p> <p><strong>\(m\) — magnetisation (whole‑brain activation bias)</strong><br/> <strong>Definition:</strong> \(m=\tfrac{1}{N}\sum_i \langle s_i\rangle\).<br/> <strong>Large \(|m|\):</strong> prolonged hypo‑ or hyper‑activation (tonic bias; long runs in one sign).<br/> <strong>Near 0:</strong> balanced on/off usage.<br/> <strong>Note:</strong> on the reference surface \(h=0\), so any non‑zero \(m\) reflects ordering from \(\mu\) (or thresholding bias when computed directly from data).</p> <p><strong>\(q\) — Edwards–Anderson order (pattern rigidity)</strong><br/> <strong>Definition:</strong> \(q=\tfrac{1}{N}\sum_i \langle s_i\rangle^2\) (persistence per unit regardless of sign).<br/> <strong>High:</strong> recurring, “frozen” configurations; can be rigid even when \(m\approx 0\) (symmetry‑related states).<br/> <strong>Low:</strong> flexible/exploratory dynamics with weak per‑unit bias.</p> <p><strong>\(\chi_{\mathrm{SG}}\) — spin‑glass susceptibility (sensitivity to <em>local</em> perturbations)</strong><br/> <strong>Meaning:</strong> response to heterogeneous, small nudges; equals the sum of squared eigenvalues of the spin covariance (normalised by \(N\)).<br/> <strong>High:</strong> small local changes can reconfigure the network; hallmark of glassy, high‑\(\sigma\) regimes with many shallow basins.<br/> <strong>Low:</strong> locally robust; topology resists piecemeal perturbations.</p> <p><strong>\(\chi_{\mathrm{Uni}}\) — uniform susceptibility (sensitivity to a <em>global</em> nudge)</strong><br/> <strong>Meaning:</strong> response when all units are pushed equally (normalised sum of pairwise covariances).<br/> <strong>High:</strong> easy, coherent whole‑brain shift (often increases with \(\mu\), decreases with \(\sigma\)).<br/> <strong>Low:</strong> globally inertial/decoupled.</p> <p><strong>\(C\) — specific heat (breadth of accessible repertoire)</strong><br/> <strong>Definition:</strong> \(C=\big(\langle E^2\rangle-\langle E\rangle^2\big)/N\), i.e., energy variance per unit.<br/> <strong>High:</strong> wide repertoire; peaks near phase boundaries/critical bands (heightened responsiveness).<br/> <strong>Low:</strong> narrow variability; stereotyped dynamics.</p> <hr/> <h3 id="typical-regimes-reading-combinations">Typical regimes (reading combinations):</h3> <ul> <li><strong>Ferromagnetic‑like:</strong> \(\mu\) high, \(\sigma\) low → large \(\|m\|\), high \(q\), high \(\chi_{\mathrm{Uni}}\), low \(\chi_{\mathrm{SG}}\); \(C\) can peak near the ordering boundary.</li> <li><strong>Spin‑glass‑like:</strong> \(\mu\approx 0\), \(\sigma\) high → \(m\approx 0\) but \(q\) elevated, \(\chi_{\mathrm{SG}}\) high, \(\chi_{\mathrm{Uni}}\) low; many metastable basins and irregular switching.</li> <li><strong>Paramagnetic‑like:</strong> \(\mu\approx 0\), \(\sigma\) low → \(m\approx 0\), low \(q\), both susceptibilities low, \(C\) low; weak coordination, noise‑like exploration.</li> <li><strong>Near‑critical band:</strong> \(C\) high with elevated susceptibilities; large fluctuations and long correlation lengths.</li> </ul> <hr/> <h3 id="practical-notes">Practical notes:</h3> <ul> <li>PDA surfaces set \(h\approx 0\); placements therefore reflect coupling structure \(\big(\mu,\sigma\big)\) rather than tonic biases.</li> <li>Compare subjects on the <strong>same</strong> reference (pooled vs control can shift absolute positions). Use uncertainty ellipses/cost bowls to judge identifiability.</li> <li>Large \(\|m\|\) in data space can arise from binarisation thresholds or residual trends—validate preprocessing.</li> <li>PDA gives macroscopic coordinates; pair with ELA for basin‑level mechanisms (attractors, barriers, kinetics).</li> </ul> <hr/> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20135822.png" alt="Five reference surfaces (m, q, chiSG, chiUni, C) over sigma and mu with subject placements"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Multi-observable phase surfaces with subject placements.</strong> Each panel shows an observable over \( (\sigma,\mu) \): top row — magnetisation \( m \), Edwards–Anderson order \( q \), spin-glass susceptibility \( \chi_{\mathrm{SG}} \); bottom row — uniform susceptibility \( \chi_{\mathrm{Uni}} \) and specific heat \( C \). Points mark each subject’s placement on the pooled reference. The steep wing indicates a near-critical band; the control occupies the smallest \( \sigma \). </figcaption> </figure> <hr/> <h3 id="68--reproducibility-knobs-defaults">6.8 Reproducibility knobs (defaults)</h3> <ul> <li>MC: chains = 12, start sweeps = 8k, burn‑in = 1k, cap = 128k, \(\hat R\) tol = 1.05.</li> <li>Working grid: \(140\times140\) by default; guard: \(\max(\Delta\mu,\Delta\sigma)\le 0.01\).</li> <li>Objective: weights = “balanced” (inverse range), optimiser = L‑BFGS‑B within bounds.</li> <li>Bootstrap: circular blocks, user‑set size; control shrinkage parameter \(\lambda\in[0,1]\).</li> </ul> <hr/> <h3 id="69--illustrative-pseudocode-orientation-only--code-is-not-released-here">6.9 Illustrative pseudocode (orientation only — code is not released here)</h3> <blockquote> <p>The snippet below mirrors the sequence used to generate the figures; it is descriptive rather than an API.</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1) Build the reference phase surfaces (pooled or control)
</span><span class="n">phase</span> <span class="o">=</span> <span class="nf">build_phase_surfaces</span><span class="p">(</span>
    <span class="n">binaries_pm1</span> <span class="o">=</span> <span class="n">BINARIES_PM1_OR_01</span><span class="p">,</span>         <span class="c1"># dict: subject → DataFrame (±1)
</span>    <span class="n">reference</span>    <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">mode</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">pooled</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">subject</span><span class="sh">"</span><span class="p">:</span> <span class="bp">None</span><span class="p">},</span>  <span class="c1"># or {"mode": "control","subject": "ID"}
</span>    <span class="n">grid_size</span>    <span class="o">=</span> <span class="mi">150</span><span class="p">,</span>                         <span class="c1"># working grid resolution
</span>    <span class="n">mc_settings</span>  <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">chains</span><span class="sh">"</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span> <span class="sh">"</span><span class="s">burn_in</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span> <span class="sh">"</span><span class="s">start_sweeps</span><span class="sh">"</span><span class="p">:</span> <span class="mi">8000</span><span class="p">}</span>
<span class="p">)</span>

<span class="c1"># 2) Place subjects via multi-observable cost (balanced weights by default)
</span><span class="n">positions</span> <span class="o">=</span> <span class="nf">place_subjects_on_surface</span><span class="p">(</span>
    <span class="n">binaries_pm1</span> <span class="o">=</span> <span class="n">BINARIES_PM1_OR_01</span><span class="p">,</span>
    <span class="n">phase</span>        <span class="o">=</span> <span class="n">phase</span><span class="p">,</span>
    <span class="n">method</span>       <span class="o">=</span> <span class="sh">"</span><span class="s">cost</span><span class="sh">"</span><span class="p">,</span>              <span class="c1"># or "iso_curves"
</span>    <span class="n">weights_mode</span> <span class="o">=</span> <span class="sh">"</span><span class="s">balanced</span><span class="sh">"</span>
<span class="p">)</span>

<span class="c1"># 3) Inspect local identifiability (“cost bowl”) for one subject
</span><span class="n">w</span> <span class="o">=</span> <span class="nf">objective_weights</span><span class="p">(</span><span class="n">phase</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">balanced</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">plot_cost_landscape</span><span class="p">(</span>
    <span class="n">subject_id</span> <span class="o">=</span> <span class="sh">"</span><span class="s">SUBJECT_ID</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">positions</span>  <span class="o">=</span> <span class="n">positions</span><span class="p">,</span>
    <span class="n">phase</span>      <span class="o">=</span> <span class="n">phase</span><span class="p">,</span>
    <span class="n">window</span>     <span class="o">=</span> <span class="mf">0.06</span><span class="p">,</span>
    <span class="n">steps</span>      <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">weights</span>    <span class="o">=</span> <span class="n">w</span>
<span class="p">)</span>

</code></pre></div></div> <hr/> <h3 id="610--caveats-specific-to-pda">6.10 Caveats specific to PDA</h3> <ul> <li>PDA assumes an SK‑like parameterisation (coupling distribution matters); it analyses <strong>macrostates</strong> and does not replace ELA’s mechanistic, basin‑level descriptors.</li> <li>The choice of reference surface (pooled vs control) can shift placements; we therefore expose the cost bowls and allow both options to be reported.</li> <li>Grid resolution and MC budgets matter near sharp boundaries; guards and diagnostics make this explicit.</li> </ul> <hr/> <h2 id="results-at-a-glance">Results at a glance</h2> <p>On resting-state functional ultrasound (fUS) recordings (mesoscopic, whole-brain), we observe low placement residuals \((10^{-6}–10^{-4})\), tight bootstrap confidence regions, convergent models, and stable ordering under pooled vs subgroup phase references; example estimates span <strong>σ ≈ 0.15–0.32</strong> and <strong>μ ≈ −0.01 to +0.03</strong>. The outputs - susceptibility to perturbations, ordering vs glassiness, transition propensity - form compact, biologically meaningful fingerprints.</p> <p>For experimentalists, this is a mechanistic dashboard (what states exist, how deep, how likely to switch); for theorists, it anchors subjects in a physics-grounded phase space with interpretable axes.</p> <p>Overall, the multi-observable placement and the combination of shared embeddings with ELA/PDA provide reliability-minded, comparable, and interpretable read-outs that support discovery, phenotyping, and model-based hypothesis generation across cohorts, data modalities, tasks, and species.</p> <hr/> <h2 id="robustness-uncertainty-and-diagnostics">Robustness, uncertainty and diagnostics</h2> <ul> <li>Uncertainty via variational-Bayes posteriors for h and J; bootstrap intervals for mu and sigma and for near-criticality; block bootstrap for autocorrelation <d-cite key="politis1992cbb,politis2004blocklength"></d-cite></li> <li>Convergence checks including MCMC R̂ and effective sample size <d-cite key="vehtari2021rhat"></d-cite> where applicable, pseudo-likelihood relative-norm stopping, and ELBO improvements for variational Bayes</li> <li>Quality-control gates including permutation or null thresholds for spurious minima, component-stability filters, and sensitivity to number of latents and alignment choice</li> <li>Ablations: pooled versus subgroup reference surfaces; method toggles among SRM, MCCA, and ICA; median versus mean thresholds</li> </ul> <hr/> <h2 id="reproducibility-and-artefacts">Reproducibility and artefacts</h2> <p>We report key settings and diagnostics to make the computational provenance clear, even though the code is not released with this post. Runs are seed-controlled with machine-parsable configuration files; convergence (e.g., R̂, ESS) and grid resolution checks are documented in the figure captions and text.</p> <p><strong>Example configuration stub (indicative):</strong></p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># example-config.yaml</span>
<span class="na">seed</span><span class="pi">:</span> <span class="m">123</span>
<span class="na">preprocess</span><span class="pi">:</span>
  <span class="na">detrend</span><span class="pi">:</span> <span class="s">conservative</span>
  <span class="na">concat_runs</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">impute</span><span class="pi">:</span> <span class="s">short_gaps_only</span>
<span class="na">alignment</span><span class="pi">:</span>
  <span class="na">methods</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">SRM</span><span class="pi">,</span> <span class="nv">MCCA</span><span class="pi">,</span> <span class="nv">GroupPCA</span><span class="pi">,</span> <span class="nv">GroupICA</span><span class="pi">]</span>
  <span class="na">consensus</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">select_dim</span><span class="pi">:</span> <span class="s">auto</span>
<span class="na">binarise</span><span class="pi">:</span>
  <span class="na">threshold</span><span class="pi">:</span> <span class="s">median</span>
<span class="na">ising</span><span class="pi">:</span>
  <span class="na">mode</span><span class="pi">:</span> <span class="s">PL</span>            <span class="c1"># {EXACT|PL|VB}</span>
  <span class="na">l2_h</span><span class="pi">:</span> <span class="s">1e-5</span>
  <span class="na">l2_J</span><span class="pi">:</span> <span class="s">1e-4</span>
  <span class="na">pl_tol</span><span class="pi">:</span> <span class="s">1e-6</span>        <span class="c1"># relative-norm stopping</span>
  <span class="na">vb</span><span class="pi">:</span>
    <span class="na">prior_prec_h</span><span class="pi">:</span> <span class="m">6.0</span>
    <span class="na">prior_prec_J</span><span class="pi">:</span> <span class="m">30.0</span>
<span class="na">ela</span><span class="pi">:</span>
  <span class="na">minima_search</span><span class="pi">:</span> <span class="s">exhaustive</span>
  <span class="na">kinetics</span><span class="pi">:</span> <span class="kc">true</span>
<span class="na">pda</span><span class="pi">:</span>
  <span class="na">observables</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">m</span><span class="pi">,</span> <span class="nv">q</span><span class="pi">,</span> <span class="nv">chiSG</span><span class="pi">,</span> <span class="nv">chiUni</span><span class="pi">,</span> <span class="nv">C</span><span class="pi">]</span>
  <span class="na">bootstrap</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">ci_level</span><span class="pi">:</span> <span class="m">0.95</span>
<span class="na">reports</span><span class="pi">:</span>
  <span class="na">phase_report</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">landscape_report</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div> <hr/> <h2 id="limitations-and-scope">Limitations and scope</h2> <ul> <li>Binarisation coarsens signals, but enables interpretable PMEM fitting and stable cross-subject comparability</li> <li>Latent selection is influential; consensus and metric-guided selection mitigate but semantics remain partly model-dependent</li> <li>The choice of reference surface affects PDA; we quantify sensitivity and expose cost bowls for transparency</li> <li>Designed for resting or task neural time series; extension to other binarisable dynamical systems is often straightforward</li> </ul> <hr/> <h2 id="outlook">Outlook</h2> <p>Population-universal latents combined with physics-grounded descriptors provide a shared language for multi-subject brain dynamics that is portable across modalities, tasks, and species, and a bridge to mechanistic interpretation and clinical translation. Planned extensions include multi-modal fusion, alignment-aware causal probes, truly dynamic/directional extensions of the methodology (e.g., by incorporating Langevin-based methods and attractor networks), developing modular workflows for modelling the state-spaces of consecutive processing stages in the brain under cognitive tasks, and targeted clinical studies.</p> <hr/> <h3 id="acknowledgements">Acknowledgements</h3> <p> We thank <a href="https://henschlab.mcb.harvard.edu/">Professor Takao K. Hensch</a> and colleagues at Harvard University and Boston Children’s Hospital for providing the dataset used to develop and validate our methods. The dataset is not publicly available and we are not authorised to redistribute it; access remains at the discretion of the data owners. We are grateful to <a href="https://sites.google.com/site/takamitsuwatanabesite/">Professor Takamitsu Watanabe</a> and the International Research Center for Neurointelligence (WPI-IRCN), University of Tokyo, for helpful discussions and early methodological orientation that informed this work. We acknowledge the ongoing institutional support of <a href="https://www.psnc.pl/">the Poznań Supercomputing and Networking Center, Polish Academy of Sciences (PAS)</a>. </p> <hr/> <h2 id="appendix-mathematical-details">Appendix: Mathematical details</h2> <p>Below are the core mathematical underpinnings of the pseudo‑likelihood and variational‑Bayes alternatives to the exact‑likelihood pairwise maximum‑entropy model (PMEM / Ising), as well as of the relevant auxiliary methods (e.g., for VB, the ARD/shrinkage or convergence diagnostics).</p> <details><summary>Click to expand the entire Appendix</summary> <h2 id="notation-common-to-pl-and-vb">Notation (common to PL and VB)</h2> <ul> <li>Binary state \(\mathbf{s}\in\{-1,+1\}^N\) with samples \(\{\mathbf{s}^{(t)}\}_{t=1}^T\).</li> <li>Parameters \(\theta = \big[h_1,\ldots,h_N,\ \{J_{ij}\}_{i&lt;j}\big]^\top \in \mathbb{R}^{P}\), where \(P=N+N(N-1)/2\).</li> <li>Feature map (sufficient statistics) \(\Phi(\mathbf{s}) = \big[s_1,\ldots,s_N,\ \{s_i s_j\}_{i&lt;j}\big]^\top \in \mathbb{R}^{P}\).</li> <li>Ising / PMEM: \(p_\theta(\mathbf{s}) \propto \exp\!\big(\theta^\top \Phi(\mathbf{s})\big), \qquad A(\theta)=\log Z(\theta)=\log \sum_{\mathbf{s}}\exp\!\big(\theta^\top \Phi(\mathbf{s})\big).\)</li> <li>Empirical feature mean: \(\bar{\Phi}=\frac{1}{T}\sum_{t=1}^T \Phi\!\big(\mathbf{s}^{(t)}\big).\)</li> <li>Model moments at parameter \(\theta\): \(m_\theta=\mathbb{E}_{p_\theta}[\Phi], \qquad C_\theta=\operatorname{Cov}_{p_\theta}(\Phi)=\nabla^2 A(\theta).\) <em>(Here \(C_\theta\) is the Fisher information.)</em></li> </ul> <hr/> <h2 id="1-pseudolikelihood-pl-pmem">1) Pseudo‑likelihood (PL) PMEM</h2> <p><strong>Node‑wise conditional</strong> (logistic form). For \(f_i(\mathbf{s}_{\setminus i}) = h_i + \sum_{j\neq i} J_{ij}\, s_j,\) we have \(\log p\!\big(s_i \mid \mathbf{s}_{\setminus i}\big) = s_i f_i - \log\!\big(2\cosh f_i\big), \qquad p\!\big(s_i \mid \mathbf{s}_{\setminus i}\big)=\frac{\exp(s_i f_i)}{2\cosh f_i}.\)</p> <p><strong>Objective with L2</strong> (ridge on \(h\) and off‑diagonal \(J\)): \(\mathcal{L}_{\mathrm{PL}}(h,J) =\sum_{t=1}^T\sum_{i=1}^N \log p\!\big(s_i^{(t)}\mid \mathbf{s}_{\setminus i}^{(t)}\big) -\frac{\lambda_h}{2}\lVert h\rVert_2^2 -\frac{\lambda_J}{2}\lVert J\rVert_F^2,\) with \(J_{ii}=0,\quad J_{ij}=J_{ji}.\)</p> <p><strong>Gradients</strong> (for L‑BFGS/CG): \(\frac{\partial \mathcal{L}_{\mathrm{PL}}}{\partial h_i} =\sum_{t}\!\big[s_i^{(t)}-\tanh f_i^{(t)}\big]-\lambda_h h_i,\) \(\frac{\partial \mathcal{L}_{\mathrm{PL}}}{\partial J_{ij}} =\sum_{t}\!\big[s_i^{(t)}s_j^{(t)}-s_j^{(t)}\tanh f_i^{(t)}-s_i^{(t)}\tanh f_j^{(t)}\big]-\lambda_J J_{ij}.\)</p> <p><strong>Practical symmetry step.</strong> Fit \(N\) independent logistic regressions, then <strong>symmetrise</strong> \(J\): \(J_{ij}\leftarrow \tfrac{1}{2}\big(\hat\beta^{(i)}_j+\hat\beta^{(j)}_i\big),\qquad J_{ii}=0.\)</p> <hr/> <h2 id="2-variational-bayes-vb-pmem--gaussian-posterior-over-theta">2) Variational Bayes (VB) PMEM — Gaussian posterior over \(\theta\)</h2> <h3 id="a-prior-and-variational-family">(a) Prior and variational family</h3> <p>Stack \(\theta=[h;\, J_{i&lt;j}]\). Use zero‑mean Gaussian prior with separate precisions for fields and couplings: \(p(\theta)=\mathcal{N}\!\big(\theta\mid \theta_0,\ \Lambda_0^{-1}\big),\qquad \theta_0=\mathbf{0},\qquad \Lambda_0=\operatorname{diag}\!\big(\tau_h \mathbf{I}_N,\ \tau_J \mathbf{I}_{P-N}\big).\) Variational posterior: \(q(\theta)=\mathcal{N}\!\big(\theta\mid \mu,\ \Sigma\big).\)</p> <h3 id="b-quadratic-bound-on-the-logpartition">(b) Quadratic bound on the log‑partition</h3> <p>By convexity of \(A(\theta)\), for any anchor point \(\eta\) (majorisation parameter), \(A(\theta)\ \le\ A(\eta) + m_\eta^\top(\theta-\eta) +\tfrac{1}{2}(\theta-\eta)^\top C_\eta (\theta-\eta).\) This turns the intractable \(\mathbb{E}_q[A(\theta)]\) into a tractable quadratic form.</p> <h3 id="c-elbo-under-the-quadratic-bound">(c) ELBO under the quadratic bound</h3> <p>Let \(\mathcal{F}(\mu,\Sigma\mid\eta)\) denote the bound on the ELBO: \(\begin{aligned} \mathcal{F}(\mu,\Sigma\mid\eta) &amp;= T\Big[\mu^\top \bar{\Phi} - A(\eta) - m_\eta^\top(\mu-\eta) -\tfrac{1}{2}\operatorname{tr}(C_\eta \Sigma) -\tfrac{1}{2}(\mu-\eta)^\top C_\eta (\mu-\eta)\Big] \\ &amp;\quad -\tfrac{1}{2}\Big[(\mu-\theta_0)^\top \Lambda_0 (\mu-\theta_0) + \operatorname{tr}(\Lambda_0\Sigma)\Big] +\tfrac{1}{2}\log\!\det(2\pi e\,\Sigma). \end{aligned}\)</p> <h3 id="d-stationarypoint-updates-closed-form">(d) Stationary‑point updates (closed form)</h3> <p>Maximising \(\mathcal{F}\) w.r.t. \(\mu,\Sigma\) with \(\eta\) fixed gives \(\boxed{\ \ \Sigma^{-1} = \Lambda_0 + T\, C_\eta\ \ } \qquad \boxed{\ \ \mu = \theta_0 + \Sigma\, T\big(\bar{\Phi} - m_\eta\big)\ \ }.\) A convenient step form (with \(\theta_0=\mathbf{0}\)) is \(\mu \leftarrow \eta + \Sigma\, T\big(\bar{\Phi}-m_\eta\big).\) Then set \(\eta \leftarrow \mu\) and repeat (majorise‑minimise) until convergence.</p> <h3 id="e-moments-needed-in-d">(e) Moments needed in (d)</h3> <ul> <li> \[m_\eta=\mathbb{E}_{p_\eta}[\Phi] = \big[\langle s_i\rangle_\eta,\ \langle s_i s_j\rangle_\eta\big].\] </li> <li>\(C_\eta=\operatorname{Cov}_{p_\eta}(\Phi)\) (Fisher matrix).</li> </ul> <p>They can be obtained <strong>exactly</strong> by state enumeration for small \(N\), or <strong>approximately</strong> for larger \(N\) via Monte Carlo: \(\hat m_\eta=\frac{1}{M}\sum_{m=1}^M \Phi(\mathbf{s}^{[m]}),\qquad \hat C_\eta=\frac{1}{M-1}\sum_{m=1}^M \big(\Phi(\mathbf{s}^{[m]})-\hat m_\eta\big)\big(\Phi(\mathbf{s}^{[m]})-\hat m_\eta\big)^\top,\) where \(\mathbf{s}^{[m]} \sim p_\eta\) (e.g., Gibbs/Metropolis).</p> <h3 id="f-optional-ard--shrinkage-on-precisions">(f) Optional ARD / shrinkage on precisions</h3> <p>With Gamma hyperpriors \(\tau_h\sim\operatorname{Gamma}(a_h,b_h)\) and \(\tau_J\sim\operatorname{Gamma}(a_J,b_J)\) (type‑II ML / evidence updates), \(\tau_h \leftarrow \frac{N/2 + a_h - 1}{\tfrac{1}{2}\big(\lVert \mu_h\rVert_2^2 + \operatorname{tr}\Sigma_{hh}\big) + b_h},\qquad \tau_J \leftarrow \frac{(P-N)/2 + a_J - 1}{\tfrac{1}{2}\big(\lVert \mu_J\rVert_2^2 + \operatorname{tr}\Sigma_{JJ}\big) + b_J}.\)</p> <h3 id="g-convergence--diagnostics">(g) Convergence &amp; diagnostics</h3> <ul> <li>Relative change: \(\max\!\Big(\tfrac{\lVert \mu^{(t)}-\mu^{(t-1)}\rVert}{\lVert \mu^{(t-1)}\rVert},\ \tfrac{\lVert \Sigma^{(t)}-\Sigma^{(t-1)}\rVert_F}{\lVert \Sigma^{(t-1)}\rVert_F}\Big) &lt; \varepsilon\).</li> <li>Monotone ascent of \(\mathcal{F}(\mu,\Sigma\mid \eta)\) (re‑compute with \(\eta=\mu\)).</li> <li>Posterior standard deviations from \(\Sigma\) provide credible intervals for \(h\) and \(J\).</li> </ul> <h3 id="h-what-each-core-formula-captures-vb-intuition">(h) What each core formula captures (VB intuition)</h3> <ul> <li>\(\bar{\Phi}\) — the <strong>empirical</strong> first/second moments of the data.</li> <li>\(m_\eta\) — the <strong>model</strong> moments at the current parameter anchor; mismatch \(\bar{\Phi}-m_\eta\) drives the mean update.</li> <li>\(C_\eta\) — the curvature (Fisher) of the log‑partition; it <strong>tempers</strong> the update and sets the posterior covariance.</li> <li>\(\Sigma^{-1} = \Lambda_0 + T C_\eta\) — precision adds <strong>prior precision</strong> and <strong>data precision</strong> (information additivity).</li> <li>\(\mu = \theta_0 + \Sigma T(\bar{\Phi}-m_\eta)\) — mean moves in the direction that reduces <strong>moment mismatch</strong>.</li> <li>Hyper‑precisions \(\tau_h, \tau_J\) — control shrinkage of fields vs couplings (can be learned).</li> </ul> <h3 id="i-minimal-algorithm-majoriseminimise-vb">(i) Minimal algorithm (majorise‑minimise VB)</h3> <ol> <li>Initialise \(\mu\) (e.g., PL‑MAP), choose \(\Lambda_0\) (or \(\tau_h,\tau_J\)).</li> <li>Repeat: <ul> <li>Compute \(m_\eta, C_\eta\) at \(\eta=\mu\) (exact/MCMC/mean‑field).</li> <li>Update \(\Sigma^{-1}=\Lambda_0+T C_\eta\).</li> <li>Update \(\mu=\theta_0+\Sigma T(\bar{\Phi}-m_\eta)\) (or step form above).</li> <li>Optionally update \(\tau_h,\tau_J\); check ELBO and relative change.</li> </ul> </li> <li>Output \(q(\theta)=\mathcal{N}(\mu,\Sigma)\) and credible intervals.</li> </ol> </details>]]></content><author><name>Julian Kędys</name></author><category term="shared latent representations"/><category term="tutorial"/><category term="subject alignment"/><category term="neural state-spaces"/><category term="energy landscapes"/><category term="intersubject comparability"/><category term="state transitions"/><category term="interpretable descriptors"/><category term="brain dynamics"/><summary type="html"><![CDATA[Overview (TL;DR)]]></summary></entry><entry><title type="html">Understanding Adversarial Vulnerabilities and Emergent Patterns in Multimodal RL</title><link href="https://unireps.org//blog/2025/multimodalEmergingPatterns/" rel="alternate" type="text/html" title="Understanding Adversarial Vulnerabilities and Emergent Patterns in Multimodal RL"/><published>2025-10-08T00:00:00+00:00</published><updated>2025-10-08T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/multimodalEmergingPatterns</id><content type="html" xml:base="https://unireps.org//blog/2025/multimodalEmergingPatterns/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Deep learning systems were once tailored to a single input type, for instance RGB pixels in image models <d-cite key="resnet2015"></d-cite> or tokenized text for language generation <d-cite key="Radford2018ImprovingLU"></d-cite>. Over the last few years we have watched machine learning expand toward richer multimodal setups that fuse information sources <d-cite key="jiao-multimodal-survey"></d-cite>. Teams now blend sensors on autonomous platforms <d-cite key="panduru2025exploring"></d-cite>, translate content across formats such as image to text or text to speech, and pair visual and language prompts for both large language models <d-cite key="openai2024gpt4technicalreport"></d-cite> and image generators <d-cite key="rombach2022highresolutionimagesynthesislatent"></d-cite>. Multimodal development is vibrant and fast moving.</p> <p>With that momentum comes a pressing need to evaluate how these systems handle adversarial pressure. Every deep learning stack depends on trust, especially when it supports safety critical decisions. We must understand how attackers can exploit multimodal pipelines and how the interaction between modalities shapes new strengths and new points of failure. While single modality models have a long history of research on perturbations and data poisoning, the community still has only partial visibility into how those threats appear when modalities overlap.</p> <p>Live reinforcement learning agents raise the stakes even further. These policies operate within dynamic environments rather than static classification tasks, so defenses must respect timing, feedback, and control constraints. They also learn without labeled supervision, which complicates how we adapt classic adversarial tooling.</p> <p>Exploring the impact of these attacks on multimodal reinforcement learning is of critical importance because these agents also see use in high risk robotics and autonomous platforms. A brittle policy in those domains risks severe safety incidents and expensive hardware failures.</p> <p>Our study aims to map the interplay between adversarial attacks, defense strategies, and modality combinations on a baseline multimodal reinforcement learning agent. We document baseline performance and walk through empirical findings that show how different modality pairings shift behavior when attacked jointly or separately. We highlight how influence varies by modality and how defensive choices reshape those dynamics when interacting with different modalities.</p> <p>To support this investigation we extend open-source code provided by authors of DDiffPG to create and release a full pipeline to prepare datasets, train reference models, and evaluate attack and defense mixes across modalities.</p> <p>In this blog post we’ll talk about:</p> <ul> <li>Providing a testbed for adversarial evaluation of a multi-modal RL agent.</li> <li>Characterizing how attacking one or both modalities changes behavior.</li> <li>Showing that defenses introduce emergent patterns across modalities, sometimes improving robustness and sometimes destabilizing policies.</li> </ul> <h3 id="background--related-works">Background &amp; Related Works</h3> <p>Research on adversarial robustness has largely centered on language and vision systems, especially as large language models expanded into multimodal applications such as text to image and image to text experiences <d-cite key="wu2025dissectingadversarialrobustnessmultimodal,wang2025manipulatingmultimodalagentscrossmodal"></d-cite>. Investigations into decision making agents that blend multiple sensors remain comparatively sparse, with most of the attention directed toward autonomous driving stacks <d-cite key="chi_autonomous_survey2024,roheda2021multimodal"></d-cite>.</p> <p>As embodied agents gain capability and broader deployment, we need a clearer view of how combined modalities influence security. Building resilient multimodal pipelines is essential for maintaining trust in systems that operate in high risk settings with steadily increasing task complexity.</p> <h3 id="adversarial-attacks">Adversarial Attacks</h3> <p>Adversarial attacks are a branch of machine learning focused on manipulating model behavior in unintended or harmful ways. In particular, adversarial attacks are aimed at a “victim” model often designed with the flaws of a particular type of model or architecture in mind. These attacks typically involve introducing carefully designed “perturbations” to the input, which are intended to mislead or alter the model’s outputs. Depending on the attacker’s level of access to the internals of the model, attacks are classified as “white box” (full access, such as weights or gradient values), “gray box” (partial access, such as particular values or weights) or “black box” (no access at all, only inputs and outputs can be discerned around the black box). While perturbing inputs is the most common form of adversarial attack, other methods such as dataset “Poisoning” exist which alter training data to induce some desired behavior from the victim model.</p> <h3 id="adversarial-attacks-1">Adversarial Attacks</h3> <p>Adversarial attacks are a critical area of study in modern machine learning, focusing on how models can be manipulated into producing incorrect or harmful outputs. These attacks exploit weaknesses in model architectures or training processes by introducing subtle, carefully designed <em>perturbations</em> to the input data. Although these changes are often imperceptible to humans, they can drastically alter a model’s predictions, revealing vulnerabilities in even the most sophisticated AI systems.</p> <h4 id="types-of-adversarial-attacks">Types of Adversarial Attacks</h4> <p>Adversarial attacks are typically categorized based on the attacker’s level of access to the model’s internal information:</p> <p><strong>1. White-Box Attacks</strong><br/> In white-box attacks, the attacker has full access to the model’s internals, including parameters, gradients, and architecture details. This allows for precise and highly effective perturbations tailored to exploit specific weaknesses in the model.</p> <p><strong>2. Gray-Box Attacks</strong><br/> Gray-box attacks occur when the attacker has only partial access to the model. They may know certain weights, gradients, or architecture components, but not the entire system. These attacks are less direct than white-box methods but still capable of misleading models effectively.</p> <p><strong>3. Black-Box Attacks</strong><br/> In black-box attacks, the attacker has no insight into the model’s internal workings. They can only observe inputs and outputs, using this limited information to infer how to alter the input data. Black-box attacks often rely on query-based or transfer-based strategies to achieve success.</p> <h4 id="beyond-input-perturbations-data-poisoning">Beyond Input Perturbations: Data Poisoning</h4> <p>While most adversarial attacks occur during inference by altering input data, another powerful form of attack targets the <strong>training phase</strong> itself. Known as <em>data poisoning</em>, this method involves introducing manipulated samples into the training dataset. Over time, these poisoned samples bias the model’s learning process, leading to unintended or malicious behaviors once deployed.</p> <h4 id="importance-of-adversarial-robustness">Importance of Adversarial Robustness</h4> <p>As AI systems become increasingly integrated into critical domains such as healthcare, finance, and autonomous systems, ensuring their resilience to adversarial manipulation is essential. Developing models that can detect, resist, and adapt to these attacks is a cornerstone of building trustworthy, safe, and reliable machine learning applications.</p> <h3 id="adversarial-defenses">Adversarial Defenses</h3> <p>Defending machine learning models against adversarial attacks is a multifaceted challenge that requires both proactive and reactive strategies. Broadly, adversarial defenses can be divided into three primary categories:</p> <h4 id="1-adversarial-training">1. Adversarial Training</h4> <p>This approach involves <strong>augmenting the training process with adversarial samples</strong> <d-cite key="zizzo2021certified,tramer_adaptive_2020,kuzina_defending_2022"></d-cite>. By exposing the model to perturbed examples during training, it learns to recognize and resist similar manipulations at inference time. Adversarial training effectively strengthens the model’s decision boundaries, making it more robust against future attacks.</p> <h4 id="2-attack-and-anomaly-detection">2. Attack and Anomaly Detection</h4> <p>Another common line of defense focuses on <strong>detecting adversarial activity</strong> by identifying perturbations, abnormal patterns, or suspicious data points before they can affect model performance <d-cite key="roth_odds_2019,fidel_when_2020,guo_detecting_2019,GolchinAnomolyDetection"></d-cite>. These methods often rely on secondary classifiers, statistical models, or clustering techniques to flag inconsistencies between normal and adversarial inputs.</p> <h4 id="3-input-filtering-and-perturbation-removal">3. Input Filtering and Perturbation Removal</h4> <p>A third approach aims to <strong>remove or disrupt adversarial perturbations</strong> before the input reaches the model <d-cite key="zhang2021defense,nie_diffusion_2022,yoon_adversarial_2021"></d-cite>. Techniques in this category may apply noise injection, smoothing, or reconstruction mechanisms that effectively “clean” the input data, neutralizing the adversarial effect.</p> <h4 id="defense-methods-used-in-this-study">Defense Methods Used in This Study</h4> <p>In our experiments, we employ three specific defense mechanisms aligned with the categories above:</p> <ul> <li><strong>Disruption:</strong> Adding Gaussian noise to the input to reduce the impact of finely tuned perturbations.</li> <li><strong>Detection:</strong> Utilizing a neural network classifier and traditional clustering techniques to identify anomalous inputs.</li> <li><strong>Filtering:</strong> Applying a Variational Auto-Encoder (VAE) to reconstruct and denoise the input, effectively filtering out adversarial artifacts.</li> </ul> <p>Together, these methods provide complementary layers of protection, enhancing the overall robustness of the model against diverse forms of adversarial interference.</p> <h3 id="soft-actor-critic-models">Soft Actor-Critic Models</h3> <p><strong>Soft Actor-Critic (SAC)</strong> <d-cite key="haarnoja2018softactorcritic"></d-cite> is a reinforcement learning (RL) algorithm designed for continuous control tasks, such as the Ant-agent used in our <em>Ant-Maze</em> experiments. SAC combines two key components: an <strong>Actor</strong>, which represents the agent’s policy, and one or more <strong>Critics</strong>, which estimate value functions to guide learning.</p> <p>What makes SAC “soft” compared to traditional Actor-Critic methods is its inclusion of an <strong>entropy term</strong> in the objective function. This encourages the policy to remain more stochastic during training, promoting exploration rather than premature convergence to sub-optimal behaviors. In effect, SAC balances learning performance with policy diversity, achieving both <strong>stability</strong> and <strong>efficiency</strong> in complex, high-dimensional environments.</p> <h2 id="methodology">Methodology</h2> <h3 id="agent">Agent</h3> <p>To establish a baseline, we train a Soft Actor-Critic (SAC) agent <d-cite key="haarnoja2018softactorcritic"></d-cite> to control the MuJoCo Ant environment <d-cite key="todorov2012mujoco,towers2024gymnasium"></d-cite>. The Ant is a four-legged quadruped equipped with eight rotors, one at each joint, enabling coordinated movement across its four limbs, each composed of two interconnected links joined by a motorized joint.</p> <p>The agent’s observation space captures a comprehensive set of physical states to guide its behavior. It includes a velocity modality that records both linear and angular velocities (in meters and radians per second, respectively) for every limb, joint, and link of the Ant. Complementing this, an angular modality tracks joint angles in radians, covering the orientation between each limb link, the relative angles of the limbs to the torso, and the overall torso orientation.</p> <p>In addition, the agent receives a z-coordinate reading representing the torso’s height above the ground in meters. Notably, all observations are unbounded, formally defined within the continuous range (-∞, ∞), allowing unrestricted representation of the agent’s motion dynamics.</p> <p>The detailed observation space is outlined in the table below:</p> <table> <thead> <tr> <th>Num</th> <th>Observation</th> <th>Name</th> <th>Joint</th> <th>Unit</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>z-coordinate of the torso (centre)</td> <td>torso</td> <td>free</td> <td>position (m)</td> </tr> <tr> <td>1</td> <td>x-orientation of the torso (centre)</td> <td>torso</td> <td>free</td> <td>angle (rad)</td> </tr> <tr> <td>2</td> <td>y-orientation of the torso (centre)</td> <td>torso</td> <td>free</td> <td>angle (rad)</td> </tr> <tr> <td>3</td> <td>z-orientation of the torso (centre)</td> <td>torso</td> <td>free</td> <td>angle (rad)</td> </tr> <tr> <td>4</td> <td>w-orientation of the torso (centre)</td> <td>torso</td> <td>free</td> <td>angle (rad)</td> </tr> <tr> <td>5</td> <td>angle between torso and front left link on front left</td> <td>hip_1 (front_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>6</td> <td>angle between the two links on the front left</td> <td>ankle_1 (front_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>7</td> <td>angle between torso and front right link on front right</td> <td>hip_2 (front_right_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>8</td> <td>angle between the two links on the front right</td> <td>ankle_2 (front_right_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>9</td> <td>angle between torso and back left link on back left</td> <td>hip_3 (back_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>10</td> <td>angle between the two links on the back left</td> <td>ankle_3 (back_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>11</td> <td>angle between torso and back right link on back right</td> <td>hip_4 (right_back_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>12</td> <td>angle between the two links on the back right</td> <td>ankle_4 (right_back_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>13</td> <td>x-coordinate velocity of the torso</td> <td>torso</td> <td>free</td> <td>velocity (m/s)</td> </tr> <tr> <td>14</td> <td>y-coordinate velocity of the torso</td> <td>torso</td> <td>free</td> <td>velocity (m/s)</td> </tr> <tr> <td>15</td> <td>z-coordinate velocity of the torso</td> <td>torso</td> <td>free</td> <td>velocity (m/s)</td> </tr> <tr> <td>16</td> <td>x-coordinate angular velocity of the torso</td> <td>torso</td> <td>free</td> <td>angular velocity (rad/s)</td> </tr> <tr> <td>17</td> <td>y-coordinate angular velocity of the torso</td> <td>torso</td> <td>free</td> <td>angular velocity (rad/s)</td> </tr> <tr> <td>18</td> <td>z-coordinate angular velocity of the torso</td> <td>torso</td> <td>free</td> <td>angular velocity (rad/s)</td> </tr> <tr> <td>19</td> <td>angular velocity of the angle between torso and front left link</td> <td>hip_1 (front_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>20</td> <td>angular velocity of the angle between front left links</td> <td>ankle_1 (front_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>21</td> <td>angular velocity of the angle between torso and front right link</td> <td>hip_2 (front_right_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>22</td> <td>angular velocity of the angle between front right links</td> <td>ankle_2 (front_right_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>23</td> <td>angular velocity of the angle between torso and back left link</td> <td>hip_3 (back_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>24</td> <td>angular velocity of the angle between back left links</td> <td>ankle_3 (back_left_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>25</td> <td>angular velocity of the angle between torso and back right link</td> <td>hip_4 (right_back_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> <tr> <td>26</td> <td>angular velocity of the angle between back right links</td> <td>ankle_4 (right_back_leg)</td> <td>hinge</td> <td>angle (rad)</td> </tr> </tbody> </table> <p><em>Table: Observations space for the MuJoCo Ant-Maze task. Ranges for all values are -Inf to +Inf.</em></p> <p>The action space consists of 8 torque values applied to the rotors:</p> <table> <thead> <tr> <th>Num</th> <th>Action</th> <th>Name</th> <th>Joint</th> <th>Type (Unit)</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>Torque applied on the rotor between the torso and back right hip</td> <td>hip_4 (right_back_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>1</td> <td>Torque applied on the rotor between the back right two links</td> <td>angle_4 (right_back_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>2</td> <td>Torque applied on the rotor between the torso and front left hip</td> <td>hip_1 (front_left_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>3</td> <td>Torque applied on the rotor between the front left two links</td> <td>angle_1 (front_left_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>4</td> <td>Torque applied on the rotor between the torso and front right hip</td> <td>hip_2 (front_right_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>5</td> <td>Torque applied on the rotor between the front right two links</td> <td>angle_2 (front_right_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>6</td> <td>Torque applied on the rotor between the torso and back left hip</td> <td>hip_3 (back_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> <tr> <td>7</td> <td>Torque applied on the rotor between the back left two links</td> <td>angle_3 (back_leg)</td> <td>hinge</td> <td>torque (N m)</td> </tr> </tbody> </table> <p><em>Table: MuJoCo Ant agent action space. All values range between [-1, 1].</em></p> <p>Below is an illustration of the body and rotor layout used in our experiments:</p> <div class="l-body-outset"> <div class="row"> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/example_ant.png" alt="Example Mujoco Ant body." style="width: 100%; max-width: 300px;"/> <figcaption>Example MuJoCo Ant body <d-cite key="towers2024gymnasium"></d-cite>.</figcaption> </figure> </div> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/ant_joints.png" alt="Rotor layout for Ant." style="width: 100%; max-width: 300px;"/> <figcaption>Rotor layout for Ant <d-cite key="towers2024gymnasium"></d-cite>.</figcaption> </figure> </div> </div> </div> <h3 id="environment">Environment</h3> <p>In addition to training the agent to embody its quadruped ant, we train it for the AI Gymnasium task “Ant Maze” <d-cite key="towers2024gymnasium"></d-cite>. This task requires the agent to learn how to walk with its body then utilize its learned movements to maneuver around obstacles to reach a predetermined destination. This can be seen in pathing and exploration heat map figures throughout this paper.</p> <h3 id="adversarial-attack">Adversarial Attack</h3> <p>We rely on the Fast Gradient Sign Method (FGSM) <d-cite key="goodfellow2015explaining"></d-cite> as the baseline attack across our experiments. FGSM is a white box technique that perturbs inputs using the sign of the gradient from the victim model. The standard form is:</p> \[\mathbf{x}_{\text{adv}} = \mathbf{x} + \epsilon\,\mathrm{sign}\big(\nabla_{\mathbf{x}} J(\theta,\mathbf{x},y)\big)\] <ul> <li><strong>$\mathbf{x}$</strong>: The original input provided to the model.</li> <li><strong>$y$</strong>: The true (ground-truth) label associated with the input.</li> <li><strong>$\epsilon$</strong>: A small scalar value that controls the magnitude of the perturbation applied to the input.</li> <li><strong>$J(\theta, \mathbf{x}, y)$</strong>: The loss function, parameterized by model weights $\theta$, input $\mathbf{x}$, and label $y$.</li> <li><strong>$\nabla_{\mathbf{x}} J(\theta, \mathbf{x}, y)$</strong>: The gradient of the loss function with respect to the input, indicating how changes in $\mathbf{x}$ affect the loss.</li> <li><strong>$\mathrm{sign}(\cdot)$</strong>: The element-wise sign function that extracts the direction (positive or negative) of each gradient component.</li> <li><strong>$\mathbf{x}_{\text{adv}}$</strong>: The resulting adversarial example, formed by perturbing the original input $\mathbf{x}$ to maximize the model’s prediction error.</li> </ul> <h4 id="modifying-fgsm">Modifying FGSM</h4> <p>FGSM is typically employed as an attack against image classifiers and perturbations are often in the form of pixel values. To get this to work in our case, we have to apply similar perturbations in the vector form matching our observation space. So we modify perturbations to represent values relative to which modality is being targeted (such as radians or meters per second).</p> <p>The <strong>Fast Gradient Sign Method (FGSM)</strong> is traditionally designed for <strong>supervised learning</strong> tasks, particularly in classification, where each input is paired with a corresponding label. In such cases, the attack relies on the loss function, which depends on the model parameters, input, and true label. However, applying FGSM directly to <strong>reinforcement learning (RL)</strong> models presents a unique challenge, as RL agents often don’t use labeled data.</p> <p>To address this, we reformulate FGSM in the following way. Instead of using a label-based loss, we leverage the <strong>critic’s Q-value</strong> as the optimization target. The modified FGSM equation is expressed as:</p> \[\mathbf{x}_{\text{adv}} = \mathbf{x} + \epsilon\,\mathrm{sign}\big(\nabla_{\mathbf{x}} Q_{\theta}(\mathbf{x}, a)\big)\] <p>Here, the <strong>Q-function</strong> from the critic network replaces the conventional loss function. This adaptation enables FGSM to generate adversarial perturbations that specifically target the <strong>policy’s valuation process</strong>, influencing how the agent perceives the consequences of its actions. By perturbing inputs based on the critic’s gradients, we effectively reorient the attack for any unsupervised evaluation the critic is capable of.</p> <h3 id="adversarial-defenses-1">Adversarial Defenses</h3> <p>Our evaluation includes three defense themes. We start with a disruption baseline that applies scaled gaussian noise to the observation vector. We then explore adversarial detection by training classifiers to flag perturbed observations and compare them with traditional clustering tools such as K Means and Gaussian Mixture Models (GMM). Finally, we assess a purification pipeline built on a defense VAE that reconstructs benign versions of the inputs.</p> <p>Both the neural network detectors and the clustering approaches rely on a dataset collected during SAC training and evaluation. The agent runs for three million steps. We treat the first one point five million steps as a warm up phase to avoid logging data from an agent that has not yet learned to move reliably. During the final 1.5 million steps we record observations into a benign dataset. We also generate a matching adversarial dataset by applying FGSM perturbations to those observations without feeding the altered signals back to the agent. The result is a paired corpus of benign and adversarial samples for every modality.</p> <h4 id="gaussian-noise-defense">Gaussian Noise Defense</h4> <p>The gaussian noise filter serves as the baseline defense. We sample perturbations from a normal distribution and scale them by $\epsilon$:</p> \[\mathbf{x_{def}} = x + \epsilon \cdot \mathbf{n}, \quad \text{where } \mathbf{n} \sim \mathcal{N}(0, \mathbf{I})\] <p>Targeted adversarial perturbations are often sensitive to small changes. By injecting a modest level of noise we can disrupt their structure and blunt the attack, accepting some degradation from the added randomness. This approach is simple and computationally inexpensive.</p> <h4 id="defense-vae">Defense VAE</h4> <p>Our defense VAE follows prior work on variational autoencoder purification <d-cite key="li2019defensevaefastaccuratedefense,shayanNDVAE"></d-cite>. The model trains on paired benign and adversarial samples like those described above. The encoder observes both forms, while the decoder learns to reconstruct the benign target. Over time the encode decode pathway maps adversarial inputs back into the benign observation space.</p> <p>We adapt the architecture from image defenses to match our one dimensional observation vector. The compact network uses four fully connected layers with ReLU activation rather than convolutional stacks. Since the modalities are not arranged as a sequence with positional structure, we also avoid one dimensional convolutions.</p> <h4 id="adversarial-detection">Adversarial Detection</h4> <p>Detection centric defenses focus on identifying an attack rather than intervening directly in the control loop. We therefore evaluate them on prediction accuracy and F1 score but do not alter the agent mid run. A production system could take many actions once an attack is detected, yet that follow up is outside the scope of this study.</p> <p>Using the labeled dataset we train Support Vector Machines (SVM), K Nearest Neighbors, and neural network classifiers to distinguish benign from adversarial observations. For comparison we also fit simpler clustering approaches such as K means and Gaussian Mixture Models to the same data.</p> <p>Classifier accuracy reflects binary predictions on benign versus adversarial inputs. For the unsupervised clustering methods we assign cluster labels to maximize accuracy after fitting the two cluster model.</p> <h3 id="adversarial-evaluation">Adversarial Evaluation</h3> <p>We use the following process to test the effects of adversarial attacks on different modalities of our baseline agent:</p> <ol> <li>The agent is trained normally on benign inputs.</li> <li>Every evaluation episode, the input is perturbed with an attack: First across both modalities simultaneously, then focused on only target modalities in the agent’s observation.</li> <li>Compare adversarial evaluation to benign training performance and identify effects of the attack on the agent.</li> </ol> <p>To better understand how defenses influence model performance, each defense method is tested under two different setups. In the first setup, the model is trained normally, and the defense is applied only during evaluation (filtering or otherwise “Defending” the input before it reaches the model). In the second setup, the model is also exposed to the defense during training by processing benign inputs through the same method before learning. The model is then attacked in the usual way. Comparing these two configurations reveals how much model tuning contributes to each defense’s effectiveness.</p> <h2 id="results">Results</h2> <h3 id="baseline-performance">Baseline Performance</h3> <p>We begin by training an SAC agent on the Ant Maze task for three million steps so that it reliably clears a simple obstacle. The configuration mirrors common SAC settings with a replay buffer of one million transitions, $\tau=0.05$, and $\gamma=0.99$.</p> <p>The figures below trace the training story. Rewards rise and level off after the run, the exploration heatmap shows how the agent learns an efficient route, and evaluation rewards peak once the policy stabilizes. The path visual confirms that the agent reaches the goal consistently.</p> <p>With no adversarial pressure this baseline agent handles the maze it was trained on.</p> <div class="l-page-outset"> <div class="row"> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/baseline_train.png" alt="SAC training reward." style="width: 100%; max-width: 400px;"/> <figcaption>Training reward progression (3M steps).</figcaption> </figure> </div> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/baseline_explore.png" alt="Exploration heatmap." style="width: 100%; max-width: 400px;"/> <figcaption>Exploration heatmap during learning.</figcaption> </figure> </div> </div> </div> <div class="l-page-outset"> <div class="row"> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/baseline_eval.png" alt="Evaluation reward." style="width: 100%; max-width: 400px;"/> <figcaption>Evaluation performance plateaus after training.</figcaption> </figure> </div> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/baseline_paths.png" alt="Paths to goal." style="width: 100%; max-width: 400px;"/> <figcaption>Typical paths from start to goal after training.</figcaption> </figure> </div> </div> </div> <h3 id="attacking-an-un-defended-model">Attacking an Un-Defended Model</h3> <p>With the modified FGSM attack ($\epsilon=0.005$) we perturb the observation vector during evaluation. The attack seeks the lowest value outcome predicted by the critic, pushing the policy toward poor decisions.</p> <h4 id="attacking-both-modalities">Attacking Both Modalities</h4> <p>When we perturb both modalities, reward traces reveal the story immediately. Sharp drops in the purple line show successful attacks, while quick recoveries indicate attack attempts that did not succeed. The path comparisons illustrate what those swings look like in the environment: a failed attack nudges the agent onto an alternate route, whereas a successful one leaves the agent wandering near the start.</p> <h4 id="attacking-individual-modalities">Attacking Individual Modalities</h4> <p>We then target each modality on its own with FGSM. The performance plot highlights how velocity (green) and angle (blue) perturbations differ from the full multimodal attack. Velocity covers roughly one quarter of the observation vector, so those attacks fail more often and produce more consistant reward. Angle perturbations cover more of the observation and are proportionally more effective.</p> <p>These results reinforce a practical takeaway: the share of the observation space tied to a modality limits how much damage an attacker can cause when only that modality is manipulated. However this has potential to change on different systems, if an agent is severely biased towards using a single modality.</p> <figure class="l-page"> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/fgsm_eval.png" alt="FGSM evaluation performance." style="width: 100%; max-width: 600px;"/> <figcaption>Benign (red) vs adversarial (purple) evaluation. Sharp reward drops indicate successful attacks.</figcaption> </figure> <div class="l-page-outset"> <div class="row"> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/fgsm_path_success.png" alt="FGSM success example." style="width: 100%; max-width: 350px;"/> <figcaption>FGSM attack success: agent gets lost early.</figcaption> </figure> </div> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/fgsm_path_fail.png" alt="FGSM failure example." style="width: 100%; max-width: 350px;"/> <figcaption>FGSM attack failure: minimal route alteration.</figcaption> </figure> </div> </div> </div> <p>Modality-specific FGSM shows that attack effectiveness scales with the attacked fraction of the observation space.</p> <figure class="l-page"> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/individual_modality_fgsm.png" alt="Modality-specific FGSM results." style="width: 100%; max-width: 600px;"/> <figcaption>Performance under FGSM for both modalities (purple), angles-only (blue), and velocity-only (green).</figcaption> </figure> <h3 id="attacking-a-defended-model">Attacking a Defended Model</h3> <p>With the baseline established, we introduce defenses to see how they reshape the interaction between attacker and agent.</p> <h4 id="gaussian-noise-defense-1">Gaussian Noise Defense</h4> <p>The gaussian noise filter is the simplest option. We add noise drawn from a normal distribution with mean zero and standard deviation one, scaled by 0.005.</p> <p>As shown in the noise defense results, even small amounts of injected noise can significantly disrupt the adversarial attack, leading to higher rewards during evaluation. Training the model with noise further improves its resilience, helping it adapt to the presence of randomness and reducing the attack’s overall success rate. In this setup, we evaluate two conditions: one where the model encounters noise only during evaluation, and another where it is pre-trained on noisy data.</p> <p>Both approaches, training with noise (orange line) and applying noise only during evaluation (red line), show clear improvements compared to the undefended model (purple line). The trained model in particular exhibits more frequent reward peaks and fewer sharp drops, indicating that controlled noise not only weakens the attacker’s influence but also enhances the model’s overall stability under adversarial conditions.</p> <p>When applying Gaussian noise defenses to attacks targeting specific modalities, an interesting shift in behavior emerges. For velocity-targeted attacks, the results closely mirror those of the full multimodal noise defense, showing that noise effectively mitigates the attack’s impact. However, when the angular modality is targeted, the pattern changes noticeably.</p> <p>In this case, training the model on noisy angular data appears to destabilize performance rather than improve it. The angular noise defense results show that the best-performing configuration occurs when noise is introduced only during evaluation, without exposing the model to it during training. The reason for this discrepancy remains unclear, it may be related to the angular modality’s dominant influence within the observation space, or potentially other underlying interactions in the model’s learning dynamics. Regardless, this behavior was observed consistently across multiple runs, suggesting a modality-specific sensitivity to noise based defenses.</p> <div class="l-page-outset"> <div class="row"> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/noise_defense_fgsm.png" alt="Noise defense under multimodal FGSM." style="width: 100%; max-width: 400px;"/> <figcaption>Defense only (red), trained with noise (orange), undefended (purple).</figcaption> </figure> </div> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/angular_noise_defense_fgsm.png" alt="Noise defense for angle-only attacks." style="width: 100%; max-width: 400px;"/> <figcaption>Angle-only attacks: defense at eval (purple) outperforms training-through-noise (green) and baseline (blue).</figcaption> </figure> </div> </div> </div> <h4 id="defense-vae-1">Defense-VAE</h4> <p>The compact Defense VAE offers another perspective. Running it only during evaluation lifts rewards under FGSM, but training the agent on VAE filtered inputs prevents the policy from solving the task. Single modality attacks expose its weaknesses: the VAE performs best when both modalities are perturbed together (as seen with the blue line in the figure below) and struggles with narrow attacks.</p> <p>This pattern alligns with how the VAE learned. Training on paired benign and adversarial samples covering the entire observation space primes it to recognize broad perturbations. Once the attacker focuses on a single modality the reconstruction no longer matches the training distribution, so protection fades.</p> <div class="l-page-outset"> <div class="row"> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/vae_both_modalities.png" alt="VAE defense under multimodal FGSM." style="width: 100%; max-width: 400px;"/> <figcaption>VAE defense (blue) vs undefended adversarial inputs (purple) at $\epsilon=0.007$.</figcaption> </figure> </div> <div class="col-sm-6"> <figure> <img src="/blog/assets/img/2025-10-08-multimodalEmergingPatterns/vae_individual_modalities.png" alt="VAE defense under modality-specific FGSM." style="width: 100%; max-width: 400px;"/> <figcaption>VAE under both-modalities (blue), velocity-only (red), angle-only (orange) attacks.</figcaption> </figure> </div> </div> </div> <h4 id="detection-results-summary">Detection Results (Summary)</h4> <p>The classifier roundup shows a clear trend: higher $\epsilon$ values (stronger perturbations) make detection easier. Angular perturbations are more detectable in general, and stand out at $\epsilon=0.007$, while velocity attacks become more visible at $\epsilon=0.015$. However clustering baselines such as K means and GMM hover near chance overall.</p> <p>The broader lesson is that modality combinations shift detection difficulty, and scaling the attack changes that balance even when success rates stay similar.</p> <p>The detailed results for all detection methods are shown in the table below:</p> <table> <thead> <tr> <th>Detection Method</th> <th>Multi-modal</th> <th> </th> <th> </th> <th>Velocity</th> <th> </th> <th> </th> <th>Angular</th> <th> </th> <th> </th> </tr> </thead> <tbody> <tr> <td> </td> <td>Accuracy</td> <td>F1 Score</td> <td>Epsilon</td> <td>Accuracy</td> <td>F1 Score</td> <td>Epsilon</td> <td>Accuracy</td> <td>F1 Score</td> <td>Epsilon</td> </tr> <tr> <td>SVM</td> <td>0.6027</td> <td>0.6936</td> <td>0.007</td> <td>0.4993</td> <td>0.6227</td> <td>0.007</td> <td>0.5606</td> <td><strong>0.6596</strong></td> <td>0.007</td> </tr> <tr> <td>SVM</td> <td>0.7188</td> <td>0.7608</td> <td>0.015</td> <td>0.6070</td> <td>0.7012</td> <td>0.015</td> <td>0.5048</td> <td>0.3782</td> <td>0.015</td> </tr> <tr> <td>KNN</td> <td>0.5909</td> <td>0.5897</td> <td>0.007</td> <td>0.5164</td> <td>0.4718</td> <td>0.007</td> <td>0.5391</td> <td>0.5412</td> <td>0.007</td> </tr> <tr> <td>KNN</td> <td>0.6606</td> <td>0.6522</td> <td>0.015</td> <td>0.5850</td> <td>0.5769</td> <td>0.015</td> <td>0.5152</td> <td>0.3962</td> <td>0.015</td> </tr> <tr> <td>NN</td> <td>0.7349</td> <td>0.725</td> <td>0.007</td> <td>0.5494</td> <td>0.6437</td> <td>0.007</td> <td>0.7104</td> <td>0.7372</td> <td>0.007</td> </tr> <tr> <td>NN</td> <td><strong>0.9892</strong></td> <td><strong>0.9892</strong></td> <td>0.015</td> <td><strong>0.8766</strong></td> <td><strong>0.8810</strong></td> <td>0.015</td> <td><strong>0.8211</strong></td> <td>0.8264</td> <td>0.015</td> </tr> <tr> <td>GMM</td> <td>0.4989</td> <td>N/A</td> <td>0.007</td> <td>0.4984</td> <td>N/A</td> <td>0.007</td> <td>0.4996</td> <td>N/A</td> <td>0.007</td> </tr> <tr> <td>GMM</td> <td>0.5094</td> <td>N/A</td> <td>0.015</td> <td>0.4976</td> <td>N/A</td> <td>0.015</td> <td>0.5021</td> <td>N/A</td> <td>0.015</td> </tr> <tr> <td>Kmeans</td> <td>0.5033</td> <td>N/A</td> <td>0.007</td> <td>0.4998</td> <td>N/A</td> <td>0.007</td> <td>0.5026</td> <td>N/A</td> <td>0.007</td> </tr> <tr> <td>Kmeans</td> <td>0.5416</td> <td>N/A</td> <td>0.015</td> <td>0.4602</td> <td>N/A</td> <td>0.015</td> <td>0.4855</td> <td>N/A</td> <td>0.015</td> </tr> </tbody> </table> <p><em>Table: Classifier performance across modalities with FGSM epsilon (scaling) values of 0.007 and 0.015.</em></p> <h2 id="conclusions">Conclusions</h2> <p>Our experiments confirm that a multimodal agent can be pushed off course through attacks on any of its inputs. The larger a modality’s footprint in the observation vector, the more influence an attacker gains. Defenses shape those dynamics in different ways: gaussian noise offers broad value with modality specific caveats, and the Defense VAE excels when the attack distribution matches its training data, even though the VAE is supposed to generalize well against new attack types/methods, it does not generalize as well across modalities.</p> <p>These findings underline the importance of viewing reinforcement learning robustness through a modality aware lens. Simple interventions already reveal nuanced behavior, and the pipeline we release provides a starting point for deeper explorations.</p> <p>The baseline setup here is intentionally lightweight, yet it highlights clear research directions for more complex agents and environments. We hope the insights and tools accelerate progress on safeguarding the next wave of multimodal systems.</p> <h2 id="reproducibility-details">Reproducibility Details</h2> <p>Experiments used SAC with 3M steps, replay size 1e6, $\tau=0.05$, $\gamma=0.99$, evaluation every 100 steps, and Adam learning rates as in our code. Defense-VAE used fully connected enc/dec layers (256-128-64 latent-64-128-256) with latent size 24, trained for 50 epochs. Classifier details and additional hyperparameters are available in the appendix of the paper and our codebase.</p> <h3 id="sac-hyperparameters">SAC Hyperparameters</h3> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Horizon Length</td> <td>1</td> </tr> <tr> <td>Memory Size</td> <td>$1 \times 10^6$</td> </tr> <tr> <td>Batch Size</td> <td>4096</td> </tr> <tr> <td>$N$-step</td> <td>1</td> </tr> <tr> <td>$\tau$ (Target smoothing coefficient)</td> <td>0.05</td> </tr> <tr> <td>$\gamma$ (Discount factor)</td> <td>0.99</td> </tr> <tr> <td>Warm-up Steps</td> <td>32</td> </tr> <tr> <td>Critic Class</td> <td>Double Q-Network</td> </tr> <tr> <td>Evaluation Frequency</td> <td>100</td> </tr> <tr> <td>Learning Rate (Alpha)</td> <td>0.005</td> </tr> <tr> <td>Update Times per Step</td> <td>8</td> </tr> </tbody> </table> <h3 id="vae-hyperparameters">VAE Hyperparameters</h3> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Optimizer</td> <td>Adam</td> </tr> <tr> <td>Learning Rate</td> <td>$1 \times 10^{-3}$</td> </tr> <tr> <td>Batch Size</td> <td>32</td> </tr> <tr> <td>Training Epochs</td> <td>50</td> </tr> <tr> <td>Latent Dimension</td> <td>24</td> </tr> <tr> <td>Encoder Layers</td> <td>5 (Observation, 256, 128, 64, Latent Dimension)</td> </tr> <tr> <td>Decoder Layers</td> <td>5 (Latent Dimension, 64, 128, 256, Observation)</td> </tr> </tbody> </table> <h3 id="neural-network-hyperparameters">Neural Network Hyperparameters</h3> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Input Dimension</td> <td>28</td> </tr> <tr> <td>Output Dimension</td> <td>1</td> </tr> <tr> <td>Hidden Layers</td> <td>5 (256, 256, 128, 32, 1)</td> </tr> <tr> <td>Learning Rate</td> <td>0.0001</td> </tr> <tr> <td>Training Epochs</td> <td>60</td> </tr> </tbody> </table> <h3 id="support-vector-machine-svm-hyperparameters">Support Vector Machine (SVM) Hyperparameters</h3> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>$C$</td> <td>1000</td> </tr> <tr> <td>Gamma</td> <td>1</td> </tr> <tr> <td>Degree</td> <td>3</td> </tr> <tr> <td>Decision Function</td> <td>One-vs-One (ovo)</td> </tr> </tbody> </table> <h3 id="gmm-hyperparameters">GMM Hyperparameters</h3> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Number of Components</td> <td>2</td> </tr> <tr> <td>Initialization Method</td> <td>k-means++</td> </tr> <tr> <td>Covariance Type</td> <td>full</td> </tr> <tr> <td>Convergence Tolerance ($\texttt{tol}$)</td> <td>0.001</td> </tr> <tr> <td>Regularization of Covariance ($\texttt{reg_covar}$)</td> <td>$1 \times 10^{-6}$</td> </tr> <tr> <td>Max Iterations</td> <td>100</td> </tr> <tr> <td>Number of Initializations ($\texttt{n_init}$)</td> <td>1</td> </tr> </tbody> </table> <h3 id="k-means">K-Means</h3> <p>K-Means clustering algorithm as implemented by Scikit-learn, with a k value of 2 clusters.</p> <d-appendix> <d-footnote> We release a lightweight pipeline adapted from DDiffPG <d-cite key="li2024learningmultimodalbehaviorsscratch"></d-cite> to reproduce training, dataset generation, attacks, and defenses. </d-footnote> </d-appendix>]]></content><author><name>Shayan Jalalipour</name></author><category term="Adversarial"/><category term="Robustness,"/><category term="Reinforcement"/><category term="Learning,"/><category term="Multimodal"/><category term="models"/><summary type="html"><![CDATA[Using a simplified multimodal RL agent to explore adversarial vulnerabilities that emerge when using different modalities.]]></summary></entry><entry><title type="html">Separatrix Locator</title><link href="https://unireps.org//blog/2025/Separatrix-Locator/" rel="alternate" type="text/html" title="Separatrix Locator"/><published>2025-09-28T00:00:00+00:00</published><updated>2025-09-28T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/Separatrix-Locator</id><content type="html" xml:base="https://unireps.org//blog/2025/Separatrix-Locator/"><![CDATA[<h2 id="tldr">TL;DR</h2> <p>Separatrices! These are boundaries between basins of attraction in dynamical systems. In high-dimensional systems like Recurrent Neural Networks, finding these boundaries can help reverse engineer their mechanism, or design optimal perturbations. But finding them is far from trivial. We recently developed a numerical method, based on approximating a Koopman Eigenfunction (KEF) of the dynamics using a deep neural network (DNN) <d-cite key="dabholkar_finding_2025"></d-cite>. While this approach works, these KEFs suffer from singularities at attractors, which makes them difficult targets for DNNs to approximate. In this blogpost we explain our original method, and also improve it by using a variant we call <em>squashed Koopman Eigenfunctions</em> (sKEFs), which alleviate the singularities. We show how they are linked to KEFs and replicate our results from the paper.</p> <p><strong>Code</strong>: We provide a Python package implementing this method at <a href="https://github.com/KabirDabholkar/separatrix_locator">github.com/KabirDabholkar/separatrix_locator</a>.</p> <h2 id="introduction-fixed-points-and-beyond">Introduction: Fixed Points and Beyond</h2> <p>Many natural and artificial systems — from neural circuits making decisions to ecosystems switching between healthy and diseased states — are modelled as <strong>multistable dynamical systems</strong>. Their behaviour is governed by multiple <strong>attractors</strong> in state space, each corresponding to a stable mode of activity. Understanding these systems often boils down to understanding their <strong>geometry</strong>: where are the stable states, and how are the different basins of attraction separated?</p> <p>For the last decade, a workhorse of neural circuit analysis has been <strong>fixed point analysis</strong>. By finding points where the flow vanishes and linearising around them, one can uncover local motifs underlying computation: line attractors, saddle points, limit cycles, and so on. This has yielded powerful insights into how trained RNNs implement cognitive tasks.</p> <h3 id="finding-fixed-points">Finding Fixed Points</h3> <p>First consider a bistable dynamical system in 2 dimensions. Below is a phase-portrait of such a system, with two stable fixed points and one unstable fixed point. Click on plot to realise trajectories of the dynamics.</p> <div class="l-body" style="text-align: center; margin: 2rem 0;"> <iframe src="/blog/assets/html/2025-09-29-Separatrix-Locator/clickable_phase_portrait_simple.html" scrolling="no" style="width: 80%; height: 400px; border: none; border-radius: 8px; overflow: hidden;"> </iframe> </div> <p>Trajectories converge to either one of the two fixed points. This naturally suggests an algorithm: run the dynamics from many initial conditions to find the stable fixed points.</p> <p>Now try to click somewhere that will lead you exactly to the saddle point. Did you succeed? It’s almost impossible.</p> <p>This motivates developing a principled way to find such points. One solution is to define a specific scalar function of the dynamics whose only minima are given by all the fixed points. One such function is the kinetic energy \(q(\boldsymbol x)=\frac{1}{2}\Vert f(\boldsymbol x)\Vert^2\) <d-cite key="sussillo_opening_2013,golub_fixedpointfinder_2018"></d-cite>. By differentiating this function, one can perform gradient descent to find these minima. The interactive plot below realises such trajectories.</p> <div class="l-body" style="text-align: center; margin: 2rem 0;"> <iframe src="/blog/assets/html/2025-09-29-Separatrix-Locator/gradient_descent_phase_portrait.html" scrolling="no" style="width: 80%; height: 400px; border: none; border-radius: 8px; overflow: hidden;"> </iframe> </div> <p>Now we can find both stable <em>and unstable</em> fixed points. Linearising around the fixed points provides an interpretable approximation of the dynamics in the neighbourhood of those points. Several works adopt this approach of fixed point finding to reverse-engineer either task-trained or data-trained RNNs <d-cite key="carnevale_dynamic_2015,maheswaranathan_reverse_2019,maheswaranathan_universality_2019,finkelstein_attractor_2021,mante_context-dependent_2013,liu_encoding_2024,driscoll_flexible_2024,chaisangmongkon_computing_2017,jaffe_modelling_2023,pagan_individual_2025,wang_flexible_2018"></d-cite>.</p> <p>But fixed points are only half the story.</p> <p>When a system receives a perturbation — for example, a sensory input or an optogenetic pulse — the key question is often not <em>where</em> it started, but <em>which side of the separatrix it ends up on</em>. The <strong>separatrix</strong> is the boundary in state space separating different basins of attraction. Crossing it means switching decisions, memories, or ecological states. Failing to cross means staying put. For high-dimensional systems, these boundaries are typically <strong>nonlinear, curved hypersurfaces</strong>, invisible to fixed points and local linearisations around them.</p> <blockquote> <p><strong>What if we could learn a single smooth scalar function whose zero level set <em>is</em> the separatrix?</strong></p> </blockquote> <p>Below is an example of such a function that we constructed for this simple system (click on it to run trajectories).</p> <div class="l-body" style="text-align: center; margin: 2rem 0;"> <iframe src="/blog/assets/html/2025-09-29-Separatrix-Locator/absolute_value_gradient_descent.html" scrolling="no" style="width: 80%; height: 400px; border: none; border-radius: 8px; overflow: hidden;"> </iframe> </div> <p>Our main contribution is a numerical method to approximate such functions using deep neural networks in order to find separatrices in multistable dynamical systems in high-dimensions.</p> <h3 id="setting">Setting</h3> <p>We consider autonomous dynamical flows of the form:</p> \[\begin{equation} \dot{\boldsymbol{x}} = f(\boldsymbol{x}) \label{eq:ODE} \end{equation}\] <p>governing the state \(\boldsymbol{x} \in \mathbb R^N\), where \(\dot \square\) is shorthand for the time derivative \(\frac{d}{dt}\square\) and \(f: \mathbb R^N \to \mathbb R^N\) defines the dynamical flow.</p> <blockquote class="goal-box" id="goal"> <p><strong>The Goal:</strong><br/> Find a smooth scalar function \(\psi:\mathbb{R}^N\to\mathbb{R}\) that grows as we move away from the separatix, i.e., \(\psi(\boldsymbol x)=0\) for \(x\in\text{separatrix}\) and grows as it moves away from the separatrix.</p> </blockquote> <h2 id="the-sandwich-of-bistability">The Sandwich of Bistability</h2> <p>Any bistable system can be decomposed as follows: it will have two attractors, their respective basins of attraction and the separatrix between them. This is like a cheese sandwich: the attractors are slices of bread, and the separatrix is the slice of cheese between them. We can call this the <strong>Sandwich of Bistability</strong>. In general, this sandwich could be arbitrarily oriented in \(\mathbb R^N\) and even nonlinearly warped.</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-09-29-Separatrix-Locator/sandwich_of_bistability.png" alt="Sandwich of Bistability" width="500"/> <div style="max-width: 500px; margin: 0.5rem auto; text-align: center;"> <em>The Sandwich of Bistability: Two attractors and their basins of attraction (bread slices) separated by a separatrix (cheese slice). We only care about mapping the coordinates along bistable axis.</em> </div> </div> <p>With our scalar function \(\psi:\mathbb{R}^N\to\mathbb{R}\) we would like to perform a special type of dimensionality reduction: we only care to identify our location along the attractor–separatrix–attractor axis, i.e., along the depth of sandwich.</p> <p>One way to achieve this is to have this scalar observable \(\psi(\boldsymbol x)\) <em>imitate</em> the bistable dynamics along this axis. Thus we pick a simple example of bistable dynamics in 1D (hover your cursor over it to see the plot):</p> <div class="equation-with-plot"> $$ \begin{equation} \dot \psi = \lambda (\psi-\psi^3) \label{eq:sKEFsimple} \end{equation} $$ <div class="plot-tooltip"> <iframe src="/blog/assets/html/2025-09-29-Separatrix-Locator/bistable_1d_plot.html" scrolling="no"></iframe> </div> </div> <p>with \(\lambda&gt;0\), dropping the \(\boldsymbol x\) notation for a moment for clarity. This system has fixed point attractors at \(\pm 1\) and an unstable fixed point (a separatrix) at \(0\) – a 1D Sandwich of Bistability.</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-09-29-Separatrix-Locator/mapping_illustration_horizontal.png" alt="Mapping" width="500"/> <div style="max-width: 500px; margin: 0.5rem auto; text-align: center;"> <em>Mapping the high-D state to a 1D bistable system.</em> </div> </div> <p>Now we want to couple the \(\psi\) dynamics with the \(\boldsymbol x\) dynamics so we bring back the \(\boldsymbol x\) dependence. Specifically as \(\boldsymbol x(t)\) evolves in time according to \(\eqref{eq:ODE}\):</p> \[\begin{equation} \frac{d}{dt}\bigg(\psi\big(\boldsymbol{x}(t)\big)\bigg) = \lambda\bigg[\psi\big(\boldsymbol{x}(t)\big) - \psi\big(\boldsymbol{x}(t)\big)^3\bigg]. \label{eq:sKEF} \end{equation}\] <p>This means that if we were to <em>observe</em> the value of \(\psi(\boldsymbol x)\) as \(\boldsymbol x\) evolved in time, that value would evolve according \(\eqref{eq:sKEFsimple}\).</p> <p>It seems that finding solutions to \(\eqref{eq:sKEF}\) could be the key to constructing a <em>separatrix locator</em>. The value \(\psi(\boldsymbol x)\) would be the coordinate of \(\boldsymbol x\) along the bistable axis. This value would be \(0\) when \(\boldsymbol x\) is anywhere on the separatrix, exactly our stated <a href="#goal">goal</a>.</p> <h2 id="squashed-koopman-eigenfunctions">(squashed) Koopman Eigenfunctions</h2> <p>At this stage, it’s worth noticing that the left hand side of \(\eqref{eq:sKEF}\) is actually a known object called the <a href="https://en.wikipedia.org/wiki/Lie_derivative">Lie derivative</a> of \(\psi\) along the flow given by \(f\), and also known as the infinitesimal generator of the <a href="https://en.wikipedia.org/wiki/Composition_operator">Koopman operator</a>, (See <d-cite key="brunton_notes_2019"></d-cite>).</p> <p>To make this link explicit, we first define the propagator function \(F_\tau(x(t)):=x(t+\tau)\) where \(x(t)\) is any solution to \(\eqref{eq:ODE}\). The Koopman operator \(\mathcal K_\tau\) is defined as</p> \[\mathcal K_\tau g = g \circ F_\tau\] <p>where \(g\) is any<d-footnote>\(g\) must belong to a Hilbert space, meaning that it must come with inner product (and it's associated norm), e.g., $$\langle f,g\rangle:=\int_{\mathbb R^N}f(\boldsymbol x)g(\boldsymbol x)d\boldsymbol x$$ thus requiring that the function be square integrable.</d-footnote> scalar function of the state-space \(\mathbb R^N\). Its infinitesimal generator \(\mathcal K\) (dropping the subscript) is essentially a time-derivative:</p> \[\begin{equation} \mathcal Kg = \lim_{\tau\to0} \frac{\mathcal K_\tau g - g}{\tau} =\lim_{\tau\to0} \frac{g\circ F_\tau - g}{\tau} = \frac{d}{d\tau} g \circ F_\tau\bigg\vert_{\tau=0}. \label{eq:koopman_generator} \end{equation}\] <p>The last version if evaluated on a trajectory \(x(t)\) is the left hand side of \(\eqref{eq:sKEF}\), allowing us to rewrite it compactly as</p> \[\begin{equation} \mathcal K\psi = \lambda (\psi-\psi^3). \label{eq:sKEF_compact} \end{equation}\] <p>This equation is <em>almost</em> an eigenfunction equation. All we need is to drop the cubic term:</p> \[\begin{equation} \mathcal K\phi = \lambda \phi. \label{eq:KEF} \end{equation}\] <p>In fact, the two problems are closely related. We can show that solutions to \(\eqref{eq:KEF}\) can be transformed into solutions of \(\eqref{eq:sKEF_compact}\) and vice versa by <em>squashing</em> and <em>unsquashing</em>. If \(\phi\) is a solution to \(\eqref{eq:KEF}\), then we can obtain a solution \(\psi\) to \(\eqref{eq:sKEF_compact}\) via:</p> \[\begin{equation} \psi(\boldsymbol{x}) = \frac{ \phi(\boldsymbol{x})}{ \sqrt{1+\phi(\boldsymbol{x})^2} } \label{eq:squash} \tag{squash} \end{equation}\] <p>Conversely, if \(\psi\) is a solution to \(\eqref{eq:sKEF_compact}\), then we can obtain a solution \(\phi\) to \(\eqref{eq:KEF}\) via:</p> \[\begin{equation} \phi(\boldsymbol{x}) = \frac{ \psi(\boldsymbol{x})}{ \sqrt{1-\psi(\boldsymbol{x})^2} } \label{eq:unsquash} \tag{unsquash} \end{equation}\] <p>We provide an informal derivation.</p> <details> <summary>Derivation: From eigenfunction to squashed eigenfunction and back</summary> <p>To do this, define the pointwise transforms \(\psi \;=\; u(\phi) \;:=\; \frac{\phi}{\sqrt{1+\phi^2}}, \qquad \phi \;=\; v(\psi) \;:=\; \frac{\psi}{\sqrt{1-\psi^2}}.\)</p> <hr/> <p>First we will derive useful identity: the chain rule for the Koopman generator.</p> <h3 id="koopman-chain-rule">Koopman chain rule</h3> <p>Let \(\phi:\mathbb R^N \to \mathbb R\) be a smooth scalar observable, and let \(u:\mathbb R \to \mathbb R\) be a smooth scalar nonlinearity. Let \(\psi(\boldsymbol x) = u(\phi(\boldsymbol x)).\)</p> <p>The Koopman generator is</p> \[\mathcal K g(\boldsymbol x) = \nabla g(\boldsymbol x)\cdot f(\boldsymbol x),\] <p>for any \(g\) where \(f(\boldsymbol x)\) is the underlying vector field.</p> <p>By the multivariable chain rule for gradients,</p> \[\nabla \psi(\boldsymbol x) = u'\big(\phi(\boldsymbol x)\big)\,\nabla \phi(\boldsymbol x).\] <p>Applying the Koopman generator gives</p> \[\mathcal K \psi(\boldsymbol x) = \nabla \psi(\boldsymbol x)\cdot f(\boldsymbol x) = u'\big(\phi(\boldsymbol x)\big)\,\nabla \phi(\boldsymbol x)\cdot f(\boldsymbol x) = u'\big(\phi(\boldsymbol x)\big)\,\mathcal K \phi(\boldsymbol x).\] <p>Therefore, for any smooth \(u\) and \(\phi\),</p> \[\boxed{\;\mathcal K[u(\phi)] = u'(\phi)\,\mathcal K\phi\; }.\] <hr/> <h3 id="from-mathcal-kphilambdaphi-to-mathcal-kpsilambdapsi-psi3">From \(\mathcal K\phi=\lambda\phi\) to \(\mathcal K\psi=\lambda(\psi-\psi^3)\)</h3> <p>Assume \(\mathcal K\phi \;=\; \lambda \phi.\)</p> <p>Recall that \(\psi = u(\phi)\) where \(u(z)=\dfrac{z}{\sqrt{1+z^2}}\). Compute \(u'(z)\):</p> \[\begin{align*} u'(z) &amp;= (1+z^2)^{-\frac{1}{2}} + z\cdot\Big(-\frac{1}{2}\Big)(1+z^2)^{-\frac{3}{2}}\cdot (2z) \\[2pt] &amp;= (1+z^2)^{-\frac{1}{2}} - z^2(1+z^2)^{-\frac{3}{2}} \\[2pt] &amp;= \frac{1+z^2-z^2}{(1+z^2)^{\frac{3}{2}}} \\[2pt] &amp;= \frac{1}{(1+z^2)^{\frac{3}{2}}} \end{align*}\] <p>By the Koopman chain rule,</p> \[\mathcal K\psi \;=\; u'(\phi)\,\mathcal K\phi \;=\; \frac{1}{(1+\phi^2)^{3/2}}\,\lambda\phi \;=\; \lambda\,\frac{\phi}{(1+\phi^2)^{3/2}}.\] <p>But</p> \[\psi - \psi^3 = \frac{\phi}{\sqrt{1+\phi^2}} - \frac{\phi^3}{(1+\phi^2)^{3/2}} = \frac{\phi(1+\phi^2)-\phi^3}{(1+\phi^2)^{3/2}} = \frac{\phi}{(1+\phi^2)^{3/2}}.\] <p>Hence \(\boxed{\;\mathcal K\psi \;=\; \lambda(\psi-\psi^3)\; }.\)</p> <hr/> <h3 id="from-mathcal-kpsilambdapsi-psi3-back-to-mathcal-kphilambdaphi">From \(\mathcal K\psi=\lambda(\psi-\psi^3)\) back to \(\mathcal K\phi=\lambda\phi\)</h3> <p>Assume \(\mathcal K\psi \;=\; \lambda(\psi-\psi^3).\)</p> <p>Recall that \(\phi = v(\psi)\) where \(v(z)=\dfrac{z}{\sqrt{1-z^2}}\). Compute \(v'(z)\):</p> \[\begin{align*} v'(z) &amp;= (1-z^2)^{-\frac{1}{2}} + z\cdot\frac{1}{2}(1-z^2)^{-\frac{3}{2}}\cdot (2z) \\[2pt] &amp;= (1-z^2)^{-\frac{1}{2}} + z^2(1-z^2)^{-\frac{3}{2}} \\[2pt] &amp;= \frac{1}{(1-z^2)^{\frac{3}{2}}} \end{align*}\] <p>Apply the Koopman chain rule with \(\phi=v(\psi)\):</p> \[\mathcal K\phi \;=\; v'(\psi)\,\mathcal K\psi \;=\; \frac{1}{(1-\psi^2)^{3/2}}\,\lambda(\psi-\psi^3) \;=\; \lambda\,\frac{\psi(1-\psi^2)}{(1-\psi^2)^{3/2}} \;=\; \lambda\,\frac{\psi}{\sqrt{1-\psi^2}} \;=\; \lambda\,\phi.\] <p>Thus \(\boxed{\;\mathcal K\phi \;=\; \lambda \phi\; }.\)</p> <hr/> <h3 id="conclusion">Conclusion</h3> <p>The pointwise transforms \(\psi = u(\phi) = \frac{\phi}{\sqrt{1+\phi^2}}, \qquad \phi = v(\psi) = \frac{\psi}{\sqrt{1-\psi^2}}\)</p> <p>carry solutions of the linear Koopman eigenfunction equation to solutions of the cubic equation and back: \(\mathcal K\phi=\lambda\phi \quad\Longleftrightarrow\quad \mathcal K\psi=\lambda(\psi-\psi^3).\)</p> </details> <p>Note that this derivation is highly non-rigorous. We gloss over the square integrability of \(\psi\) and \(\phi\), and even whether they are defined everywhere in \(\mathbb R^N\). According to our sandwich of bistability, we expect \(\psi(\boldsymbol {x^*})=\pm1\) at the attractors. According to \(\eqref{eq:unsquash}\), \(\phi(\boldsymbol {x^*})=\pm\infty\),</p> <h2 id="enter-deep-neural-networks">Enter Deep Neural Networks</h2> <p>Now that we know the properties of the desired \(\psi\), it’s time to find it. So how do we solve the \(\eqref{eq:sKEF}\) for a high-dimensional nonlinear system. This is where deep neural networks (DNNs) come in…</p> <p>First we re-write \(\eqref{eq:sKEF}\) as a partial differential equation (PDE):</p> \[\begin{equation} \nabla_{\boldsymbol{x}}\psi(\boldsymbol{x}) \cdot f(\boldsymbol{x}) = \lambda[\psi(\boldsymbol{x}) - \psi(\boldsymbol{x})^3], \label{eq:sKEFPDE} \end{equation}\] <p>recognising that \(\mathcal Kg=\nabla g \cdot f\) is another way to write the Koopman generator, using the multivariate chain rule on \(\eqref{eq:koopman_generator}\). This PDE means that instead of running the ODE \(\eqref{eq:ODE}\) to get trajectories \(\boldsymbol x(t)\), we can instead leverage the ability of DNNs to solve PDEs.</p> <p>We formulate a mean squared error loss for PDE \(\eqref{eq:sKEFPDE}\):</p> \[\begin{equation} \mathcal{L}_{\text{PDE}} = \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})} \Bigg[ \nabla \psi(\boldsymbol{x}) \cdot f(\boldsymbol{x}) - \lambda \Big(\psi(\boldsymbol{x})-\psi(\boldsymbol{x})^3\Big) \Bigg]^2, \label{eq:pde_loss} \end{equation}\] <p>where \(p(\boldsymbol{x})\) is a distribution over the phase space <d-cite key="e_deep_2018,sirignano_dgm_2018"></d-cite>. We can now parameterise \(\psi\) using a DNN, and train its weights to optimise \(\eqref{eq:pde_loss}\). This gradient-based PDE formulation is particularly convenient for implementation with DNNs since we can leverage automatic differentiation to compute the gradients efficiently. DNNs are also used in this way in physics-informed neural networks <d-cite key="raissi_physics-informed_2019"></d-cite>, encouraging DNNs to satisfy known physics, e.g., Navier–Stokes PDEs.</p> <p>Naturally, this doesn’t work out of the box. There are quite a few challenges – some common to eigenvalue problems, and some unique to our setting. You can click on them to find out more about why they arise, and how we solve them.</p> <details> <summary>Trivial solutions</summary> <p>As with any eigenvalue problem, this loss admits the trivial solution \(\psi \equiv 0\). To discourage such solutions, we introduce a shuffle-normalization loss where the two terms are sampled independently from the same distribution:</p> \[\begin{equation} \mathcal{L}_{\text{shuffle}} = \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x}), \tilde{\boldsymbol{x}} \sim p(\boldsymbol{x})} \Bigg[ \nabla \psi(\boldsymbol{x}) \cdot f(\boldsymbol{x}) - \lambda \Big(\psi(\tilde{\boldsymbol{x}}) - \psi(\tilde{\boldsymbol{x}})^3\Big) \Bigg]^2, \end{equation}\] <p>and optimise the ratio:</p> \[\begin{equation}\mathcal{L}_{\text{ratio}} = \frac{\mathcal{L}_{\text{PDE}}}{\mathcal{L}_{\text{shuffle}}}. \label{eq:ratio loss} \end{equation}\] </details> <details> <summary>Degeneracy across basins</summary> <p>Koopman eigenfunctions (KEFs) have an interesting property: a product of two KEFs is also a KEF. This can be seen by applying the PDE to the product of two such functions</p> \[\begin{equation} \nabla[\phi_1(x)\phi_2(x)] \cdot f(x) = (\lambda_1 + \lambda_2) \phi_1(x)\phi_2(x). \end{equation}\] <p>We’ll soon see that this translates to squashed KEFs as well. First, consider a smooth KEF \(\phi^1\) with \(\lambda = 1\) that vanishes only on the separatrix (what we want). Now, consider a piecewise-constant function \(\phi^0\) with \(\lambda = 0\) that is equal to 1 on one basin, and zero on another basin. The product \(\phi^1 \phi^0\) remains a valid KEF with \(\lambda = 1\), but it can now be zero across entire basins—thereby destroying the separatrix structure we aim to capture. Because of the relation between KEFs and sKEFs, this problem carries over to our squashed case. To mitigate this problem, we add another regulariser that causes the average value of \(\psi\) to be zero, encouraging negative and positive values on both sides of the separatrix.</p> </details> <details> <summary>Degeneracy in high dimensions</summary> <p>If the flow itself is separable, there is a family of KEFs that can emphasise one dimension over the others. Consider a 2D system \(\dot{x} = f_1(x), \quad \dot{y} = f_2(y)\), and the KEFs \(A(x)\) and \(B(y)\). There is a family of valid solutions \(\psi(x, y) = A(x)^{\mu} B(y)^{1 - \mu}\), for \(\mu \in R\).</p> <p>If \(\mu=0\) for instance, the \(x\) dimension is ignored. To mitigate this, we choose distributions for training the DNN that emphasise different dimensions, and then combine the results.</p> </details> <h2 id="does-it-work">Does It Work?</h2> <p>Now that we know what we are looking for (PDE equation), and how to find it (DNN), let’s put it all together. We train a DNN on a bistable damped oscillator, and on a 2D GRU trained on a 1-bit flip-flop task. In both cases, the resulting \(\psi\) has a zero level set on the separatrix.</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-09-29-Separatrix-Locator/two_2D_examples_squashed.png" alt="Two 2D Examples" width="100%"/> <div style="max-width: 500px; margin: 0.5rem auto; text-align: center;"> <em><strong>A</strong>: ODEs for the damped duffing oscillator. <strong>B</strong>: Kinetic energy function identifies stable and unstable fixed points. <strong>C</strong>: DNN approximation of the sKEF and it's level sets. The zero-level set (orange) aligns with the separatrix. <strong>D,E,F</strong>: Same for a 2D GRU RNN trained on a 1-bit flip flop task. </em> </div> </div> <p>Finally, we take a published \(N=668\) unit RNN trained to reproduce the activity of neurones from anterior lateral motor cortex of mice trained to respond to optogenetic stimulation of their somatosensory cortex <d-cite key="finkelstein_attractor_2021"></d-cite>. By simulating the RNN we can locate the two attractors. The separatrix is an \((N-1)\)-dimensional manifold in \(\mathbb{R}^N\). To evaluate our method, we sample this high-D space by drawing random cubic Hermite curves that connect the two attractors (Fig. <strong>A</strong>). We then run many simulations via a binary-search along each curve (parameterised by \(\alpha\in[0,1]\)) to find the true separatrix crossing, and compare with \(\psi=0\), finding close agreement (Fig. <strong>B</strong>). This also allows us to design optimal perturbations. If we want to change the network’s decision, pushing the system towards the desired attractor may not be the most efficient direction. Using \(\psi\), we design minimal perturbations that cross the separatrix. The resulting perturbation size is smaller than perturbations aimed at the target fixed point or random separatrix locations (Fig. <strong>C</strong>).</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-09-29-Separatrix-Locator/finkelstein_blog.png" alt="Two 2D Examples" width="100%"/> <div style="max-width: 500px; margin: 0.5rem auto; text-align: center;"> <em><strong>A</strong>: Hermite curves connecting attractors of a data-trained RNN <d-cite key="finkelstein_attractor_2021"></d-cite> (2D projection from 668D) with true separatrix points (red). <strong>B</strong>: sKEF zeroes versus true separatrix points along each curve. <strong>C</strong>: Norm of perturbations to reach separatrix from base point $\boldsymbol{x}_\text{base}$. </em> </div> </div> <h2 id="summary-and-outlook">Summary and Outlook</h2> <p>Making sense of high-dimensional dynamical systems is not trivial. We added another tool to the toolbox – a separatrix finder. More generally, one can think of our cubic \(\eqref{eq:sKEFsimple}\) as a <a href="https://en.wikipedia.org/wiki/Normal_form_(dynamical_systems)">normal form</a> for bistability. This is a canonical, or simple, version of a dynamical system with the same <em>topology</em>. Our method allows to reduce the high-D dynamics into such a form. In the future, we hope to extend this to many more applications. Check out <a href="https://github.com/KabirDabholkar/separatrix_locator">our code</a> and apply it to your own dynamical systems. Feel free to reach out to us, we’re excited to help and learn about new applications!</p> ]]></content><author><name>Kabir V. Dabholkar</name></author><category term="dynamical system"/><category term="recurrent neural network"/><category term="reverse engineering"/><summary type="html"><![CDATA[Finding Separatrices with Deep squashed Koopman Eigenfunctions]]></summary></entry><entry><title type="html">MLIR-ARX: Accelerator-Aware MLIR-to-RISC-V Compilation Integrated with an EDA Flow</title><link href="https://unireps.org//blog/2025/mlir-arx-accelerator-aware-mlir-to-risc-v-compilation-integrated-with-an-eda-flow/" rel="alternate" type="text/html" title="MLIR-ARX: Accelerator-Aware MLIR-to-RISC-V Compilation Integrated with an EDA Flow"/><published>2025-09-20T00:00:00+00:00</published><updated>2025-09-20T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/mlir-arx-accelerator-aware-mlir-to-risc-v-compilation-integrated-with-an-eda-flow</id><content type="html" xml:base="https://unireps.org//blog/2025/mlir-arx-accelerator-aware-mlir-to-risc-v-compilation-integrated-with-an-eda-flow/"><![CDATA[<p><strong>TLDR</strong></p> <p>Heterogeneous RISC-V systems challenge us to keep model representations coherent while deciding what to accelerate as hardware evolves. <code class="language-plaintext highlighter-rouge">mlir-arx</code> treats MLIR as a unifying representation layer: we introduce an <code class="language-plaintext highlighter-rouge">arx</code> dialect that encodes accelerator capabilities, inject lightweight profiling ops, and use a two-stage (analytic + profile-guided) cost model to form maximal offload regions under resource and dependency constraints. By aligning model computations across CPU and accelerators in a single IR space, our compile–-measure loop reliably selects profitable regions and delivers substantial end-to-end speedups on a MNIST CNN with a configurable VTA overlay on a Genesis FPGA prototype.</p> <p>Availability. Source code and artifacts are available at: <a href="https://gitlab.com/ones-ai/mlir-arx">Repository in Our Gitlab</a></p> <hr/> <h1 id="1-introduction"><strong>1. Introduction</strong></h1> <p>Edge and embedded AI systems increasingly pair general-purpose RISC-V cores with domain accelerators (from NPUs to lightweight tensor engines) to meet stringent latency and energy targets<d-cite key="jouppi2017tpu, eyeriss2016, nvdla2019"></d-cite>. In such settings, the practical challenge is not only to partition a model across CPU and accelerator boundaries, but also to decide what to accelerate when device capabilities are unknown or evolving at project start. A sensible path is to begin from a CPU-only baseline, profile real executions, and then offload the most profitable regions subject to resource and orchestration constraints. In this work we cast that workflow as a problem of representation alignment: by expressing computations in a common intermediate representation (MLIR), we keep the model’s computational structure coherent across heterogeneous RISC-V–attached devices while letting profiling and cost models determine which aligned regions should migrate to accelerators.</p> <p>MLIR’s multi-dialect design and progressive lowering are a natural fit for this workflow<d-cite key="mlir"></d-cite>. However, turning it into an end-to-end solution for RISC-V SoCs requires additional pieces:</p> <h3 id="11-requires">1.1 Requires</h3> <ol> <li>IR-level capability modeling to express an accelerator’s constraints</li> <li>lightweight profiling instrumentation that can be injected/stripped by passes</li> <li>a cost model that joins analytic estimates with profile-derived efficiencies</li> <li>packaging that integrates with an SoC/EDA flow.</li> </ol> <p>We present <code class="language-plaintext highlighter-rouge">MLIR-ARX</code>, a profile-guided compiler built on MLIR that introduces an <code class="language-plaintext highlighter-rouge">arx</code> dialect for accelerator capabilities, selects profitable offload regions via a two-stage cost model, and provides a retargetable backend integrated with RISC-V eXpress<d-footnote>We thank collaborators who contributed to the RVX integration and platform bring-up.</d-footnote> (<code class="language-plaintext highlighter-rouge">RVX</code>) for FPGA bring-up<d-cite key="rvx-etrij"></d-cite>. For initial experiments we use VTA as a configurable accelerator to test profile-driven selection and resource-aware partitioning, while the CPU-only path remains the numerically correct fallback<d-cite key="iree22tiny,tvm2018"></d-cite>.</p> <h3 id="12-contributions">1.2 Contributions</h3> <ul> <li><strong>Accelerator-aware IR</strong>. An <code class="language-plaintext highlighter-rouge">arx</code> dialect that captures accelerator capabilities (constraints, tiling, resource usage) and enables principled lifting of standard tensor ops.</li> <li><strong>Cost-guided partitioning</strong>. A two-stage (analytic + profile-guided) model that forms offload regions with explicit DMA orchestration and safe CPU fallbacks.</li> <li><strong>Retargetable backend + RVX integration</strong>. Code generation for RISC-V plus accelerator stubs and a device manifest that RVX uses for automatic integration.</li> <li><strong>Early evaluation</strong>. A small CNN on MNIST running on RVX-based FPGA prototypes with VTA, validating the compile–measure loop and selection mechanism.</li> </ul> <hr/> <h1 id="2-background"><strong>2. Background</strong></h1> <p><strong>MLIR Foundations and Target Platform</strong></p> <h3 id="21-mlir-in-brief">2.1 MLIR in brief</h3> <p>MLIR is a multi-level compiler infrastructure that represents computations at various abstraction levels and supports progressive lowering through dialects and pattern-based rewrites<d-cite key="mlir"></d-cite>. Dialects capture operations, types, and constraints; key ones for <code class="language-plaintext highlighter-rouge">MLIR-ARX</code> include <code class="language-plaintext highlighter-rouge">mhlo/StableHLO</code> for tensor semantics<d-cite key="stablehlo"></d-cite>, <code class="language-plaintext highlighter-rouge">linalg</code> for structured kernels, and <code class="language-plaintext highlighter-rouge">memref</code> for explicit memory.</p> <p>Two features are central to our setting: bufferization, which separates algorithmic transformations from storage decisions by converting tensors to <code class="language-plaintext highlighter-rouge">memref</code>s with explicit lifetimes, and dialect interfaces/converters, which allow capability-aware lifting or lowering between dialects. These make it possible to express accelerator constraints in IR and offload only the supported regions. Prior systems, from TVM’s multi-backend flow<d-cite key="tvm2018"></d-cite> to IREE’s embedded pipelines<d-cite key="iree22tiny"></d-cite>, demonstrate the viability of such end-to-end MLIR-based compilation.</p> <h3 id="22-why-mlir-for-risc-v--accelerators">2.2 Why MLIR for RISC-V + Accelerators</h3> <p>RISC-V-based edge platforms often combine a control processor (with or without a vector extension) and one or more domain accelerators attached via a memory-mapped interconnect.</p> <p>This creates three immediate needs:</p> <ol> <li>partitioning of the model into CPU and accelerator regions.</li> <li>explicit orchestration of DMA, synchronization, and address spaces.</li> <li>graceful fallback when constraints are violated. MLIR’s dialect modularity lets us.</li> </ol> <p>(a) express accelerator capabilities as IR-level contracts, (b) form and legalize offload regions under those contracts, and (c) lower both sides—CPU and accelerator stubs—within a single pass manager, sharing analyses (shape, alias, dependence) across the boundary. Compared with ad-hoc code generators, the benefits are: reuse of upstream transformations, uniform debuggability, and a single IR for both accelerated and non-accelerated builds.</p> <h3 id="23-rvx-overview">2.3 RVX overview</h3> <p>RVX is an EDA environment to assemble RISC-V SoCs (single-/multi-core, memory subsystem, interconnect, peripheral IP), validate them on FPGA, and produce handoff artifacts for silicon<d-cite key="rvx-etrij"></d-cite>. On the software side, RVX provides toolchain integration points (boot/firmware layout, MMIO address map, interrupt lines) and profiling hooks. <code class="language-plaintext highlighter-rouge">MLIR-ARX</code> targets this boundary: it emits CPU binaries, an accelerator runtime and driver stubs, and a device manifest that RVX consumes to wire up interconnect ports and firmware tables. By aligning compiler outputs with RVX’s manifests, we avoid manual bring-up steps when retargeting cores or adding/removing accelerators.</p> <blockquote> <p>We primarily target edge-style RISC-V SoCs where accelerators are memory-mapped with local SRAM and DMA engines. Our current prototype assumes statically known shapes in offload regions; dynamic-shape support is under active development.</p> </blockquote> <hr/> <h1 id="3-design-and-architecture"><strong>3. Design and Architecture</strong></h1> <p><strong>Profile-Guided MLIR-to-RISC-V Offload</strong></p> <h3 id="31-pipeline-overview">3.1 Pipeline Overview</h3> <p><a href="#flow">Figure 1</a> shows the end-to-end flow: models import into MLIR, run once on a CPU baseline to profile, mine offload candidates, select them under FPGA budgets, form offload regions, and lower to CPU and accelerator backends. A machine-readable accelerator description generates the <code class="language-plaintext highlighter-rouge">arx</code> dialect and converters; RVX profiling closes the compile–measure loop.</p> <h3 id="32-baseline-execution-and-profiling">3.2 Baseline Execution and Profiling</h3> <p><code class="language-plaintext highlighter-rouge">MLIR-ARX</code> assumes no accelerator a priori. Every model first executes end-to-end on RISC-V, producing per-op/region profiles (cycles, bytes moved, stalls, shapes/dtypes/layouts) keyed by stable IR handles. This path is also the correctness fallback for any region that proves illegal or unprofitable to offload.</p> <h3 id="33-candidate-discovery-and-cost-modeling">3.3 Candidate Discovery and Cost Modeling</h3> <p>Hot single ops or short fusable patterns are grouped by semantics/constraints as offload candidates. For a region $R$, we use an inline estimate $\Delta T(R)=T_{\mathrm{cpu}}(R)-T_{\mathrm{off}}(R)$ with $T_{\mathrm{off}}\approx T_{\mathrm{setup}}+\max(T_{\mathrm{dma}},T_{\mathrm{cmp}})+T_{\mathrm{sync}}$ where analytic terms (op counts, tiling, bandwidth/latency) are corrected by profile-derived efficiencies.</p> <h3 id="34-resource-aware-selection-and-partitioning">3.4 Resource-Aware Selection and Partitioning</h3> <p>On FPGA targets, selection respects LUT/FF/DSP/BRAM, local SRAM, DMA lanes, and clock budgets. Given per-candidate resource costs $\mathcal{R}(c)$ and budget $B$, we maximize $\sum\Delta T(c)$ subject to $\sum\mathcal{R}(c)\le B$, preferring high benefit density when tight. Selected ops become maximal \emph{offload regions} under dependency/memory constraints; the compiler inserts explicit host–device copies/fences and schedules DMA to overlap with compute.</p> <h3 id="35-lowering-and-runtime">3.5 Lowering and Runtime</h3> <p>Both CPU and accelerator paths first lower into the <code class="language-plaintext highlighter-rouge">arx</code> dialect. CPU ops then follow <code class="language-plaintext highlighter-rouge">linalg</code> $\to$ <code class="language-plaintext highlighter-rouge">scf/affine</code> $\to$ <code class="language-plaintext highlighter-rouge">LLVM</code> to produce RISC-V ELFs, while accelerator ops lower directly to library calls with a thin runtime and DMA descriptors. A device manifest (MMIO ranges, IRQ lines) is also emitted for RVX integration.</p> <h3 id="36-retargeting-and-feedback">3.6 Retargeting and feedback</h3> <p>A YAML/JSON accelerator description regenerates the <code class="language-plaintext highlighter-rouge">arx</code> capability model and converters, enabling recompilation without model changes. Deployed binaries feed fresh RVX profiles back into the cost model, which updates efficiencies and re-ranks candidates for the next iteration.</p> <p><a id="flow"></a> </p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-09-29-mlir-arx-accelerator-aware-mlir-to-risc-v-compilation-integrated-with-an-eda-flow/01_system_overview.png" alt="My Image" width="700"/> </div> <hr/> <h1 id="4-implementation"><strong>4. Implementation</strong></h1> <p><strong>ONNX-MLIR Base, ARX Dialect, and RVX/VTA Bring-up</strong></p> <h3 id="41-codebase-and-mlir-integration">4.1 Codebase and MLIR Integration</h3> <p>We implement <code class="language-plaintext highlighter-rouge">MLIR-ARX</code> by extending the open-source ONNX-MLIR<d-cite key="onnxmlir2020"></d-cite> stack. Models are imported and legalized through ONNX-MLIR into MLIR’s tensor/structured dialects, upon which we add our <code class="language-plaintext highlighter-rouge">ARX</code> components. The output code runs unmodified on RVX-synthesized RISC-V platforms; when no accelerator is present, the CPU path serves as the numerically correct fallback.</p> <h3 id="42-arx-dialect-and-lowering-paths">4.2 ARX Dialect and Lowering Paths</h3> <p>The <code class="language-plaintext highlighter-rouge">arx</code> dialect lifts eligible tensor and <code class="language-plaintext highlighter-rouge">linalg</code> ops into accelerator-aware form, annotated with capability and tiling metadata. Both CPU and accelerator ops lower through this dialect: CPU paths continue via <code class="language-plaintext highlighter-rouge">linalg</code> $\to$ <code class="language-plaintext highlighter-rouge">scf/affine</code> $\to$ <code class="language-plaintext highlighter-rouge">LLVM</code> for RISC-V binaries, while accelerator ops become driver/library calls with explicit host–device transfers. Because both paths share the same pass pipeline, analyses such as shape, aliasing, and dependence are reused across CPU and accelerator lowering.</p> <h3 id="43-profiling-instrumentation">4.3 Profiling Instrumentation</h3> <p>To support the profile-driven flow, we define lightweight profiling ops (begin/end counters, byte/traffic counters, DMA/compute timestamps). When the compiler is invoked with a profiling option, an <em>instrumentation pass</em> inserts these ops around selected regions/ops during canonicalization and bufferization. The pass is designed to be idempotent and can be stripped in a late “de-instrumentation” pass for production builds. Profiles are keyed by stable IR handles to survive recompilation unless shapes change.</p> <h3 id="44-accelerator-target-vta">4.4 Accelerator Target: VTA</h3> <p>For an initial hardware target we use Apache TVM’s VTA soft accelerator. VTA implements a small set of tensor primitives with parameterizable compute parallelism (e.g., PE width), local SRAM sizes, and instruction buffer depth. This configurability makes it a good vehicle to test <code class="language-plaintext highlighter-rouge">MLIR-ARX</code>’s candidate selection and resource-aware partitioning: The capability model is regenerated from the chosen VTA configuration, while the cost model estimates tile fit, DMA overlap, and achievable throughput under the given SRAM and bandwidth constraints.</p> <hr/> <h1 id="5-early-evaluation"><strong>5. Early Evaluation</strong></h1> <h3 id="51-setup">5.1 Setup</h3> <p>Our prototype system follows the dual-ORCA configuration in prior TIP work, with two ORCA RISC-V RV32IM cores synthesized at 100 MHz on a Genesis FPGA and connected via the RVX-generated $\mu$NoC and AXI-based DRAM subsystem. A configurable VTA overlay is attached as an MMIO+DMA device, using 8-bit inputs/weights, 32-bit accumulators, and a $16{\times}16{\times}16$ GEMM block. All components are generated by RVX from a manifest and deployed to FPGA.</p> <h3 id="52-benchmark">5.2 Benchmark</h3> <p>For evaluation, we use a small CNN for MNIST consisting of two conv+ReLU blocks with $2{\times}2$ pooling, followed by a fully connected layer (input $1{\times}28{\times}28$). Profiles are first collected from the CPU-only execution to guide candidate discovery. Operators selected for offload are <em>conv2d(+bn)+relu</em> blocks (Conv1, Conv2) and the final fully connected layer; control flow, quantize/dequantize, and maxpool remain on the CPU.</p> <h3 id="53-cpu-only-baseline">5.3 CPU-only Baseline</h3> <p><a href="#inference-time-arx">Table 1</a> reports the measured per-layer latency on the dual ORCA cores. The two convolution layers dominate the runtime (over 95 % of total latency), making them the primary candidates for acceleration.</p> <h3 id="54-projected-accelerator-performance">5.4 Projected Accelerator Performance</h3> <p>Since the FPGA prototype is still under integration, we estimate accelerator-side performance using a simple throughput model assuming three VTA configurations (A/B/C) with 256/512/1024 MAC/cycle. <a href="#inference-time-arx">Table 1</a> shows the projected latency when Conv1, Conv2, and the fully connected layer are offloaded to VTA, while other layers remain on the CPU. The results indicate a potential end-to-end speedup of $50 \times$–$90\times$ compared to the CPU-only baseline, with hardware resource utilization scaling from $\sim$ 15 % to $\sim$ 60 % of available DSPs on the target FPGA.</p> <p><a id="inference-time-arx"></a></p> <table> <caption> Table 1. Predicted latency of MNIST CNN layers with CPU-only vs. three VTA configurations (A/B/C). VTA throughput assumes 256/512/1024&nbsp;MAC/cycle at 75% utilization. Hardware utilization is estimated on XC7K325T FPGA. </caption> <thead> <tr> <th style="text-align:left;">Layer</th> <th style="text-align:right;">Ops</th> <th style="text-align:right;">CPU-only (&micro;s)</th> <th style="text-align:right;">VTA-A (&micro;s)</th> <th style="text-align:right;">VTA-B (&micro;s)</th> <th style="text-align:right;">VTA-C (&micro;s)</th> </tr> </thead> <tbody> <tr> <td style="text-align:left;">1. Quantize</td> <td style="text-align:right;">&mdash;</td> <td style="text-align:right;">5.73</td> <td style="text-align:right;">&mdash;</td> <td style="text-align:right;">&mdash;</td> <td style="text-align:right;">&mdash;</td> </tr> <tr> <td style="text-align:left;">2. Conv1</td> <td style="text-align:right;">97,344</td> <td style="text-align:right;">884.20</td> <td style="text-align:right;">5.07</td> <td style="text-align:right;">2.54</td> <td style="text-align:right;">1.27</td> </tr> <tr> <td style="text-align:left;">3. MaxPool1</td> <td style="text-align:right;">10,816</td> <td style="text-align:right;">28.96</td> <td style="text-align:right;">&mdash;</td> <td style="text-align:right;">&mdash;</td> <td style="text-align:right;">&mdash;</td> </tr> <tr> <td style="text-align:left;">4. Conv2</td> <td style="text-align:right;">557,568</td> <td style="text-align:right;">2949.15</td> <td style="text-align:right;">29.04</td> <td style="text-align:right;">14.52</td> <td style="text-align:right;">7.26</td> </tr> <tr> <td style="text-align:left;">5. FullyConnected</td> <td style="text-align:right;">5,120</td> <td style="text-align:right;">4.76</td> <td style="text-align:right;">0.27</td> <td style="text-align:right;">0.13</td> <td style="text-align:right;">0.07</td> </tr> <tr> <td style="text-align:left;">6. Dequantize</td> <td style="text-align:right;">&mdash;</td> <td style="text-align:right;">0.50</td> <td style="text-align:right;">&mdash;</td> <td style="text-align:right;">&mdash;</td> <td style="text-align:right;">&mdash;</td> </tr> <tr> <th style="text-align:left;">Total</th> <th style="text-align:right;">670,848</th> <th style="text-align:right;">3873.30</th> <th style="text-align:right;">69.57</th> <th style="text-align:right;">52.38</th> <th style="text-align:right;">43.78</th> </tr> <tr> <th scope="row" colspan="2" style="text-align:left;">HW Util. (DSP / BRAM / LUT %)</th> <td style="text-align:right;">&mdash;</td> <td style="text-align:right;">15 / 44 / 12</td> <td style="text-align:right;">31 / 44 / 21</td> <td style="text-align:right;">61 / 46 / 36</td> </tr> </tbody> </table> <hr/> <h1 id="6-conclusion"><strong>6. Conclusion</strong></h1> <p>We presented <code class="language-plaintext highlighter-rouge">MLIR-ARX</code>, a profile-driven MLIR compiler that begins from a CPU-only RISC-V baseline, identifies profitable regions, and offloads them to accelerators under resource and orchestration constraints. <code class="language-plaintext highlighter-rouge">MLIR-ARX</code> introduces the <code class="language-plaintext highlighter-rouge">arx</code> dialect for capability-aware lifting, lightweight profiling instrumentation, and a retargetable backend integrated with RVX. Early evaluation on a small MNIST CNN with a configurable VTA overlay shows that offloading Conv1/Conv2/FC achieves order-of-magnitude latency reductions over the dual-ORCA CPU baseline, consistent with our cost-model predictions.</p> <hr/> <h1 id="7-limitations-and-outlook"><strong>7. Limitations and outlook</strong></h1> <p>Our cost model is analytic with profile-derived corrections; learned models may better capture controller effects and DMA/compute overlap. Offload regions currently assume static shapes; extending legality/bufferization for dynamic-shape cases is ongoing. Finally, while the CPU path can exploit vector intrinsics, full scheduling for attention-like blocks and multi-accelerator concurrency remains future work. We expect these extensions—alongside broader accelerator backends—to further tighten the compile–measure loop and reduce manual retargeting on RVX platforms.</p> <hr/> <h1 id="8-related-work"><strong>8. Related Work</strong></h1> <p><strong>RISC-V CPUs, Vector Extensions, AI Accelerators, and Design Space Exploration</strong></p> <h3 id="81-risc-v-as-the-control-plane-for-heterogeneous-ml-socs">8.1 RISC-V as the Control Plane for Heterogeneous ML SoCs</h3> <p>Open RISC-V implementations range from tiny in-order microcontrollers to out-of-order Linux-class cores, making them a natural control plane for accelerator-rich SoCs. Representative open cores include Rocket (in-order) and BOOM (out-of-order) from the Berkeley stack<d-cite key="rocket,boom"></d-cite>, CVA6/Ariane<d-cite key="cva6"></d-cite>, and the PULP family for ultra-low-power microcontrollers with DSP-like packed-SIMD extensions<d-cite key="pulp"></d-cite>. SoC generators such as Chipyard<d-cite key="chipyard"></d-cite> offer standard interconnects (TileLink/AXI) and co-processor attachment points (e.g., RoCC), reducing the cost of integrating DMA-capable accelerators alongside a RISC-V host.</p> <h3 id="82-risc-v-vector-and-packed-simd-for-ml">8.2 RISC-V Vector and Packed-SIMD for ML</h3> <p>The RISC-V Vector extension (RVV)<d-cite key="rvv"></d-cite> adopts a vector-length-agnostic model that decouples vector width from the ISA, enabling portability across microarchitectures. Implementations can choose lane count and microarchitectural details, as explored in Hwacha and Ara<d-cite key="hwacha"></d-cite>. For MCU-class devices, the packed-SIMD “P” extensions from PULP<d-cite key="pulp"></d-cite> target fixed-point and dot-product primitives. In practice, RVV or packed SIMD is well-suited for control and medium-granularity tensor compute, while large GEMMs/convolutions are often offloaded to dedicated accelerators.</p> <h3 id="83-attachment-patterns-for-ai-accelerators">8.3 Attachment Patterns for AI Accelerators</h3> <p>Three patterns dominate:</p> <ol> <li> <p><strong>Memory-mapped DMA engines</strong>: accelerators with local SRAM and DMA, controlled as MMIO devices; the most common in embedded contexts.</p> </li> <li> <p><strong>Coprocessors (e.g., RoCC)</strong>: accelerators invoked via custom instructions or queues, reducing software overhead but tying the ABI to a core design<d-cite key="rocket,chipyard,gemmini"></d-cite>.</p> </li> <li> <p><strong>Streaming/NoC-attached engines</strong>: connected via on-chip networks with stream interfaces; the host sets up dataflow graphs and dispatches jobs.</p> </li> </ol> <p><code class="language-plaintext highlighter-rouge">MLIR-ARX</code> assumes the first pattern (MMIO+DMA) but its IR contracts generalize to other attachments.</p> <h3 id="84-tensor-accelerators-and-dataflows">8.4 Tensor Accelerators and Dataflows</h3> <p>A wide body of work covers compute/dataflow design for DNNs: systolic arrays (e.g., TPU<d-cite key="jouppi2017tpu"></d-cite>), spatial dataflows (row/output/weight-stationary, e.g., Eyeriss<d-cite key="eyeriss2016"></d-cite>), and precision-specialized engines (e.g., NVDLA<d-cite key="nvdla2019"></d-cite>). Open-source generators like Gemmini<d-cite key="gemmini"></d-cite> produce RISC-V-attached systolic arrays with tunable parameters. These designs show that with aggressive on-chip reuse and explicit DMA/compute overlap, accelerators can deliver order-of-magnitude energy savings if the compiler/runtime manages packing, tiling, and synchronization.</p> <h3 id="85-fpga-overlays-and-soft-accelerators">8.5 FPGA Overlays and Soft Accelerators</h3> <p>FPGAs serve as prototyping and deployment platforms for edge ML accelerators. Overlay designs, including VTA<d-cite key="vta"></d-cite>, FINN<d-cite key="finn"></d-cite>, and hls4ml<d-cite key="hls4ml"></d-cite>, expose parameter spaces (PE array size, SRAM depth, DMA width) suitable for compiler-driven design space exploration. Compared to fixed-function ASICs, overlays trade some efficiency for rapid iteration and portability.</p> <h3 id="86-memory-systems-and-data-movement">8.6 Memory Systems and Data Movement</h3> <p>Memory hierarchy decisions strongly influence performance and energy. Designs like Eyeriss exploit row-stationary mapping to minimize off-chip traffic; others (e.g., MAERI, SCNN) adapt to sparsity and flexible reductions. For RISC-V-attached accelerators, key challenges are software-visible packing/tiling to match SRAM/DMA configurations and overlapping DMA with compute. <code class="language-plaintext highlighter-rouge">MLIR-ARX</code>’s ARX dialect makes these constraints explicit in IR.</p> <h3 id="87-design-space-exploration-dse-for-ai-accelerators">8.7 Design Space Exploration (DSE) for AI Accelerators</h3> <p>DSE methods co-optimize accelerator architecture and mapping. Frameworks like ZigZag<d-cite key="zigzag"></d-cite> generate and evaluate architecture–mapping pairs with cost models for area, energy, and performance, achieving significant energy gains over baseline mappings. Tools such as Timeloop<d-cite key="timeloop"></d-cite> and Accelergy<d-cite key="accelergy"></d-cite> focus on loop mapping and energy estimation.</p> <p>These DSE strategies are applicable to RISC-V–integrated accelerators, where FPGA/SoC constraints require balancing performance and resource use. In <code class="language-plaintext highlighter-rouge">MLIR-ARX</code>, the cost model and candidate selection can be extended to search over accelerator microarchitectures and RISC-V/accelerator interface options, further tightening the compile–measure loop.</p> <h3 id="88-positioning-of-mlir-arx">8.8 Positioning of MLIR-ARX</h3> <p><code class="language-plaintext highlighter-rouge">MLIR-ARX</code> complements the hardware and DSE work above by embedding accelerator capability models in IR, using profile-guided cost modeling to identify profitable offloads, inserting legal DMA/synchronization with overlap, and retargeting automatically to new accelerators or configurations without model changes. Hardware advances supply the building blocks; <code class="language-plaintext highlighter-rouge">MLIR-ARX</code> provides the IR-level integration and automation in a RISC-V EDA flow.</p> <h3 id="89-mlir-centric-compiler-stacks-and-hlsrtl-generation">8.9 MLIR-centric compiler stacks and HLS/RTL generation</h3> <p>MLIR has increasingly been used not only as a software-oriented IR but also as a hardware-construction and HLS coordination layer. The CIRCT project extends MLIR with hardware-facing dialects (e.g., <code class="language-plaintext highlighter-rouge">hw</code>, <code class="language-plaintext highlighter-rouge">comb</code>, <code class="language-plaintext highlighter-rouge">sv</code>, <code class="language-plaintext highlighter-rouge">fsm</code>) and export passes to synthesizable SystemVerilog, enabling end-to-end generation of RTL directly from MLIR programs <d-cite key="circt"></d-cite>. For dataflow-style accelerators, the <code class="language-plaintext highlighter-rouge">handshake</code> and <code class="language-plaintext highlighter-rouge">staticlogic</code> dialects capture fine- and coarse-grain control and enable automated scheduling/retiming before \emph{ExportVerilog}. These flows complement software-oriented tensor dialects by giving a path to hardware under the same abstraction umbrella.</p> <p>A second line of work connects MLIR to C/C++ code generation as an HLS front end. The <code class="language-plaintext highlighter-rouge">EmitC</code> path lowers structured MLIR to portable C++ suitable for downstream toolchains, including HLS compilers, while preserving shape and buffer semantics. Building on this idea, ScaleHLS uses MLIR to drive loop transformations (tiling, unrolling, pipelining) and memory partitioning so that the emitted C/C++ attains predictable quality-of-results when synthesized by commercial HLS tools <d-cite key="scalehls"></d-cite>. This separation—high-level legality and transformation in MLIR, hardware construction in HLS—aligns with our design where algorithmic and storage decisions are expressed in IR.</p> <p>Dynamic and elastic dataflow HLS has also been explored atop MLIR. Dynamatic integrates with MLIR’s <code class="language-plaintext highlighter-rouge">handshake</code> pipeline to generate elastic circuits that tolerate variable-latency operators and memory, bringing modulo scheduling and token-based control to the HLS space <d-cite key="dynamatic"></d-cite>. At the coarse-grain end, MLIR-based AIE flows target CGRA-like AI engines (e.g., Xilinx/AMD AI Engine), using MLIR dialects to express tile-local compute, DMA, and interconnect routing before producing device-ready binaries <d-cite key="mliraie"></d-cite>. Finally, the Calyx project exposes a hardware-centric intermediate representation and an MLIR dialect that make resource sharing, banking, and control explicit, providing another route from MLIR programs to verifiable RTL generators <d-cite key="calyx"></d-cite>.</p> <p>Position relative to our system. These efforts show two practical integration patterns for MLIR: (i) MLIR $\to$ RTL via CIRCT-style dialects, and (ii) MLIR $\to$ C/C++ via EmitC/ScaleHLS for HLS back ends. <code class="language-plaintext highlighter-rouge">MLIR-ARX</code> currently focuses on partitioning and orchestrating offload regions under a unified IR for CPU and accelerator execution, but its capability model and region formation are compatible with both patterns: the same <code class="language-plaintext highlighter-rouge">arx</code> ops can be lowered either to MMIO-driven stubs (our VTA/RVX path) or, in a future backend, to HLS-friendly C++ or directly to RTL through CIRCT. This suggests a path to automatically synthesize specialized accelerators for hot regions discovered by the profile-guided flow, while preserving the CPU fallback and the EDA integration boundary already in place.</p> <hr/> <h1 id="9-vta-configuration-and-expected-performance-on-fpga"><strong>9. VTA Configuration and Expected Performance on FPGA</strong></h1> <h3 id="91-hardware-overview">9.1 Hardware overview</h3> <p>VTA is a soft deep-learning accelerator intended for FPGAs. It implements a decoupled three-stage pipeline (load, compute, store) with dedicated on-chip buffers and task queues, enabling overlap of DMA and compute when tiling admits double buffering<d-cite key="vta,ml2tuner25"></d-cite>. The architecture exposes configuration knobs for arithmetic precision, on-chip buffer sizes, and the inner matrix-multiply shape that together determine tile fit, bandwidth demand, and achievable utilization.</p> <h3 id="92-configuration-reported-on-zcu102">9.2 Configuration reported on ZCU102</h3> <p>As background, we refer to the ZCU102-oriented VTA configuration summarized in <a href="#hardware-vta">Table 2</a>, reported by prior work<d-cite key="ml2tuner25"></d-cite>. Values are expressed in log2 form in TVM/VTA’s JSON; for clarity we additionally list the corresponding bit-widths and buffer capacities. The reported buffer sizes are one step larger than the common defaults, chosen to better utilize Zynq UltraScale+ resources.</p> <p><a id="hardware-vta"></a></p> <table> <caption> Table 2. VTA configuration on Xilinx ZCU102 (derived from the configuration used in EVTA<d-cite key="ml2tuner25"/>). </caption> <thead> <tr> <th style="text-align:left;">Attribute</th> <th style="text-align:right;">JSON (log2)</th> <th style="text-align:left;">Interpreted value</th> <th style="text-align:left;">Effect</th> </tr> </thead> <tbody> <tr> <td><code>LOG_INP_WIDTH</code></td> <td style="text-align:right;">3</td> <td>8-bit int</td> <td>input precision</td> </tr> <tr> <td><code>LOG_WGT_WIDTH</code></td> <td style="text-align:right;">3</td> <td>8-bit int</td> <td>weight precision</td> </tr> <tr> <td><code>LOG_ACC_WIDTH</code></td> <td style="text-align:right;">5</td> <td>32-bit int</td> <td>accumulator precision</td> </tr> <tr> <td><code>LOG_BATCH</code></td> <td style="text-align:right;">0</td> <td>1</td> <td>batch factor in intrinsic</td> </tr> <tr> <td><code>LOG_BLOCK</code></td> <td style="text-align:right;">4</td> <td>16</td> <td>inner GEMM tile (PE block)</td> </tr> <tr> <td><code>LOG_UOP_BUFF_SIZE</code></td> <td style="text-align:right;">16</td> <td>64&nbsp;KiB</td> <td>micro-op buffer</td> </tr> <tr> <td><code>LOG_INP_BUFF_SIZE</code></td> <td style="text-align:right;">16</td> <td>64&nbsp;KiB</td> <td>input buffer</td> </tr> <tr> <td><code>LOG_WGT_BUFF_SIZE</code></td> <td style="text-align:right;">19</td> <td>512&nbsp;KiB</td> <td>weight buffer</td> </tr> <tr> <td><code>LOG_ACC_BUFF_SIZE</code></td> <td style="text-align:right;">18</td> <td>256&nbsp;KiB</td> <td>accumulator buffer</td> </tr> </tbody> </table> <h3 id="93-implications-for-tiling-and-overlap">9.3 Implications for tiling and overlap</h3> <p>Given <code class="language-plaintext highlighter-rouge">LOG_BLOCK</code>=4, the intrinsic compute block is $16{\times}16{\times}16$.</p> <p>Legal tiles must satisfy buffer capacity and alignment constraints for the three-stage pipeline:</p> <p>(i) an input/weight sub-tile that fits {64 KiB, 512 KiB} with layout-specific padding. (ii) an accumulator tile that fits 256 KiB. (iii) DMA chunking that aligns with the memory interface.</p> <p>When these constraints are met, double buffering allows:</p> \[T_{\mathrm{off}}\approx T_{\mathrm{setup}}+\max(T_{\mathrm{dma}},T_{\mathrm{cmp}})+T_{\mathrm{sync}},\] <p>and hides the smaller of DMA/compute times.</p> <h3 id="94-mapping-to-mlir-arx">9.4 Mapping to MLIR-ARX</h3> <p>In <code class="language-plaintext highlighter-rouge">mlir-arx</code>’s YAML capability description, the configuration in <a href="#hardware-vta">Table 2</a> becomes the static part of the accelerator model (precision, intrinsic block, buffer capacities). The partitioner only lifts regions whose tiles provably fit, and the cost model accounts for (i) the $\max(T_{\mathrm{dma}},T_{\mathrm{cmp}})$ overlap enabled by double buffering, (ii) setup/synchronization, and (iii) memory-traffic inflation from packing and padding. Under multi-VTA, the resource model exposes the number of instances and the shared DRAM bandwidth so that selection can avoid overcommitting the memory system.</p> <hr/> <h1 id="10-arx-dialect"><strong>10. ARX Dialect</strong></h1> <p><strong>Selected Operations and Capability Schema</strong></p> <p>This section sketches the subset of ARX operations and attributes that our prototype uses to plan and legalize offload regions. The design mirrors MLIR’s convention of making capabilities explicit at IR boundaries so that legality and code generation are mechanically checkable.</p> <h3 id="101-core-ops-and-attributes">10.1 Core ops and attributes</h3> <p><a href="#arx-ops">Table 3</a> summarizes representative ops and their key attributes. The attributes are chosen so that (i) legality checks are local, (ii) tiling constraints can be statically validated, and (iii) DMA and compute costs can be derived from sizes and layouts.</p> <p><a id="arx-ops"></a></p> <table> <caption>Table 3. Selected ARX ops and attributes used in the prototype.</caption> <thead> <tr> <th style="text-align:left;">Op</th> <th style="text-align:left; width:5cm;">Key attributes</th> <th style="text-align:left; width:6cm;">Notes</th> </tr> </thead> <tbody> <tr> <td><code>arx.conv2d</code></td> <td><code>dtype, strides, dilations, padding, tile_h, tile_w, ic_blk, oc_blk</code></td> <td>Convolution lifted from <code>linalg.conv_*</code>. Tiling attributes reflect scratchpad fit and inner GEMM blocking.</td> </tr> <tr> <td><code>arx.gemm</code></td> <td><code>dtype, M_blk, N_blk, K_blk</code></td> <td>Canonicalized matmul; blocks must be multiples of the accelerator&#39;s intrinsic block.</td> </tr> <tr> <td><code>arx.pool</code></td> <td><code>mode, kH, kW, strides</code></td> <td>Optional; emitted only when the accelerator implements pooling.</td> </tr> <tr> <td><code>arx.copy</code></td> <td><code>src_space, dst_space, bytes, align</code></td> <td>Logical copies across host/device address spaces. Lowered to DMA descriptors when possible.</td> </tr> <tr> <td><code>arx.region</code></td> <td><code>reads, writes, sram_bytes</code></td> <td>Opaque offload region container; captures side conditions (aliasing, fences).</td> </tr> </tbody> </table> <h3 id="102-example-lifting">10.2 Example lifting</h3> <p>A legal <code class="language-plaintext highlighter-rouge">linalg.matmul</code> with shapes that fit the intrinsic block becomes:</p> <pre style="color:#111827; background:#f3f4f6">
%y = arx.gemm  %a, %b
     { dtype = i8, M_blk = 16, N_blk = 16, K_blk = 16 } : ...
</pre> <hr/> <h1 id="11-profiling-instrumentation-and-log-schema"><strong>11. Profiling Instrumentation and Log Schema</strong></h1> <h3 id="111-instrumentation-ops">11.1 Instrumentation ops</h3> <p>Profiling is injected by a dedicated pass when a command-line flag is set. The ops are intentionally minimal so that they can be stripped late in the pipeline.</p> <ul> <li><code class="language-plaintext highlighter-rouge">arx.prof.begin handle: i64 {counters = [cycles, bytes_rd, bytes_wr]}</code></li> <li><code class="language-plaintext highlighter-rouge">arx.prof.end handle: i64</code></li> </ul> <p>The pass places <code class="language-plaintext highlighter-rouge">begin/end</code> around candidate ops and region boundaries after bufferization, ensuring that the measured bytes reflect concrete <code class="language-plaintext highlighter-rouge">memref</code> layouts and copies.</p> <h3 id="112-runtime-counters-and-emission">11.2 Runtime counters and emission</h3> <p>On RVX, the runtime reads CPU cycle counters and DMA byte counters at <code class="language-plaintext highlighter-rouge">begin/end</code>. Each record is keyed by the IR handle (a stable 64-bit hash).</p> <pre style="color:#111827; background:#f3f4f6">
record {
  handle: 0x17a3...
  cycles:  239812
  bytes_rd:  1572864
  bytes_wr:   262144
  stalls: {dma_wait: 0.12, sram_bank: 0.03}
}
</pre> <p>In instrumented CPU-only builds, median overhead was about 2.1% on MNIST-sized graphs (illustrative; replace with measured values in <a href="#inference-time-arx">Table 1</a>).</p> <hr/> <h1 id="12-cost-model-details-and-selection-algorithm"><strong>12. Cost Model Details and Selection Algorithm</strong></h1> <h3 id="121-timing-model">12.1 Timing model</h3> <p>For a region $R$ with tiled compute and double buffering:</p> \[T_{\mathrm{off}}(R) \approx T_{\mathrm{setup}} + \max\!\big(T_{\mathrm{dma}}(R), T_{\mathrm{cmp}}(R)\big) + T_{\mathrm{sync}}.\] <p>DMA time uses transferred bytes and effective bandwidth $B_{\mathrm{dma}}$, including packing/padding inflation $\rho \ge 1$:</p> \[T_{\mathrm{dma}}(R) = \frac{\rho\cdot(\mathrm{bytes}_{\mathrm{in}}+\mathrm{bytes}_{\mathrm{out}})}{B_{\mathrm{dma}}}.\] <p>Compute time is derived from MAC counts divided by peak MAC/s and corrected by a profile-derived efficiency $\eta\in(0,1]$:</p> \[T_{\mathrm{cmp}}(R) = \frac{\mathrm{MACs}(R)}{\eta\cdot P_{\mathrm{peak}}}.\] <p>The net benefit is $\Delta T(R) = T_{\mathrm{cpu}}(R) - T_{\mathrm{off}}(R)$.</p> <h3 id="122-resource-model">12.2 Resource model</h3> <p>Each candidate $c$ has a resource vector $\mathcal{R}(c)$ over {LUT, FF, DSP, BRAM, SRAM, DMA lanes}. Selection maximizes $\sum \Delta T(c)$ subject to $\sum \mathcal{R}(c) \le B$ with a benefit-density tie-breaker when budgets are tight.</p> <h3 id="123-region-formation">12.3 Region formation</h3> <p>Candidates are merged greedily into maximal regions when:</p> <ol> <li>data dependencies allow reordering or fusion,</li> <li>the merged tile still fits on-chip buffers, and</li> <li>the merged cost is superadditive after accounting for fewer host–device crossings.</li> </ol> <hr/> <h1 id="13-mnist-model-shapes-and-mapping-notes"><strong>13. MNIST Model Shapes and Mapping Notes</strong></h1> <p>For reproducibility and debugging, <a href="#mnist-shapes">Table 4</a> lists the operator shapes used in the early evaluation.</p> <p><a id="mnist-shapes"></a></p> <table> <caption>Table 4. MNIST operator shapes and the induced tiling on configuration B.</caption> <thead> <tr> <th style="text-align:left;">Op</th> <th style="text-align:left;">Input shape</th> <th style="text-align:left;">Weight shape</th> <th style="text-align:left;">Output shape</th> <th style="text-align:left;">Tile selection</th> </tr> </thead> <tbody> <tr> <td>conv1</td> <td>1×1×28×28</td> <td>16×1×3×3</td> <td>1×16×26×26</td> <td>M=16, N=16, K=16 blocks</td> </tr> <tr> <td>conv2</td> <td>1×16×13×13</td> <td>32×16×3×3</td> <td>1×32×11×11</td> <td>same as above</td> </tr> <tr> <td>gemm</td> <td>1×512</td> <td>512×10</td> <td>1×10</td> <td>M=16, N=16, K=16 with packing</td> </tr> </tbody> </table> <hr/> <h1 id="acknowledgments">Acknowledgments</h1> <p>This work was supported by the Institute of Information \&amp; Communications Technology Planning \&amp; Evaluation (IITP)grant funded by the Korea government (MSIT) (No.RS-2024-00459797, No.RS-2023-00277060, No.RS-2025-02217404, No.RS-2025-02214497, No.RS-2025-02216517)</p>]]></content><author><name>Yongin Kwon</name></author><category term="mlir"/><category term="compiler"/><category term="deep-learning"/><summary type="html"><![CDATA[TLDR]]></summary></entry><entry><title type="html">Do Sparse Autoencoders (SAEs) transfer across base and finetuned language models?</title><link href="https://unireps.org//blog/2025/do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/" rel="alternate" type="text/html" title="Do Sparse Autoencoders (SAEs) transfer across base and finetuned language models?"/><published>2025-02-06T00:00:00+00:00</published><updated>2025-02-06T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models</id><content type="html" xml:base="https://unireps.org//blog/2025/do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/"><![CDATA[<h1 id="tldr-executive-summary"><strong>TLDR</strong> (Executive Summary)</h1> <ul> <li>We explored <strong>whether Sparse Autoencoders (SAEs)</strong> can effectively transfer from base language models to their finetuned counterparts, focusing on two base models: <a href="https://huggingface.co/google/gemma-2b">Gemma-2b</a> <d-cite key="gemmateam2024gemmaopenmodelsbased"></d-cite> and <a href="https://huggingface.co/mistralai/Mistral-7B-v0.1">Mistral-7B-V0.1</a> <d-cite key="jiang2023mistral7b"></d-cite> (we tested finetuned versions for coding and mathematics respectively)</li> <li>In particular, we split our analysis into three steps: <ol> <li>We analysed the similarity (<strong>Cosine and Euclidian Distance</strong>) of the residual activations, which was <strong>highly correlated with the resulting transferability of the SAEs</strong> for the two models.</li> <li>We computed several performance metrics (L0 Loss, Reconstruction CE Loss, Variance Explained) of the base SAEs on the fine-tuned models. Almost all metrics agreed on a <strong>significant degradation of the SAE performance for the Gemma-2b</strong> model, and <strong>remained within a reasonable range for the Mistral-7B model</strong>, indicating a much better transferability.</li> <li>We took a further step by operationalizing the idea of transferability of SAE from base models to fine-tuned models by applying an <a href="https://transformer-circuits.pub/2023/monosemantic-features#phenomenology-universality">approach from Towards Monosemanticity</a><d-cite key="bricken2023monosemanticity"></d-cite> for studying feature universality through <strong>feature activation similarity</strong> and <strong>feature logit similarity</strong>. These similarity scores were mostly consistent with the results from the previous step, albeit with one caveat for the Gemma-2b model, suggesting that <strong>some SAE features may still transfer</strong> even if the overall SAE performance is poor for the finetuned model.</li> </ol> </li> <li>Overall, our results agree with <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">previous work that studied Instruct models</a><d-cite key="sae_finetuning"></d-cite>. That is, SAEs transferability seems to be model-dependent and sensitive to the finetuning process.</li> <li>We make our <a href="https://github.com/tommasomncttn/SAE-Transferability">code repository public</a> to facilitate future work in this direction.</li> </ul> <hr/> <h1 id="1-introduction-and-motivation">1. Introduction and motivation</h1> <h2 id="11-what-are-saes-and-why-do-we-care-about-them">1.1 What are SAEs and why do we care about them</h2> <p>We find ourselves in a world where we have machines that speak fluently dozens of languages, can do a wide variety of tasks like programming at a reasonable level, <strong>and we have no idea how they do it!</strong> This is a standard <strong>mechanistic interpretability</strong> (a.k.a. mech interp) pitch - a field that is trying to <strong>express neural networks’ behaviours as human-understandable algorithms</strong>, i.e. <strong>reverse engineer</strong> algorithms learned by a neural network (or a model, in short). The main motivation is that even though we know the exact form of computation being done by the model to transform the input (e.g. text prompt) to the output (e.g. text answer), we don’t know <em>why</em> this computation is doing what it’s doing, and this is a major concern from a standpoint of AI Safety. The model can perform the computation because it’s genuinely trained to perform the task well, or because it learned that doing the task well correlates with its other learned goals like gaining more power and resources. Without understanding the computation, we have no direct way of distinguishing between the two.</p> <p>The solution proposed by mechanistic interpretability is closely analogous to reverse engineering ordinary computer programs from their compiled binaries. In both cases, we have an intrinsically non-interpretable model of computation - a sequence of binary instructions performed on a string of 0s and 1s, and the (mathematical) function of the neural network’s architecture applied with its learned parameters (weights)<d-footnote>This is a pretty important analogy to understand and you can read more about it in [this Anthropic post](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)<d-cite key="Olah_2022"></d-cite> where it's explained better. </d-footnote>. Programmers know that a natural way to think about computer programs is mapping <strong><em>variables</em></strong> into other variables (or new states of existing variables), starting from some pre-initialized state. So, reverse engineering complied binaries boils down to (oversimplifying) identifying binary memory segments that correspond to variables, tracking how these segments change as the program is being executed, coming up with the explanations of the purpose of these variables, and ultimately arriving at the replication of the program source code - a sequence of human-understandable instructions.</p> <p>But what makes us think that the same is possible for neural networks, especially the ones as large as the current Large Language Models (LLMs)? In particular, why should we even expect that neural networks solve tasks similarly to humans, and thus adopt the same “variable-centered” model of computation? While the proof-of-existence for the first question appeared relatively early (see <a href="https://distill.pub/2020/circuits/zoom-in/">Circuits thread by Chris Olah et al.</a><d-cite key="olah2020zoom"></d-cite> for CNNs or a <a href="https://arxiv.org/abs/2301.05217">more recent work by Neel Nanda et al.</a><d-cite key="nanda2023progressmeasuresgrokkingmechanistic"></d-cite> for language models), the second question is a more general claim, and thus requires more general evidence. The first fundamental work that provided such evidence was the <a href="https://transformer-circuits.pub/2023/monosemantic-features">“Towards Monosemanticity” paper by Anthropic</a><d-cite key="bricken2023monosemanticity"></d-cite>, which introduced Sparse Autoencoders (SAEs) for interpreting the language models’ activations. The activations are any intermediate state of the models’ computation, such as residual stream, MLP layers etc. and can be seen as analogous to a program’s memory state. And just as the program’s memory state can be decomposed into variables, the <strong>main purpose of SAEs is to decompose models’ activations into features</strong>.</p> <p>A feature, in general, is a fuzzy term, and you can find some good attempts to define it <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=BQds7CQ8ytq2rolt7p0XQPbt">here</a><d-cite key="nanda_2022"></d-cite>. For this post we’ll use the analogy with variables and link it to a very general definition of a feature as “<em>a</em> <em>property of the input</em>”. The link is pretty natural: <strong>the types and the number of variables a programmer needs to solve a task depends on the task itself</strong> (i.e. on the problem input). So for a model it would seem reasonable if it used different kinds of variables/features depending on its input: you don’t need a feature “this line is inside a for-loop in Python” in a poetry text, or a feature “this word rhymes with ‘sunset’” in the Python code. And given that models have a finite amount of parameters (which limits a total number of variables they can use), we should expect that they will utilize this kind of input-specificity to use as many unique features as they need to perform a specific task.</p> <p>Why are sparse autoencoders called sparse? It’s actually deeply linked with the idea from the previous paragraph: if you want to use many features in a limited activation space (limited by a number of neurons), you have to exploit the fact that <strong>for any input, most of the features will not be there</strong>. So given that modern language models are trained to predict a next token in a huge variety of possible inputs, we should expect that any feature learned by the model will be <strong>sparse</strong>, i.e. it <strong>will be used by the model only for a small fraction of all possible inputs</strong>.</p> <p>But wait, how is it even possible for a model to learn input-specific features if it has a low-dimensional activations space (where dimension equals the number of neurons) but a very high-dimensional input space? The answer is <strong><em>superposition</em></strong> - an idea of exploiting feature sparsity to store more features than dimensions in the activation space. It has a rich mathematical background and we invite an unfamiliar reader to learn more about it in the <a href="https://transformer-circuits.pub/2022/toy_model/index.html">“Toy Models of Superposition” paper by Elhage et al.</a><d-cite key="elhage2022superposition"></d-cite></p> <p>Coming back to SAEs, they were introduced with all of these ideas in mind to <em>solve superposition</em>, i.e. to recover more than <em>n</em> features in an <em>n</em>-dimensional activation space of a model. How are they supposed to do it? The answer is once again in the name - <em>autoencoders</em>, which means that SAEs are neural networks with the “autoencoder” architecture, which is illustrated in a diagram below (borrowed from the great <a href="https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html">Adam Karvonen’s post</a><d-cite key="Karvonen_2024"></d-cite>):</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/sae.png" alt="My Image" width="500"/> </div> <p>So the model activations are “encoded” into a high-dimensional vector of feature activations (top right, note that it always has many more elements than the model’s input), and this high-dimensional vector (a.k.a. “code”) is “decoded” back to reconstruct the input, hence the name “autoencoder”. We advise the reader to take a quick look at the <a href="https://transformer-circuits.pub/2023/monosemantic-features#appendix-autoencoder">“Towards monosematicity” appendix</a><d-cite key="bricken2023monosemanticity"></d-cite> where this architecture is presented mathematically<d-footnote>Note that it's different from the diagram in two ways: adding biases vectors **b** and using a transposed encoder/decoder matrix compared to what is seen in the diagram.</d-footnote>, but the core point to understand is that we’re interested in the right part of the above diagram: <strong>how the reconstructed activations are decomposed into a linear combination of feature vectors</strong> from the Decoder matrix (with the weights of a linear combination equal to SAE <em>feature activations</em>, due to how matrix-vector multiplication works). Mathematically, it means that for each input \(x^j\) (which is the model’s activation vector at the place where we ‘attach’ the SAE - residual layer, hidden head activations etc.), we’re looking to express it in the following form:</p> \[\mathbf{x}^j \approx \mathbf{b} + \sum_i f_i(\mathbf{x}^j) \mathbf{d}_i\] <p>where \(f_i(\mathbf{x}) = \text{ReLU}\left( \mathbf{W}_{enc} \mathbf{x} + \mathbf{b}_{enc} \right)_i\) are the feature activations that are computed in the left (“encoder”) part of the diagram, and \(\mathbf{d}_i\) are the rows of the decoder matrix (or columns, if you take the transpose and multiply from the other side). Note that the diagram omits bias vectors \(\mathbf{b}\) for simplicity, but conceptually they don’t change much: instead of decomposing the activation space, we’re decomposing a translation of that space by a fixed vector (because this is just easier for an SAE to learn).</p> <p>If you think about it, it’s exactly what we hoped to do in an analogy with decomposing program memory into variable names! The variables are now features - <strong>vectors (directions) in the activation space</strong>. And <em>if</em> the autoencoder is doing a good job at reconstructing the input, we can expect that this decomposition (and hence the features) to make sense!</p> <p>The last part is tricky though. Unlike variables that are deliberately used by humans to write sensible algorithms, there is no reason to expect that the features we recover with an SAE will be <em>interpretable</em> in a sense that a human can understand on which inputs they activate and can predict their “roles” based on that (e.g. which tokens they help to predict). But this is where the <em>sparsity</em> condition comes in: we don’t only want an SAE to reconstruct the input from a high-dimensional feature-activation representation, <strong>but we also want this representation to be sparse</strong>, i.e. have only a handful of non-zero feature activations at a time. We already touched on the reason for this - the hope is that we’ll be able to recover the “true” features used by the model in this way<d-footnote>It's quite a slippery area to consider the logical relationship between the feature quality of being "truly used" by the model (analogously to correctly recovered variables from the compiled binary) and its interpretability. If the model came up with some genius way to solve a particular task using features no human can comprehend, would they still be considered as interpretable? The answer can vary from "no" to "kind of yes", because it can be argued that humans with their evolutionally developed problem-solving skills can eventually understand (i.e. interpret) how things work, even though it may not be obvious at a first glance. It's also discussed by Neel Nanda [here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=dzkF4Sh89hg1GUJj5h2TiGVx)<d-cite key="nanda_2022"></d-cite> </d-footnote>. And the way this is achieved is by imposing an L1-loss penalty on the feature activation vector, which intuitively incentivizes the model to not learn any features unless they are really useful in reconstructing the input<d-footnote>There's also a better justified mathematical reason for sparsity, greatly explained [here](http://ufldl.stanford.edu/tutorial/unsupervised/SparseCoding/)<d-cite key="Ng"></d-cite>. Essentially, by learning to decompose the model's activation space into feature activations, we're trying to find an overcomplete basis of feature directions (a basis with more than n vectors in an n-dimensional space), which is impossible to do without imposing some additional criteria. The ["Toy Models of Superposition"](https://transformer-circuits.pub/2022/toy_model/index.html)<d-cite key="elhage2022superposition"></d-cite> is also incredibly helpful to refine one's intuition about this. </d-footnote>.</p> <h3 id="111-sae-features-for-ai-safety">1.1.1 SAE features for AI Safety</h3> <p>The traditional view in mech interp has been that <strong>one cannot interpret the model’s weights if one cannot interpret the neurons that the weights are connecting</strong>. But due to the <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=RDddls6iedarJZiVvLWwnaYI">neurons polysemanticity</a><d-cite key="nanda_2022"></d-cite> (a consequence of superposition), interpreting individual neurons in the language model is extremely hard if at all possible. That’s where SAEs come to the rescue: by revealing the directions in the neuron activation space (i.e. features) that have a clear, interpretable meaning, they allow for a new form of <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=GeeSfnALcakOYfxQcKaAwV6x">circuits</a><d-cite key="nanda_2022"></d-cite> analysis: instead of interpreting weights between neurons, we can instead interpret weights connecting features. Thus the SAE features potentially serve as a new “basis” for circuit analysis, and some of the recent work e.g. by <a href="https://arxiv.org/abs/2403.19647">Marks et al.</a><d-cite key="marks2024sparsefeaturecircuitsdiscovering"></d-cite> and <a href="https://transformer-circuits.pub/2024/march-update/index.html#feature-heads">Batson et al.</a><d-cite key="Batson_Chen_Jones_2024"></d-cite> has already started exploring this idea and producing the first results.</p> <p>So what does this mean for AI Safety? We’ll cite the Anthropic team’s view on this topic (layed out in their <a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html#safety">“Interpretability Dreams”</a><d-cite key="Olah_2023"></d-cite> post and in the <a href="https://transformer-circuits.pub/2022/toy_model/index.html#strategic">“Strategic Picture” section</a><d-cite key="elhage2022superposition"></d-cite> of the Toy Models paper):</p> <blockquote> <p>We’d like a way to have confidence that models will never do certain behaviors such as “deliberately deceive” or “manipulate.” Today, it’s unclear how one might show this, but we believe a promising tool would be the ability to identify and enumerate over all features.</p> </blockquote> <blockquote> <p>Ultimately we want to say that a model doesn’t implement some class of behaviors. Enumerating over all features makes it easy to say a feature doesn’t exist (e.g. “there is no ‘deceptive behavior’ feature”) but that isn’t quite what we want. We expect models that need to represent the world to represent unsavory behaviors. But it may be possible to build more subtle claims such as “all ‘deceptive behavior’ features do not participate in circuits X, Y and Z.”</p> </blockquote> <p>Summarizing, the hope is to be able to prove statements of the following form:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/eq.png" alt="My Image" width="500"/> </div> <h2 id="12-finetuning-models-is-a-challenge-to-ai-safety---saes-to-the-rescue">1.2 Finetuning models is a challenge to AI safety - SAEs to the rescue?</h2> <p>After outlining the procedure behind SAE-interpretability, we can answer a more general question: why is it relevant to translate the matrix language of neural networks (not more understandable to us than binary code) into a human-readable algorithmic language? There are several reasons, but, among the others, once we are able to do so, we can understand what features of an input a model identifies before predicting an answer. This can allow us to identify when a model is learning to deploy features spuriously correlated with the actual labels (an intuitive example <a href="https://ar5iv.labs.arxiv.org/html/1712.02950#:~:text=these%20image%20domains.-,2,Hidden%20Information,-We%20begin%20with">here</a><d-cite key="DBLP:journals/corr/abs-1712-02950"></d-cite>) or when the model is even <a href="https://arxiv.org/abs/2310.06824">lying to us</a><d-cite key="marks2024geometrytruthemergentlinear"></d-cite>. In both of these cases, it is a primary safety concern that these behaviors are not occurring in our model when used in production. Moreover, SAE-interpretability allows us to gain some insight into solving these problems precisely!</p> <p>Nevertheless, reality is often rougher than abstraction, and mechanistic interpretability suffers from one big problem: once we crack the interpretation of a model, we are only able to decode what is going on inside <strong>a singular, particular model, and not all models with the same architecture and different weights</strong>. Luckily, to have a model that shows emergent abilities, <a href="https://epochai.org/blog/compute-trends">we need a lot of compute</a><d-cite key="computetrends"></d-cite>, which remarkably restricts the Pareto frontier of competitive models and therefore the number of pre-trained models that we need to interpret. Therefore, one could think that if we manage to get some good SAE-interpreters for these few, we will be done. This may not be true! While indeed there are few state-of-the-art models, there are tons of finetuned versions of them (<a href="https://twitter.com/ClementDelangue/status/1839375655688884305?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1839375655688884305%7Ctwgr%5Eb537ad0b54dfc2d9ec69e2b01a337c5b0ce9d4e9%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Freadwrite.com%2Fai-startup-hugging-face-reaches-one-million-downloadable-ai-models-thats-a-lot-you-have-never-heard-of%2F">hugging face reached 1 million of models</a>), which are quite cheap to obtain compared to pretraining. <strong>If a simple finetuning will make the model uninterpretable, then we might be in danger</strong>. This could be the case, as <a href="https://arxiv.org/abs/2310.02949">previous studies</a><d-cite key="yang2023shadowalignmenteasesubverting"></d-cite> showed that alignment can be erased with a small finetuning. Then we ask ourselves:</p> <p><em>Is the interpretability of a model as weak as alignment to finetuning?</em></p> <p>In this post, we try to answer these questions and extend the positive results derived from a similar study by <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite>, where SAEs for the residual stream have been shown to be easily transferable (at the cost of some finetuning).</p> <p>Lastly, we want to remark how this kind of study derives its importance from the weakness of outer alignment forced by some ad-hoc finetuning. Indeed, if interpretability is more resistant to being broken than alignment, the path towards AI safety could be reached via <a href="https://www.cold-takes.com/high-level-hopes-for-ai-alignment/">digital neuroscience</a><d-cite key="Karnofsky_2023"></d-cite>, rather than simply through external finetuning.</p> <hr/> <h1 id="2-problem-setup">2. Problem setup</h1> <p>In choosing finetuned models to work with, we tried to strike a balance between the potential relevance of these models (how many people will actually use similar models), and the availability of pre-trained SAEs from the <a href="https://jbloomaus.github.io/SAELens/">SAELens</a><d-cite key="bloom2024saetrainingcodebase"></d-cite> library we used. So, we arrived at the following models and their finetunes:</p> <ol> <li>Gemma-2b (v1) -&gt; <a href="https://huggingface.co/shahdishank/gemma-2b-it-finetune-python-codes">Gemma-2b-it-finetuned-python-codes</a><d-cite key="gemmateam2024gemmaopenmodelsbased"></d-cite> finetune on <strong>Python code</strong> by Dishank Shah.</li> <li>Mistral-7B (v0.1) -&gt; <a href="https://huggingface.co/meta-math/MetaMath-Mistral-7B">MetaMath-Mistral-7B</a><d-cite key="jiang2023mistral7b"></d-cite> finetune on <strong>math problems</strong> by Meta from their <a href="https://arxiv.org/abs/2309.12284">MetaMath paper</a><d-cite key="yu2024metamath"></d-cite> by Yu et al.</li> </ol> <p>We then loaded the following SAEs for these models from SAELens (SAE layer numbering starts from 0):</p> <table style="margin: auto; text-align: center;"> <thead> <tr> <th>Model</th> <th>SAE Release</th> <th>SAE Layer</th> <th>N Features</th> </tr> </thead> <tbody> <tr> <td>Gemma-2b (v1)</td> <td>gemma-2b-res-jb by Joseph Bloom</td> <td>Residual layer #6</td> <td>16384</td> </tr> <tr> <td>Mistral-7B (v0.1)</td> <td>mistral-7b-res-wg by Josh Engels</td> <td>Residual layer #8</td> <td>65536</td> </tr> </tbody> </table> <p>Two important things to note:</p> <ul> <li>Gemma-2b SAE was trained on the <em>base</em> Gemma-2b model, while our Gemma-2b finetune was obtained from the <em>instruct</em> model, so there was one more “finetuning step” compared to the Mistral-7B case.</li> <li>Both finetunes that we used are <em>full</em> finetunes (with respect to the base model), i.e. no layer was frozen during the finetuning process. This is important for our SAE study, because all SAEs would trivially generalize (in terms of their reconstruction quality) if they were applied at the layer where activations are not affected a priori by the finetuning process.</li> </ul> <h2 id="21-studying-default-transferability">2.1 Studying “default” transferability</h2> <p>Similarly to what <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite> did with the instruct models, we’ll study the SAE transferability “by default”. That is, we’ll take an SAE trained on the base model, and apply it to the finetuned model to see if it maintains its performance (operationalized below). We won’t do any additional finetuning of our SAEs (on the activations from the finetune model), but as the same results from <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite> indicate: even when SAEs do not transfer by default, they can be finetuned relatively cheaply to recover their performance.</p> <p>Prior to evaluating the SAEs’ performance, we computed different similarity metrics for residual stream activations at the specific layer our SAEs are used for. The goal was to obtain some sort of a prior probability that our SAEs will transfer to the finetune model: the more similar the activations are, the higher is the (expected) probability that our SAEs will transfer. On the one hand, this analysis can be used as a <em>first step to select a fine-tuned model</em> from the thousands available on Hugging-Face. On the other hand, further studies can try to analyze <em>whether the phenomenon of SAE transferability actually correlates with the difference between activations</em> of the base and fine-tuned models (which we treat here only as an unproven heuristic).</p> <h2 id="22-evaluating-saes-performance">2.2 Evaluating SAEs performance</h2> <p>Designing rigorous approaches to evaluate the SAEs’ performance is an open problem in mechanistic interpretability. The main complicating factor is that we’re interested not so much in the SAEs reconstructed output, but rather in <strong>the SAE feature activations and feature vectors</strong>. However, measuring whether the SAEs features are interpretable or whether the features “are truly used by the model” is not straightforward. For our work, we’ll just start with computing standard evaluation metrics proposed either in the original “Towards monosemanticity” paper, or used in the later work, <a href="https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream">e.g. this one by Joseph Bloom</a><d-cite key="jbloom_lesswrong"></d-cite>:</p> <ol> <li><strong>L0 loss</strong>, namely the number of non-zero values in the feature activations vector. If the features retain their sparsity, we should expect L0 loss to be low compared to the total number of features, with the fraction being usually less than 1% (\(\frac{L_0}{N_{\text{features}}} &lt; 0.01\))</li> <li><strong>Reconstruction Cross-Entropy (CE) loss</strong> (a.k.a. substitution loss) which is computed as follows: <ol> <li>Run the model up to the layer where we apply the SAE, get this layer’s activations</li> <li>Run the activations through the SAEs, obtaining the reconstructions</li> <li><strong>Substitute</strong> the original activations with the reconstructed activations, continue the forward pass of the model, and get the corresponding cross-entropy loss</li> </ol> </li> <li><strong>Variance explained</strong>, is one of the standard ways to measure the difference of original activations and the activations reconstructed by the SAE. Specifically, we’ll use \(R^2\) score a.k.a. <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">Coefficient of determination</a></li> <li><strong>Feature density histograms</strong>: <a href="https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream#Why_can_training_Sparse_AutoEncoders_be_difficult__">as explained by Joseph Bloom</a><d-cite key="jbloom_lesswrong"></d-cite>, ideally the features should be “within good sparsity range”: <strong>not too sparse</strong> (e.g. when the features are “dead” and never activate) and <strong>not too dense</strong> (e.g. activating in more than 10% of the inputs). In both edge cases, anecdotally the features are mostly uninterpretable. One (rather qualitative) way to check this is to plot feature histograms: <ol> <li>Run a given sample of tokens through the model, and get the SAE feature activations.</li> <li>For each feature, record the number of times (tokens) it had a non-zero activation.</li> <li>Divide by the total number of tokens to get the fraction, and take the log10 of it (adding some epsilon value to avoid log-of-zero)</li> <li>Plot the histogram of the resulting log-10 fractions (the number of histogram samples equals to the number of features)</li> </ol> </li> </ol> <p>We’ll compute these metrics first for the base model and its SAE to get a baseline, then for the finetuned model with the same SAE, and compare the resulting metrics against the baseline<d-footnote>Even though density histograms are not technically a metric, we can infer quantitative metrics from them like the number of dead features</d-footnote>. The dataset used in both cases is the original training dataset of the corresponding SAE:</p> <ol> <li><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb">Fineweb</a><d-cite key="fineweb"></d-cite> dataset for Gemma-2b.</li> <li><a href="https://huggingface.co/datasets/monology/pile-uncopyrighted">The Pile</a><d-cite key="thepile"></d-cite> dataset for Mistral-7B.</li> </ol> <p>Based on the feature density histograms, we additionally zoomed in on individual features to see how well they transfer using <a href="https://transformer-circuits.pub/2023/monosemantic-features#phenomenology-universality">feature activation similarity and logit weight similarity</a><d-cite key="bricken2023monosemanticity"></d-cite>, as elaborated in the later section of this post.</p> <hr/> <h1 id="3-how-similar-are-residual-activations-of-finetuned-models">3. How similar are residual activations of finetuned models?</h1> <p>Before analyzing the SAE metrics on the finetuned models, we will visualize some easier computations on the <strong>residual</strong> activations (at the residual stream of the layer where we apply the corresponding SAE) to get a sense of the SAE transferability. Specifically, we are interested in the similarities between the base and finetuned model activations. We consider two metrics: the Cosine Similarity and the Euclidian Distance, for the model and datasets specified above with the <a href="https://huggingface.co/shahdishank/gemma-2b-it-finetune-python-codes">Gemma-2b Python-codes</a> and <a href="https://huggingface.co/meta-math/MetaMath-Mistral-7B">Mistral-7b MetaMath</a><d-cite key="yu2024metamath"></d-cite> finetunes respectively.</p> <p>Computing the Cosine Similarities and Euclidian Distances of the activations yields a tensor of shape <code class="language-plaintext highlighter-rouge">[N_BATCH, N_CONTEXT]</code> (each token position is determined by its batch number and position in the context). A simple metric to start with is to consider the global mean of the Cosine Similarities of the activations across both batch and context dimensions, giving a single scalar representing the overall similarity. This can be seen in the following table:</p> <table> <thead> <tr> <th>Model/Finetune</th> <th>Global Mean (Cosine) Similarity</th> </tr> </thead> <tbody> <tr> <td>Gemma-2b/Gemma-2b-Python-codes</td> <td>0.6691</td> </tr> <tr> <td>Mistral-7b/Mistral-7b-MetaMath</td> <td>0.9648</td> </tr> </tbody> </table> <p>This already suggests much better transferability of the Mistral-7b SAE for its MetaMath finetune. For a more fine-grained comparison, we flatten the similarities into a <code class="language-plaintext highlighter-rouge">N_BATCH * N_CONTEXT</code> vector and plot the histogram across all tokens:</p> <p>Gemma-2b - Cosine Similarity Histogram</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.1.png" alt="My Image" width="700"/> </div> <p>Mistral-7b - Cosine Similarity Histogram</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.2.png" alt="My Image" width="700"/> </div> <p>Gemma-2b - Euclidian Distance Histogram</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.3.png" alt="My Image" width="700"/> </div> <p>Mistral-7b - Euclidian Distance Histogram</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.4.png" alt="My Image" width="700"/> </div> <p>We can see how the Cosine Similarities for Mistral-7b are concentrated around a value close to 1, whereas the Gemma-2b similarities are more spread around the mean of 0.66 (higher variance). The Euclidian Distances histogram shows a similar distinction, with the Gemma-2b distances being spread around a mean of around 120, while the bulk of Mistral-7b distances stay at a low value.</p> <p>We also visualize the per-context mean of Cosine Similarities and Euclidian Distances. We compute the mean across batches but preserve the context dimension, giving a tensor of shape <code class="language-plaintext highlighter-rouge">[N_CONTEXT]</code>, which reflects how similarity changes over the context length.</p> <p>Gemma-2b - Cosine Similarity Context Line</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.5.png" alt="My Image" width="700"/> </div> <p>Mistral-7b - Cosine Similarity Context Line</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.6.png" alt="My Image" width="700"/> </div> <p>Gemma-2b - Euclidian Distance Context Line</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.7.png" alt="My Image" width="700"/> </div> <p>Mistral-7b - Euclidian Distance Context Line</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.8.png" alt="My Image" width="700"/> </div> <p>In the above, we can see how the similarities and distances stabilise quickly after a few tokens of context, albeit around different values. Both models start with close to 1 similarity for the first token, and then stabilize after a few tokens.</p> <p>These results already anticipate a considerable difference in the transferability of the SAEs for the two models, which will be explored more in-depth in the following section.</p> <hr/> <h1 id="4-how-well-do-the-base-saes-work-on-the-finetuned-models">4. How well do the base SAEs work on the finetuned models?</h1> <h2 id="41-methodology">4.1 Methodology</h2> <p>In this section, we’ll compute a set of standard SAE metrics for base and finetuned models, using the same base SAE in both scenarios (i.e., the SAE that was trained on the base model activations):</p> <ol> <li>For the <strong>base model</strong>: <ol> <li>we sample input tokens from the <strong>original SAE training dataset</strong></li> <li>pass the tokens through the base model to get <strong>the model’s activations</strong></li> <li>pass the activations through the SAE to <strong>get the feature activations</strong></li> <li>complete the forward pass of the base model to <strong>get the final loss</strong> (used afterward for the reconstructed loss)</li> </ol> </li> <li>Then we repeat the same steps for the <strong>finetuned</strong> <strong>model</strong>, using the same tokens dataset</li> <li>Finally, we compute the metrics mentioned in the Evaluating SAEs performance section.</li> </ol> <h2 id="42-technical-details">4.2 Technical Details</h2> <p>Before delving deeper into the results, we want to point out three technical details:</p> <ol> <li>The sample size used across nearly all experiments is <strong>256K tokens</strong></li> <li> <p>Similarly to <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite> we observed a major numerical instability when computing our reconstruction loss and variance explained metrics. As the authors noted:</p> <blockquote> <p>SAEs fail to reconstruct activations from the opposite model that have outlier norms (e.g. BOS tokens). These account for less than 1% of the total activations, but cause cascading errors, so we need to filter these out in much of our analysis.</p> </blockquote> </li> <li> <p>To solve this problem we used a similar outlier filtering technique, where an outlier is defined as <em>an activation vector whose (L2) norm exceeds a given threshold</em>. We tried several ways to find a “good” threshold and arrived at values similar to those used by <em>Kissane et al</em>:</p> <ul> <li><strong>290 norm value</strong> for the Gemma-2b model</li> <li><strong>200 norm value</strong> for the Mistral-7B model</li> </ul> <p>Using these threshold values, we found that <strong>only 0.24% activations are classified as outliers in the Gemma-2b model</strong>, and <strong>0.7% in the Mistral-7B</strong>, agreeing with the Kissane et al. result that these outliers account for less than 1% of activations. It should be noticed, however, that we <em>only used this outlier filtering technique for our reconstruction loss &amp; variance explained</em> experiments to avoid numerical errors. In practice, it means that for this experiment the true sample size was a little smaller than for the other experiments, equal to \(\left( 1 - \text{outlier_fraction} \right) \times 256{,}000\) with the \(\text{outlier_fraction}\) defined above.</p> </li> </ol> <h2 id="43-results">4.3 Results</h2> <p>In the following table, we report the results for the first experiment with the <strong>Mistral</strong> model pair:</p> <table style="margin: auto; text-align: center;"> <thead> <tr> <th>Model\\Metric</th> <th>L0 Loss</th> <th>Clean CE Loss</th> <th>Reconstruction CE Loss</th> <th>Loss Delta</th> <th>$$R^2$$ Score (Variance Explained)</th> <th>Dead Features (%)</th> </tr> </thead> <tbody> <tr> <td>Mistral-7B</td> <td>83.37</td> <td>1.78</td> <td>1.93</td> <td><b>0.15</b></td> <td>0.68</td> <td>0.76%</td> </tr> <tr> <td>Mistral-7B MetaMath</td> <td>90.22</td> <td>1.94</td> <td>2.1</td> <td><b>0.16</b></td> <td>0.58</td> <td>0.64%</td> </tr> </tbody> </table> <p>As you can see, the L0-Loss of the features and variance explained increase a bit, but the reconstruction loss delta is almost the same! It suggests that our Mistral SAE may still transfer after finetuning, although with a slightly worse reconstruction quality. Let’s compare these results with the Gemma-2b and its Python finetune:</p> <table style="margin: auto; text-align: center;"> <thead> <tr> <th>Model\\Metric</th> <th>L0 Loss</th> <th>Clean CE Loss</th> <th>Reconstruction CE Loss</th> <th>Loss Delta</th> <th>$$R^2$$ Score (Variance Explained)</th> <th>Dead Features (%)</th> </tr> </thead> <tbody> <tr> <td>Gemma-2b Base</td> <td>53.59</td> <td>2.65</td> <td>3.16</td> <td>0.51</td> <td>0.97</td> <td>48.1%</td> </tr> <tr> <td>Gemma-2b Python-codes</td> <td>84.74</td> <td>3.29</td> <td><b>7.5</b></td> <td><b>4.21</b></td> <td><b>-10.27</b></td> <td>0.1%</td> </tr> </tbody> </table> <p>Now, this is what <em>bad</em> SAE transferability looks like! But actually this should come as no surprise after the <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite> result: they concluded that Gemma-2b SAEs do not transfer even between the base and the <em>instruct</em> models, so when you add an additional finetuning step on top of the instruct, it’s completely expected that the metrics will get even worse. The authors explain this behavior with an abnormal weights deviation in the instruct model:</p> <blockquote> <p>Here we show that the weights for Gemma v1 2B base vs chat models are unusually different, explaining this phenomenon (credit to Tom Lieberum for finding and sharing this result):</p> </blockquote> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.1.png" alt="My Image" width="700"/> </div> <p>But what effect does this have on the SAE features? Well, we could expect that if an SAE is no longer able to reconstruct the input activations, it will always “hallucinate” - any features it “detects” will not make any sense. Let’s see if this expectation holds in practice for the Gemma-2b model.</p> <p>We’ll start with the feature activations histogram plot. In general, this kind of histogram gives little insight since you will always have a large mode at 0 due to feature sparsity, and some kind of log-normal distribution at non-zero activations. Indeed, this is what happens in the base Gemma-2b model, when we plot its log10 feature activations histogram:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.2.png" alt="My Image" width="700"/> </div> <p>Two things to note:</p> <ul> <li>The first bar’s count value is <strong>clipped</strong> - it’s much larger than 900k, equal to more than 6 million.</li> <li>We used a smaller sample size for this experiment due to the need to store all the feature activations in memory to plot the histogram - here the sample size is equal to <strong>128K</strong>.</li> </ul> <p>With this in mind, let’s compare it with the same kind of histogram for our Gemma-2b finetune (where the features are given by the same SAE):</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.3.png" alt="My Image" width="700"/> </div> <p>If that’s not a characterization for “cursed”, we don’t know what is! Instead of a nice bell curve, we now have some sort of a 3-mode monster in the non-zero activations section. To be clear - nothing like that was present when we repeated this experiment for the Mistral-7B: we obtained the well-expected bell curves with similar mean and standard deviation for both base and finetuned models. We don’t have a good explanation for this Gemma-2b anomaly, but we’ll try to give some deeper insight into what happens with the SAE features in the next section.</p> <p>Let’s move on to the feature densities plot, which was produced as described in the Evaluating SAEs Performance section. Starting from Gemma-2b:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.4.png" alt="My Image" width="700"/> </div> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.5.png" alt="My Image" width="700"/> </div> <p>As expected from the above results, the two plots have little in common. We see that most of our dead features (in the base model) turn alive in the finetuned one! To see where exactly these dead feature densities land in the finetuned model (what are their new densities), we also made a parallel coordinate plot (below we show two versions of the same plot: with different density ranges highlighted):</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.6.png" alt="My Image" width="700"/> </div> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.7.png" alt="My Image" width="700"/> </div> <p>So it looks like the dead features spread out quite widely in the finetuned model, contributing to more probability mass before the -3 log-density. As for the dense features (-4 to -1 log density) in the base model, their density interval gets squeezed to (-3, -1) in the finetuned model, causing a sharp mode near the -2.5 log-density value.</p> <p>We’ll continue the Gemma-2b investigation in the next chapter, and conclude this section with the Mistral-7B feature density histograms:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.8.png" alt="My Image" width="700"/> </div> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.9.png" alt="My Image" width="700"/> </div> <p>We can see that for Mistral the feature densities distribution almost doesn’t change after the model finetuning! The only slight difference is in the number of dead features: the finetuned Mistral has around 80 dead features less than the base one. To zoom in closer, we also show the parallel coordinate plot:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.10.png" alt="My Image" width="700"/> </div> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.11.png" alt="My Image" width="700"/> </div> <p>So yes, a small number of features do turn alive, but also some features (even a smaller amount) turn dead in the finetuned model! Overall though, the feature densities look very similar, with the Pearson correlation of their log10 densities equal to 0.94 (versus 0.47 for the Gemma-2b case).</p> <hr/> <h1 id="5-do-the-base-sae-features-transfer-to-the-finetuned-model">5. Do the base SAE features transfer to the finetuned model?</h1> <p>We want to motivate this section with a more thoughtful consideration of the question <strong>what is the best way to operationalize SAE transferability</strong>. In the previous section, we simply checked the standard SAE evaluation metrics to see how well they reconstruct the activations. But this doesn’t necessarily reflect the main goal of using SAEs - <strong>interpreting the model.</strong></p> <p>As noted in the SAE features for AI Safety section of our post, the end goal of using SAEs for interpretability is to be able to <strong>use features as the basis for circuit analysis</strong>. And if we assume that some kind of circuit analysis has been done for the base model to prove that it doesn’t implement certain undesirable behaviors, the most ambitious operationalization of SAE transferability (for AI Safety) would be the ability to apply <strong>the same kind of circuit analysis with the same SAE</strong> (or the finetuned one) <strong>to prove or disprove that the finetuned model is safe.</strong></p> <p>In our case of studying transferability “by default”, the better way to demonstrate it is to show that our SAE features “stay relevant” in the finetuned model, so that we can expect that they still potentially serve as the basis for circuit analysis. Showing this rigorously would be a really difficult task (partly because there’s no standard way to do circuit analysis in the SAE basis yet) and it’s out of scope for this blog post. What we did instead is apply an <a href="https://transformer-circuits.pub/2023/monosemantic-features#phenomenology-universality">approach from Towards Monosemanticity</a><d-cite key="bricken2023monosemanticity"></d-cite> for studying features <strong>universality</strong>:</p> <ul> <li>Normally to study if a feature from model A is conceptually the same (has the same “role” in the model) as another feature in the model B, one can compute <ul> <li><strong>feature activation similarity</strong>: represent a feature as a vector of its activations across a given sample of tokens, obtaining a <em>feature activations vector →</em> do it for model A’s feature, model B’s feature and compute a <strong>correlation between their activations vectors</strong>.</li> <li><strong>feature logits similarity:</strong> represent a feature as a vector of its <a href="https://transformer-circuits.pub/2023/monosemantic-features#feature-arabic-effect">logit weights</a><d-cite key="bricken2023monosemanticity"></d-cite> (for each token of the vocab a logit weight is the relative probability of that token as predicted by the feature direct effect), obtaining a <em>feature logit vector→</em> do it for model A’s feature, model B’s feature and compute a <strong>correlation between their logit vectors</strong>.</li> </ul> </li> <li>So, we call model A our base model, model B - the corresponding finetune, and compute feature activation similarity and logits similarity for a given sample of the SAE features (which are the same for the base and finetuned models).</li> </ul> <p>This can be seen as a (very) rough proxy for “the feature is doing the same job in the finetuned model”, and we call it the “<strong>feature transferability test</strong>”.</p> <h2 id="51-feature-selection-procedures">5.1 Feature Selection Procedures</h2> <p>Conceptually, dead features are completely different from the ordinary features: <a href="https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream">as explained by Joseph Bloom</a><d-cite key="jbloom_lesswrong"></d-cite>, they represent permanently lost capacity in an SAE and thus are merely an artifact of the SAE training<d-footnote>Essentially, an SAE is saying “If I cannot find relevant features for reconstructing my input anymore, I’m going to learn a direction(s) in the activation space that is orthogonal to all the inputs I’ve seen, so that I get zero activations for the features I cannot learn and thus I’m no longer penalized by sparsity, at least”. If a feature was dead in the base model but is no longer dead in the finetuned one, it implies a distributional shift in the activation space (for which the SAE was not adapted, but could potentially be adapted by finetuning)</d-footnote>. So we decided to make a separate analysis of dead features and “<strong>regular</strong>” features, that we defined as <strong>features with a log10 density between -5 and -1.</strong></p> <p>By dead features, we mean features that are <strong>exclusively</strong> dead (never activating across our entire 256K sample of tokens), i.e. <strong>dead only in one of the models</strong>:</p> <ul> <li>a “dead base” feature is a feature that is dead in the base model, but not in the finetuned one</li> <li>a “dead finetune” feature is a feature that is dead in the finetuned model, but not in the base one.</li> </ul> <p>We observe that only a handful of features are dead in both models, so we think our definitions give more information on what we’re analysing.</p> <p>Then, our approach for the rest of this section looks as follows:</p> <ol> <li>We sample max 100 exclusively dead features and 1000 regular features using our density histogram values for each base model and its finetune.</li> <li>We convert these features to their activation vector and logit vector representations for both the base model and its finetune.</li> <li>For each regular feature, we compute their <strong>activation similarity</strong> and the <strong>logits similarity</strong> with respect to the corresponding finetune, and for the exclusively dead features - their <strong>activation error:</strong> <ul> <li>We cannot really compute the activation similarity as a correlation score if one of the feature’s activation vectors is constantly 0, i.e. the feature is dead. In this case we take the log10 of these activation vectors (with <code class="language-plaintext highlighter-rouge">1e-10</code> as the epsilon value to avoid a log of zero), take the <a href="https://en.wikipedia.org/wiki/Mean_absolute_error">Mean Absolute Error</a> of the resulting vectors and call it <strong>activation error</strong><d-footnote>It makes little sense to compute dead features logit similarity: if the feature never activates, it doesn’t matter what its logit effect is - it will never manifest itself in the model. </d-footnote>.</li> </ul> </li> <li>Additionally, we plot a <strong>histogram of similarities</strong> for each feature type, since we observed a significant deviation of the similarity score (mainly activation similarity) in some experiments.</li> </ol> <h2 id="52-gemma-2b-features-transferability-test">5.2 Gemma-2b features transferability test</h2> <p>One could say that in the Gemma-2b case, it’s obvious from the previous results that our SAE doesn’t transfer. But we could imagine a case where <em>some</em> (perhaps a tiny fraction) of our SAE features from the regular density interval do still transfer, so we decided to conduct this experiment anyway.</p> <p>Starting with the features that are exclusively dead in the <em>base</em> model, their mean activation error for Gemma-2b and Gemma-2b python-codes finetune is <strong>0.025</strong>. A histogram of these 100 activation errors is given below:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.1.png" alt="My Image" width="700"/> </div> <p>This made us think that “dead features turning alive” anomaly is not so much of an anomaly, because the dead features activate only (very) slightly in the finetuned model. The max activation value across all 100 dead features in the finetuned model was <strong>1.1,</strong> indicating that our “dead feature direction” is only slightly off in the finetuned model, and can be easily adjusted by SAE finetuning.</p> <p>As for the features that are exclusively dead in the <em>finetune</em> model, Gemma-2b had only two of them on our sample, with the activation error equal to 0.34 and 3.19, which is considerably higher than in the previous case.</p> <p>Moving on to the regular features, we expected to see a much more drastic dissimilarity of their activations. Indeed, the <strong>mean activation similarity for our sample of Gemma-2b regular feature is 0.39</strong>. Let’s check the histogram of these similarity scores:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.2.png" alt="My Image" width="700"/> </div> <p>Interestingly, we see that a small fraction of features (~10%) have an activation similarity above 0.8! This implies that if these features were interpretable in the base model, they will most likely stay interpretable in the finetune model<d-footnote>We didn’t try to manually interpret these features’ activations to verify this claim, and it would be interesting to see future works in this direction</d-footnote>. But we’re not sure about the significance of this result: this could just as well be noise, so we invite further research in this area.</p> <p>As for the logit similarity of these regular features, it turns out it’s much higher than our activation similarity, with a mean value of <strong>0.952.</strong> Looking at the logit similarity scores histogram, it’s also much more concentrated towards the end of the interval:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.3.png" alt="My Image" width="700"/> </div> <p>However, it’s easy to be misled by the mean logits similarity score. What it’s really saying is that our unembedding matrix (which is multiplied by the feature direction to get the logits similarity) hasn’t changed that much after finetuning (with a Frobenius norm ratio equal to 1.117 as we checked for our Gemma finetune). So <em>if the feature has still the same direction, we can indeed say that the “direct feature effect” hasn’t changed in the finetuned model, but we never checked this premise!</em> All we know is that there exist ~10% of features which have reasonably high activation similarity scores with the features from the base model. <em>The key point is that the latter is a statement about the feature’s encoder direction</em> (one that is used to project onto to get the feature’s activation, <a href="https://www.lesswrong.com/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s">explained by Neel Nanda here</a><d-cite key="Nanda_2023"></d-cite>), <em>not the decoder one -</em> which is what we mean when we talk about <em>feature directions. So it could be the case that the feature is still there but changed its direction</em> as discussed in <a href="https://www.lesswrong.com/posts/bsXPTiAhhwt5nwBW3/do-sparse-autoencoders-saes-transfer-across-base-and?commentId=pJHfoZ2GLD8neS57g">this comment,</a><d-cite key="sae_finetuning"></d-cite> it could also be that some features change their directions and the others don’t - it’s impossible to tell when the reconstruction score (e.g. variance explained) is as poor as in the Gemma-2b case.</p> <h2 id="53-mistral-7b-features-transferability-test">5.3 Mistral-7B features transferability test</h2> <p>Here we repeat all the same experiments for Mistral-7B and its MetaMath finetune, and compare the result with the Gemma-2b case.</p> <p>Let’s start with the features that are exclusively dead in the Mistral base model. Their mean activation error is 0.0003, which is almost <em>two orders of magnitude</em> lower than in the Gemma-2b case. The corresponding histogram looks like this:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.4.png" alt="My Image" width="700"/> </div> <p>Once again, the results suggest <em>that even though the dead features in the base model are no longer dead in the finetuned one</em>, they activate really weakly on average, so it should be easy to adjust them with a cheap SAE finetuning.</p> <p>The activation error for the features exclusively dead in the finetuned model tells a similar story:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.5.png" alt="My Image" width="700"/> </div> <p>Here the error is even smaller, implying that even though some features stopped activating after finetuning, their corresponding activation values in the base model were really low. And the features are often uninterpretable in the lowest activation intervals anyway, so it should have a minor overall effect on SAEs transferability.</p> <p>Let’s conclude this section with an analysis of our regular features. As expected from the results of the last section, the activation similarity of these features is quite high, with a mean value of <strong>0.958</strong>. As for the activation scores histogram:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.6.png" alt="My Image" width="700"/> </div> <p>As we can see, the distribution of the scores is strongly attracted to the 0.9-1.0 correlation interval, so we can conclude that SAE feature transferability is significantly high in this case. This is also backed up by the mean logits similarity of 0.9996, and a rather straightforward logits similarity histogram:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.7.png" alt="My Image" width="700"/> </div> <hr/> <h1 id="6-conclusions--limitations">6. Conclusions &amp; Limitations</h1> <h2 id="61-conclusions">6.1 Conclusions</h2> <p>Going back to our original question of <em>“Do SAEs trained on a base model transfer to the finetuned one?”</em>, the most obvious answer that comes to mind now is - it depends! We got drastically different results for our Gemma-2b-python-codes and Mistral-7B-MetaMath finetunes. However, <strong>it seems possible that one could estimate the “degree of transferability” in advance<em>.</em></strong> One method is to compute various weight deviation metrics, such as the one used by <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al</a><d-cite key="sae_finetuning"></d-cite> for Gemma-2b, and another method that we used - to compute activation similarities of the model that are fed into an SAE. Both of these anecdotally correlate with the results of our transferability experiments, but a more thorough study is definitely needed.</p> <p>Another takeaway we’ve had after finishing this post is that <strong>“SAE transferability” can mean different things</strong>. One can utilize the standard SAE evaluation metric to get a high-level evaluation of the SAE quality on the finetuned model, but it doesn’t always give a deeper insight into what happens with the SAE feature once we zoom in (which may be more interesting for the real SAE applications in mech interp). Our Gemma-2b results suggest that some SAE features may still be interpretable, even when finetuning has completely rendered the SAE incapable of reconstructing the input. And although the significance of this result can be rightly questioned, we still think it is interesting to investigate further.</p> <h2 id="62-limitations">6.2 Limitations</h2> <p>The main limitations we see in our work are the following:</p> <ul> <li>It’s not clear how our results will generalize to other finetunes. A more principled approach would be to use a custom finetuning setup, where one could e.g. study the relationship between the amount of compute put into finetuning and some key SAE transferability metrics like the reconstruction loss etc. <ul> <li>Our finetuned models also had almost the same dictionaries as the base model (with the exception of a single padding token), so it’s also not clear whether our results generalize to the finetuned model with significantly modified dictionaries (e.g. language finetunes for languages that were not in the original training dataset of the base model)</li> </ul> </li> <li>We only studied SAEs for a single residual layer for Gemma-2b and Mistral-7B models. A more thorough study is needed to see how these results will vary when considering different layers and different SAE activations, e.g. MLP or hidden head activations.</li> <li>All our experiments were performed on the training dataset of the base SAE, i.e. on the original training distribution of the base models. But the finetuned models are mostly used for tasks that they have been finetuned on, so we definitely need some future work here to extend these results to a more specific setting of finetuned models.</li> <li>Our analysis of SAE features transferability was somewhat superfluous, because we didn’t do a thorough investigation of the interpretability of our features after the finetuning. An even more representative study would be to replicate some kind of circuit analysis in the SAE basis to rigorously prove if (at least some) features are still involved in the same computation of the finetuned model.</li> </ul> <hr/> <h1 id="appendix">Appendix</h1> <p>All code is available on <a href="https://github.com/tommasomncttn/SAE-Transferability">github</a></p>]]></content><author><name>Taras Kutsyk</name></author><category term="sae"/><category term="mechanistic interpretability"/><category term="model diffing"/><summary type="html"><![CDATA[TLDR (Executive Summary)]]></summary></entry><entry><title type="html">UniReps 2024 Awards</title><link href="https://unireps.org//blog/2025/unireps2024awards/" rel="alternate" type="text/html" title="UniReps 2024 Awards"/><published>2025-01-11T00:00:00+00:00</published><updated>2025-01-11T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/unireps2024awards</id><content type="html" xml:base="https://unireps.org//blog/2025/unireps2024awards/"><![CDATA[<p>The second edition of the UniReps Workshop at NeurIPS 2024 was a huge success! Building on the momentum from last year, we were thrilled to welcome over 1,000 participants from academia and industry for a day packed with engaging talks, lively discussions, and an exciting poster session. We received 106 submissions this year, including 25 fantastic proceedings papers that will soon be published in the workshop volume.</p> <p>We couldn’t have done it without all of you! A big thank you to the authors, our amazing program committee of 181 members, the participants who made the event so special, and our sponsors for their incredible support.</p> <p>We were also proud to highlight some exceptional work through our awards. Here’s a look at the winners and their inspiring contributions.</p> <hr/> <h2 id="best-paper-awards">Best Paper Awards</h2> <h3 id="proceedings-track">Proceedings Track</h3> <p><strong>Authors</strong>: <a href="https://sarahharvey.github.io">Sarah Harvey</a>, <a href="https://sites.google.com/view/lipshutz/home">David Lipshutz</a>, and <a href="https://alexhwilliams.info">Alex H. Williams</a></p> <p><strong>Title</strong>: <em>“What Representational Similarity Measures Imply about Decodable Information.”</em> <d-cite key="harvey2024what"></d-cite></p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_best_proceedings_2024.jpg" alt="Best Paper Proceedings Track Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <hr/> <h3 id="extended-abstracts-track">Extended Abstracts Track</h3> <p><strong>Authors</strong>: <a href="https://scholar.google.com/citations?user=EtEVFLoAAAAJ">Richard Antonello</a> and <a href="https://chengemily1.github.io">Emily Shana Cheng</a></p> <p><strong>Title</strong>: <em>“Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models.”</em> <d-cite key="antonello2024evidence"></d-cite></p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_best_abstract_2024.jpg" alt="Best Paper Extended Abstracts Track Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <hr/> <h2 id="honorable-mentions">Honorable Mentions</h2> <h3 id="proceedings-track-1">Proceedings Track</h3> <p><strong>Author</strong>: <a href="https://alexhwilliams.info">Alex H. Williams</a></p> <p><strong>Title</strong>: <em>“Equivalence between Representational Similarity Analysis, Centered Kernel Alignment, and Canonical Correlations Analysis.”</em> <d-cite key="williams2024equivalence"></d-cite></p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_honorable_proceedings_2024.jpg" alt="Honorable Mention Proceedings Track Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <h3 id="extended-abstracts-track-1">Extended Abstracts Track</h3> <p><strong>Authors</strong>: <a href="https://chenyuwang-monica.github.io">Chenyu Wang</a>, <a href="https://www.mit.edu/~sharut/">Sharut Gupta</a>, <a href="https://scholar.google.com/citations?user=2gU9PYQAAAAJ">Xinyi Zhang</a>, <a href="https://www.cs.toronto.edu/~stonekaboni/">Sana Tonekaboni</a>, <a href="https://scholar.google.ch/citations?user=gTWUZlsAAAAJ">Stefanie Jegelka</a>, <a href="https://people.csail.mit.edu/tommi/">Tommi Jaakkola</a>, and <a href="https://www.carolineuhler.com/caroline-uhler">Caroline Uhler</a></p> <p><strong>Title</strong>: <em>“An Information Criterion for Controlled Disentanglement of Multimodal Data.”</em> <d-cite key="wang2024an"></d-cite></p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_honorable_abstract_2024.jpg" alt="Honorable Mention Extended Abstracts Track Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <hr/> <h2 id="best-reviewer-award">Best Reviewer Award</h2> <p>A special <strong>Best Reviewer Award</strong> was given to <a href="https://akamboj2.github.io">Abhi Kamboj</a> for their exceptional feedback.</p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_best_reviewer_2024.jpg" alt="Best Reviewer Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <hr/> <h2 id="acknowledgments">Acknowledgments</h2> <p>🔴 Congratulations to all the award recipients for their outstanding work, and a heartfelt thank you to everyone who participated in the workshop. The UniReps community keeps growing! We look forward to seeing you again next year! 🔵</p>]]></content><author><name>UniReps Organizing Team</name></author><category term="NeurIPS,"/><category term="awards,"/><category term="2024,"/><category term="UniReps"/><summary type="html"><![CDATA[A gallery of the UniReps 2024 Awards winners and photos]]></summary></entry><entry><title type="html">Failures in Perspective-Taking of Multimodal AI Systems</title><link href="https://unireps.org//blog/2024/failures_perspectivetaking/" rel="alternate" type="text/html" title="Failures in Perspective-Taking of Multimodal AI Systems"/><published>2024-11-20T00:00:00+00:00</published><updated>2024-11-20T00:00:00+00:00</updated><id>https://unireps.org//blog/2024/failures_perspectivetaking</id><content type="html" xml:base="https://unireps.org//blog/2024/failures_perspectivetaking/"><![CDATA[<div class="caption"> Listen to the AI-generated podcast based on our preprint or check out the benchmark paper and project on GitHub: </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/blog/assets/audio/2024-11-20-failures_perspectivetaking/podcast.mp3" controls=""/> </figure> </div> </div> <div style="display: flex; align-items: center;"> <a href="https://github.com/bridgetleonard2/perspectiveTaking" style="margin-left: 70px; margin-right: 80px; display: inline-block;"> <img src="/blog/assets/img/2024-11-20-failures_perspectivetaking/github-mark.png" alt="GitHub" class="logo" width="40"/> </a> <a href="https://arxiv.org/abs/2409.13929" style="display: inline-block;"> <img src="/blog/assets/img/2024-11-20-failures_perspectivetaking/arxiv-logomark-small@2x.png" alt="arXiv" class="logo" width="30"/> </a> </div> <h2 id="introduction">Introduction</h2> <p>Recent research in AI has exposed a critical limitation: the inability of current models to effectively perform spatial reasoning tasks. Despite their impressive visual perception capabilities, these models struggle to understand spatial relationships and make inferences about them. While previous research has explored aspects of spatial cognition in AI, it often lacks the specificity characteristic of human spatial cognition studies. In cognitive psychology, tasks are carefully designed to isolate distinct processes, enabling precise measurement and minimizing bias or reliance on alternative strategies. To bridge the gap between cognitive science and artificial intelligence, we focus on a fundamental aspect of human spatial reasoning: visual perspective-taking.</p> <blockquote> <p><strong>Visual perspective-taking</strong> is the ability to mentally simulate a viewpoint other than one’s own. It allows us to understand the relationship between objects and how we might have to manipulate a scene to align with our perspective, which is essential for tasks like navigation and social interaction.</p> </blockquote> <p>By leveraging established methodologies, we can rigorously evaluate AI’s spatial cognition, starting with perspective-taking. The extensive human literature on spatial reasoning offers a valuable benchmark, enabling comparisons between model performance and the human developmental trajectory. This comparison helps identify critical gaps and opportunities for enhancing AI models.</p> <p>Our aim was to create a targeted perspective-taking benchmark for multimodal AI systems, probing various levels and components of the cognitive process.</p> <h3 id="definitions-and-terminology">Definitions and Terminology</h3> <ul> <li> <p><strong>Level 1 Perspective-taking</strong> refers to knowing that a person may be able to see something another person does not</p> </li> <li> <p><strong>Level 2 Perspective-taking</strong> refers to the ability to represent how a scene would look from a different perspective</p> </li> <li> <p><strong>Mental Rotation</strong> where one imagines an object or scene rotating in space to align with a perspective</p> </li> <li> <p><strong>Spatial vs Visual Judgments</strong> responding to queries about the spatial orientations of objects or their non-spatial visual characteristics</p> </li> </ul> <details><summary>Click here to learn more about perspective-taking</summary> <p>In the human developmental literature, perspective-taking has been stratified into two levels, defined above. Based on developmental literature, level 1 perspective-taking appears fully developed by the age of two <d-cite key="moll2006level1"></d-cite>. In contrast, although success on some simple Level 2 tasks is first seen around age 4 <d-cite key="newcombe1992children"></d-cite>, Level 2 perspective-taking continues to develop into middle childhood <d-cite key="surtees2012egocentrism"></d-cite> and even into young adulthood <d-cite key="dumontheil2010online"></d-cite>. In terms of measurement, a common Level 1 task might ask if an object is viewable (or positioned to the front or back) of a person or avatar in a scene. Level 2 is often measured by having subjects assess the spatial relationship between objects.</p> <p>A more specific cognitive process, <strong>mental rotation</strong>, where one imagines an object or scene rotating in space to align with a perspective, plays an important role in perspective-taking. Surtees et al. <d-cite key="surtees2013similarities"></d-cite> experimentally manipulated Level 1 and Level 2 perspective-taking by presenting participants with tasks where they viewed numbers or blocks relative to an avatar. Different stimuli were used to elicit visual and spatial judgments, like whether the number was a “6” or a “9” from the person’s perspective, or if the block was to the person’s right or left. Level 1 tasks involved indicating whether the number/block was visible to the avatar, while Level 2 involved reporting either the number seen by the avatar or whether it was to the avatar’s left or right (Level 2). For both visual and spatial judgments, response times were longer for Level 2 tasks as the angular difference between the avatar and the participant increased, while response times remained unaffected by the angle in Level 1 tasks. This increase in response time when the participant’s view was unaligned with the avatar’s perspective is attributed to the mental rotation process, either rotating the scene or rotating one’s own reference frame to align with the avatar.</p> </details> <hr/> <h3 id="creating-a-new-benchmark">Creating a New Benchmark</h3> <h4 id="limitations-of-current-benchmarks">Limitations of Current Benchmarks</h4> <p>There are two main limitations current AI spatial cognition assessment:</p> <details><summary>Reasoning with language alone can inflate performance on spatial benchmarks</summary> <p>Text-only GPT-4 achieves a score of 31.4, while multimodal GPT-4v achieves a score of 42.6 on the spatial understanding category of Meta’s openEQA episodic memory task <d-cite key="majumdar2024openeqa"></d-cite>. The strong baseline score achieved by the text-only GPT-4 suggests that many “real-world” questions based on visual scenes can be deduced linguistically. Additionally, the limited improvement when moving from a blind LLM to a multimodal one suggests that vision models do not gain a significant understanding of space beyond what can be inferred through language.</p> </details> <details><summary>Benchmark scores can be hard to interpret since models often perform poorly</summary> <p>BLINK <d-cite key="fu2024blink"></d-cite>, a benchmark more specifically focused on visual perception capabilities, contains categories related to spatial cognition, such as relative depth and multi-view reasoning. On this benchmark, GPT-4v achieved an accuracy of 51.26%, only 13.17% higher than random guessing and 44.44% lower than human performance. When benchmarks are highly focused on visuospatial tasks, the significant shortcomings of multimodal models suggest that further advancements are needed before these models can reliably perform in real-world scenarios. Even within specific categories, it is often difficult to determine <em>why</em> models fail on certain tasks while succeeding on others, as these failures cannot be easily linked to the absence of a particular cognitive process.</p> </details> <p>To target some of these issues, we apply established tasks in cognitive psychology that measure spatial cognition in a precise manner. By applying these tasks to AI systems, we gain not only improved measurement precision but also the ability to compare AI performance with human development, providing clear insights into model limitations and areas for improvement.</p> <h4 id="perspective-taking-benchmark">Perspective Taking Benchmark</h4> <p>Leveraging the distinction between Level 1 and Level 2 perspective-taking <d-cite key="surtees2013similarities"></d-cite>, we propose a small perspective-taking benchmark that assesses multimodal model capabilities across three tasks: Level 1, Level 2 with spatial judgments, and Level 2 with visual judgments. Although human performance remains stable regardless of judgment type, we include this differentiation of Level 2 stimuli to examine potential egocentric biases that may arise in multimodal models when interpreting spatial relations compared to optical character recognition (OCR). This benchmark aims to address gaps in current AI spatial cognition measures by increasing process specificity, limiting language-based solutions, and offering straightforward comparisons to human cognition.</p> <hr/> <h2 id="methods">Methods</h2> <p>Our study utilized GPT-4o (“gpt-4o-2024-05-13” via OpenAI’s API) to conduct a series of perspective-taking experiments designed to capture the system’s spatial reasoning abilities. We kept <code class="language-plaintext highlighter-rouge">top_p = 0.5</code> to restrict the model from choosing from the top 50% of words that could come next in its response.</p> <p>Our experimental design was inspired by previous studies that evaluated viewpoint dependence using targets like toy photographers [2] and avatars with blocks [12]. In our study, we used an avatar as a target and different stimuli, either cubes with numbers and letters or cubes and spheres, to investigate the influence of visual and spatial judgments on model performance. Each task consisted of 16 trial types, featuring images at 8 different angles (0°, 45°, 90°, 135°, 180°, 225°, 270°, 315°) with 2 response options for each task (e.g., cube in front or behind, 6/9 or M/W on the cube, and cube left or right).</p> <p>Ten iterations of each image were passed through the model to calculate the percentage of correct responses.</p> <div class="table-responsive"> <table class="table table-striped table-bordered align-middle"> <thead class="table-light"> <tr> <th scope="col">Task</th> <th scope="col">Example Stimulus</th> <th scope="col">Prompt</th> </tr> </thead> <tbody> <tr> <td><strong>Level 1</strong></td> <td> <img src="/blog/assets/img/2024-11-20-failures_perspectivetaking/infront_behind_ex.jpg" alt="Level 1: &quot;IN FRONT&quot; 45°" class="img-fluid rounded shadow-sm" style="max-width: 300px;"/> </td> <td>For the following images respond with <em>in front</em> or <em>behind</em> to indicate if the cube is in front or behind from the perspective of the person.</td> </tr> <tr> <td><strong>Level 2: Spatial Judgment</strong></td> <td> <img src="/blog/assets/img/2024-11-20-failures_perspectivetaking/left_right_ex.jpg" alt="Level 2 Spatial: &quot;RIGHT&quot; 225°" class="img-fluid rounded shadow-sm" style="max-width: 300px;"/> </td> <td>For the following images respond with <em>left</em> or <em>right</em> to indicate if the cube is to the left or to the right from the perspective of the person.</td> </tr> <tr> <td><strong>Level 2: Visual Judgment</strong></td> <td> <img src="/blog/assets/img/2024-11-20-failures_perspectivetaking/number_ex.jpg" alt="Level 2 Visual: &quot;6&quot; 90°" class="img-fluid rounded shadow-sm" style="max-width: 300px;"/> </td> <td>For the following images respond with <em>6</em> or <em>9</em> to indicate if the number on the cube is a 6 or a 9 from the perspective of the person.</td> </tr> <tr> <td><strong>Level 2: Visual Judgment</strong></td> <td> <img src="/blog/assets/img/2024-11-20-failures_perspectivetaking/letter_ex.jpg" alt="Level 2 Visual: &quot;W&quot; 315°" class="img-fluid rounded shadow-sm" style="max-width: 300px;"/> </td> <td>For the following images respond with <em>M</em> or <em>W</em> to indicate if the letter on the cube is an M or a W from the perspective of the person.</td> </tr> </tbody> </table> </div> <h3 id="chain-of-thought-prompting">Chain of Thought Prompting</h3> <p>To further examine how language might be used to solve spatial tasks, we included chain-of-thought prompting to the Level 2 spatial task with the prompt:</p> <p>“Analyze this image step by step to determine if the cube is to the person’s left or right, from the person’s perspective. First, identify the direction the person is looking relative to the camera. Second, determine if the cube is to the left or right, relative to the camera. Third, if the person is facing the camera, then from their perspective, the cube is to the inverse of the camera’s left or right. If the person is facing away from the camera, then the cube is on the same side as seen from the camera. Respond with whether the cube is to the person’s left or right.”</p> <hr/> <h2 id="results">Results</h2> <h3 id="level-1">Level 1</h3> <p>GPT-4o performed with near-perfect accuracy on 6 out of the 8 image angles as seen below. Its poor performance on 0° images is likely due to an accidental viewpoint where the avatar blocked one of the shapes. However, poor performance on 315° image types is less interpretable, especially in contrast to GPT-4o’s impressive performance on 45° images, which have the same angular perspective.</p> <div class="l-page"> <iframe src="/blog/assets/plotly/2024-11-20-failures_perspectivetaking/infront_behind.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <h3 id="level-2-spatial-and-visual-judgments">Level 2 Spatial and Visual Judgments</h3> <p>As previously mentioned, human response times increase on perspective-taking tasks as the angular difference between the target and observer increases <d-cite key="surtees2013similarities"></d-cite>. We administered the task to a small number of human participants and replicated this effect with both our stimuli types, finding a bell-shaped curve in the relationship between response time and angle. Response times peaked when the target required a full mental rotation (180°), as seen in the green line in the figure below. As expected, GPT-4o struggled with the task when mental rotation was involved, beginning around a 90° angular difference. Interestingly, in both tasks, GPT-4o exhibited a response bias toward either “left” or “6” or “W” when the angular difference of the avatar is 90° or 135° in either direction. This likely reflects uncertainty from an egocentric perspective, and thus, a default to one response over another.</p> <div class="l-page"> <iframe src="/blog/assets/plotly/2024-11-20-failures_perspectivetaking/subplots.html" frameborder="0" scrolling="no" height="550px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <h3 id="chain-of-thought">Chain of Thought</h3> <p>GPT-4o performance significantly improved with chain-of-thought prompting on 180° stimuli. However, this linguistic strategy did not improve the model’s ability to handle intermediate rotations between 90° and 180°. This suggests that while language can convey some level of spatial information, it lacks the precision required for human-level spatial cognition. This demonstration of surface-level perspective-taking abilities can partially explain how multimodal models achieve high performance on certain spatial benchmarks.</p> <div class="l-page"> <iframe src="/blog/assets/plotly/2024-11-20-failures_perspectivetaking/cot.html" frameborder="0" scrolling="no" height="450px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <hr/> <h2 id="conclusion">Conclusion</h2> <p>With this project, we highlight the value of applying cognitive science techniques to explore AI capabilities in spatial cognition.</p> <ol> <li> <p>We investigated GPT-4o’s perspective-taking abilities, finding it fails when there is a large difference between image-based and avatar-based perspectives</p> </li> <li> <p>We developed a targeted set of three tasks to assess multimodal model performance on Level 1 and Level 2 perspective-taking, with spatial and visual judgments</p> <ul> <li> <p>GPT-4o can do Level 1, aligning with the spatial reasoning abilities of a human infant/toddler</p> </li> <li> <p>GPT-4o fails on Level 2 tasks when mental rotation is required—the avatar’s perspective is not aligned with image perspective</p> </li> </ul> </li> <li> <p>We investigated if chain-of-thought prompting could elicit more spatial reasoning through language</p> <ul> <li>This enabled GPT-4o to succeed on 180° tasks, but it continued to fail at intermediate angles, underscoring its limitations in performing true mental rotation</li> </ul> </li> </ol> <p>While GPT-4o’s performance decreases on tasks that humans typically solve using mental rotation, this does not necessarily indicate that GPT-4o struggles with or cannot perform mental rotation. Instead, it suggests that GPT-4o likely employs a fundamentally different strategy to approach these tasks. Rather than engaging in mental rotation, GPT-4o appears to rely primarily on image-based information processing. We found more support for this when testing an open prompt for Level 2 visual images that did not specify which letters or numbers to respond with. GPT-4o often responded with “E” and “0” for images around a 90° angular difference, where from the image view, an M/W would look like an E, and a 9/6 would look like a 0.</p> <p>It could be that current multimodal models aren’t trained on the appropriate data to achieve the reasoning necessary for Level 2 perspective-taking. However, considering the developmental trajectory of humans, it becomes evident that this issue may not be solely data-related. Level 2 perspective-taking typically develops between the ages of 6 and 10 <d-cite key="frick2014picturing"></d-cite><d-cite key="frick2018measuring"></d-cite>, even after children have had exposure to extensive amounts of “data” through experience. This late development suggests that the challenge may be more computational than data-driven. Specifically, this ability likely relies on computations occurring outside of the visual and language networks, perhaps in areas responsible for cognitive processes like mental rotation or spatial transformation or even theory of mind <d-cite key="gunia2021brain"></d-cite><d-cite key="schurz2013common"></d-cite><d-cite key="surtees2013use"></d-cite><d-cite key="surtees2013similarities"></d-cite>. While the argument that better or more focused training data could improve model performance remains valid, it is possible that entirely new computational strategies are needed to mirror the complex, integrative processes that enable Level 2 reasoning in humans.</p> <p>This project demonstrates the potential of cognitive science methods to establish baselines for AI assessment. Using these well-established techniques, we achieve clear, interpretable measures that are less susceptible to bias. Additionally, these measures can be directly compared to human performance and developmental trajectories, providing a robust framework for understanding AI’s strengths and weaknesses in relation to well-researched human cognitive processes.</p>]]></content><author><name>Bridget Leonard</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[An investigation into the spatial reasoning abilities of multimodal LLMs.]]></summary></entry></feed>