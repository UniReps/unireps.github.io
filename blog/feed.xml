<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://unireps.org//blog/feed.xml" rel="self" type="application/atom+xml"/><link href="https://unireps.org//blog/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-05T14:12:18+00:00</updated><id>https://unireps.org//blog/feed.xml</id><title type="html">blank</title><entry><title type="html">Boomerang Distillation Enables Zero-Shot Model Size Interpolation</title><link href="https://unireps.org//blog/2025/boomerang-distillation/" rel="alternate" type="text/html" title="Boomerang Distillation Enables Zero-Shot Model Size Interpolation"/><published>2025-10-31T00:00:00+00:00</published><updated>2025-10-31T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/boomerang-distillation</id><content type="html" xml:base="https://unireps.org//blog/2025/boomerang-distillation/"><![CDATA[<p>Today’s large language models power everything from chatbots on your phone to massive AI systems running in data centers. But not all devices can handle the same model sizes: a model that’s fast on a GPU cluster might be unusable on a laptop. AI model developers try to solve this by releasing “model families” consisting of different-sized versions of the same model. For example, Llama3 models come in sizes ranging from 1 billion to 70 billion parameters. Yet training each size from scratch is expensive, leaving big gaps between available models. What if we could fill those gaps <em>without any extra training</em>? That’s exactly what we explore with <strong>Boomerang Distillation</strong>, a new way to “recombine” parts of large and small models to create many intermediate sizes – all from a single training run.</p> <hr/> <h2 id="current-approaches-for-training-llm-families-are-computationally-expensive">Current Approaches for Training LLM Families are Computationally Expensive</h2> <p>As training each model size from scratch is very computationally intensive, many modern LLM families start with one large pretrained model (the teacher) and distill it into smaller ones (the students). This procedure is called knowledge distillation. Typically, the student models learn with the usual next-token prediction objective, plus extra losses that make them imitate the teacher’s behavior (e.g. KL divergence and cosine distance). Distillation is more compute-friendly than training every model without a teacher, but it still requires training each model independently on up to a trillion tokens. This expensive process limits how many models developers can release, so we typically end up with a small set tuned for common GPU setups. Meanwhile, practitioners need models tailored to <em>their</em> hardware and compute budgets. Unless they train a new model themselves, they’re limited to a few prebuilt options, leaving large gaps in the trade-off between model compute and performance (Figure 1).</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/model_sizes_new.jpeg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/model_sizes_new.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 1: The landscape of pretrained LLM families. There are large gaps in size between available LLMs. Figure credit to <a href="https://qwen.ai/research">Qwen research</a></p> <hr/> <h2 id="boomerang-distillation-creating-multiple-models-for-the-price-of-one">Boomerang distillation: creating multiple models for the price of one</h2> <p>Given the limitations of current approaches, how can we efficiently create models of different sizes? We show that surprisingly, distillation is not just useful for training good student models – with the right setup, we can mix and match parts of the teacher and student models to build intermediate models that smoothly trade off size and performance! We call this phenomenon <em>boomerang distillation</em>: starting with a large teacher, we distill a single smaller student, and then “boomerang” back toward the teacher by selectively swapping in teacher components, creating many models of intermediate size <strong>without any additional training</strong>.</p> <p>Intuitively, boomerang distillation works because we encourage each student layer in the distilled model to approximate the function of some block (contiguous set) of teacher layers. In this setup, each student layer can be thought of as a compact summary of one or more teacher layers. Then, swapping out a layer in the student and replacing it with its corresponding block of teacher layers produces a larger model with a <em>better approximation</em> of what the student is trying to compute. Each swap increases model size and improves similarity to the teacher, producing a new, usable model with no extra training required.</p> <p>Boomerang distillation consists of three key steps: (1) student initialization, (2) knowledge distillation, and (3) student patching (Figure 2). We explain each of these steps in detail below.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/size_interpolation_v4.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/size_interpolation_v4.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 2: Overview of boomerang distillation. ➀ The student model is initialized by copying layers from the pretrained teacher model. ➁ The teacher model is distilled into the student model with cross-entropy loss, knowledge distillation loss, and cosine distance loss. ➂ After training the student model, a block of teacher layers corresponding to a student layer is inserted back into the model to get the interpolated intermediate model.</p> <h3 id="step-1-student-initialization">Step 1: Student initialization</h3> <p>We start aligning student layers to teacher blocks (a “block” is one or more contiguous teacher layers) by initializing the student with layers copied from the teacher (Figure 2, left). This creates a clean mapping: every student layer should stand in for a specific teacher block.</p> <p>For example, in Figure 2, we copy layers 1, 3, and 5 from the teacher to create the student. Then,</p> <ul> <li>The first student layer <strong>S1</strong> maps to teacher layers <strong>T1</strong> and <strong>T2</strong></li> <li>The second student layer <strong>S2</strong> maps to teacher layers <strong>T3</strong> and <strong>T4</strong></li> <li>The third student layer <strong>S3</strong> maps to teacher layer <strong>T5</strong></li> </ul> <p>Copying gives us a strong head start, since each student layer behaves like the first layer in its corresponding teacher block.</p> <h3 id="step-2-knowledge-distillation">Step 2: Knowledge distillation</h3> <p>Once we have the student initialized, we train the model using knowledge distillation (Figure 2 center). To improve student performance, we use cross entropy loss for next token prediction and KL divergence loss to make the student’s predicted probabilities match those of the teacher. We also add a cosine distance loss between the outputs of the student layers and their corresponding teacher blocks.</p> <p>Continuing the example from Figure 2 above, we align</p> <ul> <li><strong>S1</strong> with <strong>T2</strong>’s output (for the T1-T2 block)</li> <li><strong>S2</strong> with <strong>T4</strong> (for T3-T4)</li> <li><strong>S3</strong> with <strong>T5</strong> (for T5)</li> </ul> <p>This layer-to-block alignment trains each student layer to mimic the computations in its mapped teacher block.</p> <h3 id="step-3-student-patching">Step 3: Student patching</h3> <p>After distillation, we can assemble models of many sizes by patching student layers, i.e., replacing any student layer with its corresponding block of teacher layers (Figure 2, right). Because we enforced layer-wise alignment in Steps 1 and 2, each swap creates an intermediate model that <em>better approximates</em> the teacher.</p> <p>In our example from Figure 2, we can choose any subset of swaps:</p> <ul> <li>Replace <strong>S1</strong> with <strong>T1-T2</strong></li> <li>Replace <strong>S2</strong> with <strong>T3-T4</strong></li> <li>Replace <strong>S3</strong> with <strong>T5</strong></li> </ul> <p>Choosing different combinations of swaps yields intermediate models of different sizes, giving a smooth menu of compute/accuracy trade-offs <strong>without additional training</strong>.</p> <hr/> <h2 id="boomerang-distillation-interpolates-between-student-and-teacher-performance">Boomerang distillation interpolates between student and teacher performance</h2> <h3 id="what-are-the-necessary-conditions-for-boomerang-distillation-to-succeed">What are the necessary conditions for boomerang distillation to succeed?</h3> <p>To probe when boomerang distillation works, we run two simple stress tests. First, we study whether a student that’s randomly initialized (instead of copying layers from the teacher) could still yield useful intermediate models. Second, we try <em>naive pruning</em>: initialize the student from teacher layers, but skip the distillation step entirely.</p> <p>We find that across multiple model families and sizes, boomerang distillation creates intermediate models whose <strong>performance interpolates smoothly</strong> between the student and teacher (Figures 3 and 4). In contrast, both baselines’ performance sharply decreases with model size. This shows that <strong>both</strong> ingredients – the right initialization <em>and</em> knowledge distillation – matter. Leave out either step and the boomerang effect largely disappears.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_layer_dropping.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_layer_dropping.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 3: Boomerang distillation produces models with smooth size–performance interpolation, consistently outperforming naive layer pruning and interpolation from randomly initialized distilled models.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/all_classification_results.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/all_classification_results.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 4: Boomerang distillation emerges across model families like Qwen, Pythia, and Llama.</p> <p>We also test which parts of the distillation objective are needed for boomerang distillation. To do so, we train four students with ablated loss terms and then interpolate from them. In Figure 5, we find that for most model sizes, leaving out the per-layer cosine distance (the green and purple lines) does not meaningfully reduce interpolation performance. This suggests that the initialization in Step 1 already provides enough alignment for boomerang distillation to work reasonably well without explicitly training every intermediate layer to match the teacher. That said, students distilled with the per-layer cosine term show more stable interpolation than those without. Looking ahead, we are interested in understanding whether we can keep that stability without explicit layer-wise alignment during distillation, because removing the need to keep the teacher in memory would significantly reduce the GPU footprint of training the student.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_loss_type.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_loss_type.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 5: Per-layer loss yields stable and smoother interpolation performance.</p> <h3 id="how-good-is-boomerang-distillation">How good is boomerang distillation?</h3> <h4 id="comparison-to-standard-distilled-models">Comparison to standard distilled models</h4> <p>Now that we have interpolated models from boomerang distillation, we compare them to same-size models trained with a standard distillation objective. For an apples-to-apples test, we use the same initialization and distillation setup as the small student model.</p> <p>We find that boomerang distillation creates intermediate models with performance on par with standard distilled models of the same size, even outperforming them at larger sizes (Figure 6). In practice, this means we only need to distill a <em>single student</em> to get a full lineup of intermediate models that perform similarly to models individually distilled with the same token budget. These interpolated models also stack up well against pretrained models like Pythia-2.8B and Llama-3.2-3B, which train on far more tokens than the student.</p> <p>We also observe that knowledge distillation can hurt models like Qwen at larger sizes (toward the right of Figure 6), likely because they originate from proprietary, high-quality data; retraining on public data (of presumably lower quality) can cause a drop in performance. With boomerang distillation, we mitigate this issue because we interpolate directly between the student and the teacher.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_versus_distilled.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_versus_distilled.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 6: Interpolated models produced using boomerang distillation have comparable performance to pretrained and standard distilled models.</p> <h4 id="comparison-to-layer-pruning">Comparison to layer pruning</h4> <p>How does boomerang distillation compare to smart layer pruning methods? Layer Collapse (LaCo)<d-cite key="yang2024laco"></d-cite> and ShortGPT<d-cite key="men2024shortgpt"></d-cite> are popular approaches that look for redundancy in a transformer and drop entire layers to shrink the model without training.</p> <p>In practice, boomerang distillation exhibits much better performance. As Figure 7 shows, its interpolated models consistently outperform layer-pruned models of the same size, and the gap widens as models get smaller. This is likely because boomerang distillation blends information from <strong>both</strong> the distilled student and the original teacher, so the intermediate models can use information from both models. Pruning, by contrast, compresses only the teacher; once you shrink far below the teacher’s size, quality tends to fall off.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_pruning_method_comparison.jpg" sizes="95vw"/> <img src="/blog/assets/img/2025-10-31-boomerang-distillation/qwen_pruning_method_comparison.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 7: Boomerang distillation performs significantly better than layer pruning methods.</p> <hr/> <h2 id="future-directions">Future directions</h2> <p>We introduce boomerang distillation, a simple recipe for training-free creation of intermediate-sized language models: start with a big teacher model, distill a compact student, then mix and match student and teacher layers to build models that smoothly scale in size and performance. We are excited by the potential of boomerang distillation to shift how developers create model families. Instead of training many models from scratch, they can simply <strong>distill one student and then interpolate</strong>, swapping in teacher blocks as needed to produce a full lineup that covers different accuracy-latency targets. This opens the door to finer-grained LLM customization for real-world constraints. Looking ahead, extending these ideas to pretraining-scale token budgets and into other domains (such as vision) can build model families tailored to a wide range of deployment settings.</p> <hr/> <p>This blog is adapted from our paper <a href="https://arxiv.org/abs/2510.05064">Boomerang Distillation Enables Zero-Shot Model Size Interpolation</a>. This blog post was first published on the <em>Deeper Learning</em> blog at the Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University.</p> <hr/>]]></content><author><name>Sara Kangaslahti</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[Real-world deployments of LLMs require models of different sizes to meet performance, latency, and cost targets. Yet pretraining every size is prohibitively expensive, leaving large gaps in size-performance curves. We identify a novel phenomenon, Boomerang Distillation, which occurs when distilling a large language model into a smaller one. In this blog post, we describe how Boomerang Distillation can be used to create entire families of LLMs of fine-grained sizes from a single student-teacher pair without any additional training.]]></summary></entry><entry><title type="html">Shared Coordinates for Cross-Subject Brain Dynamics: Universal Latents and Directly Comparable Phase Diagrams</title><link href="https://unireps.org//blog/2025/phase-diagram-playbook/" rel="alternate" type="text/html" title="Shared Coordinates for Cross-Subject Brain Dynamics: Universal Latents and Directly Comparable Phase Diagrams"/><published>2025-10-25T00:00:00+00:00</published><updated>2025-10-25T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/phase-diagram-playbook</id><content type="html" xml:base="https://unireps.org//blog/2025/phase-diagram-playbook/"><![CDATA[<h2 id="overview-tldr">Overview (TL;DR)</h2> <p>We present a modular, modality-agnostic workflow that turns heterogeneous whole-brain time series into cohort-comparable, interpretable coordinates on a shared phase diagram, together with energy-landscape descriptors such as attractors, barriers, and kinetics.</p> <p>Key steps: 1) population-universal latent spaces (Shared Response Model - SRM, Multiset Canonical Correlation Analysis - MCCA, Group PCA or Group ICA with consensus and automated dimensionality selection) <d-cite key="NIPS2015_b3967a0e,60b14841-3c7e-3751-88f2-15308f78bf55,decheveigne2019mcca,correa2010mcca,SMITH2014738,calhoun2001groupica"></d-cite> 2) per-latent binarisation to the +/-1 format 3) Pairwise Maximum Entropy Model (PMEM) or Ising fitting: exact for small N, pseudo-likelihood, or variational Bayes <d-cite key="jaynes1957maxent,schneidman2006nature,besag1975pl,ravikumar2010ising,opper2001advancedmf"></d-cite> 4) energy landscape analysis (ELA): minima, disconnectivity, barriers, occupancies, kinetics <d-cite key="watanabe2014ela,becker1997disconnectivity,wales2006jpcb"></d-cite> 5) phase diagram analysis (PDA): novel multi-observable placement on a shared reference surface with uncertainty <d-cite key="edwards1975ea,sherrington1975sk,ezaki2020critical"></d-cite></p> <p>Outputs include uncertainty, quality control, and interactive visuals. Methods are user-tweakable, reliable, and reproducible.</p> <hr/> <h2 id="introduction">Introduction</h2> <h4 id="1-bigpicture-overview---why-shared-latents--pmem--ela--pda">1. Big‑picture overview - why shared latents + PMEM → ELA → PDA?</h4> <p>Modern whole‑brain recordings are heterogeneous across subjects, sessions, tasks and modalities. If we analyse each participant in their own idiosyncratic space, descriptors of “brain state” are not directly comparable. Our pipeline solves this in two moves:</p> <ol> <li>Population‑universal shared latents: We align subjects into a common, low‑dimensional space (SRM / MCCA / Group PCA‑ICA with consensus). Variables have stable meaning across participants and runs, so everything downstream is comparable and reproducible.</li> <li> <p>Physics‑grounded descriptors: On the binarised latents we fit a pairwise maximum‑entropy model (PMEM/Ising), then read out two complementary summaries:</p> <ul> <li>Energy‑Landscape Analysis (ELA) - an attractor‑and‑barrier view of the fitted Ising energy. It yields minima/basins, disconnectivity graphs, barrier spectra, and kinetic descriptors (basin dwell times, mean first-passage times (MFPTs), committors, relaxation times). This is the mechanistic, state‑space view.</li> <li>Phase‑Diagram Analysis (PDA) - a macroscopic view that places each subject on the <strong>(\(\mu\), \(\sigma\))</strong> plane of a disordered (via parametric perturbations) Ising model (SK‑like). In broad outline, it uses multiple observables at once to locate individuals relative to critical boundaries, providing cohort‑comparable coordinates and uncertainty.</li> </ul> </li> </ol> <h4 id="2-where-the-methods-come-from-intuitive-recap">2. Where the methods come from (intuitive recap)</h4> <ul> <li><strong>PMEM/Ising:</strong> Among all binary models that match the empirical means and pairwise correlations, PMEM has maximum entropy. It is equivalent to the zero‑temperature Ising family used throughout statistical physics. Minimal assumptions; parameters are interpretable as fields \(h_i\) and couplings \(J_{ij}\).</li> <li> <p><strong>ELA:</strong> Treat the fitted Ising as an energy landscape:</p> <p>\(E(\mathbf{s}) = -\sum_i h_i s_i - \tfrac{1}{2}\sum_{i\neq j} J_{ij} s_i s_j\),</p> <p>over binary states \(\mathbf{s}\in\{-1,+1\}^N\).</p> <p>Local minima = attractors; energy differences = barriers; transition graphs + Markov kinetics = how the brain moves between them.</p> </li> <li><strong>PDA:</strong> In spin‑glass models, the distribution of couplings matters. If the off‑diagonal \(J_{ij}\) have mean \(\mu\) and standard deviation \(\sigma\) with \(h_i\approx 0\), the system sits in regions (paramagnetic / spin‑glass / ferromagnetic) that govern ordering, glassiness and susceptibility. PDA maps each subject onto this phase surface so cohorts can be compared at a glance.</li> </ul> <h4 id="3-why-shared-latents-are-necessary-and-useful">3. Why shared latents are necessary (and useful)</h4> <ul> <li><strong>Stable semantics:</strong> Each latent represents the same population‑level pattern across participants, which makes ELA basins and PDA coordinates directly comparable.</li> <li><strong>Tractability:</strong> PMEM scales as \(\mathcal{O}(N^2)\) parameters; a well‑chosen latent space puts us in the sweet spot between information retention and robust estimation.</li> <li><strong>Downstream identifiability:</strong> Binarisation → PMEM → ELA/PDA relies on on/off switching. Our alignment preserves this structure and gives us comparable switching rasters across the cohort.</li> <li><strong>Utility‑forward:</strong> with one aligned space we can publish shared phase diagrams and landscape reports that are re‑usable across datasets and modalities, enabling baseline‑independent comparisons and cross‑study synthesis.</li> </ul> <p><strong>Select practical advantages of our framework:</strong></p> <p>The workflow is largely standalone and implemented locally, from scratch - allowing researchers/analysts to adapt its workings to their exact needs - with numerical-stability and computational-efficiency improvements (relative to analogous implementations in the domain), novel algorithmic extensions (i.a. for multi-subject dimensionality reduction, comparative phase diagram analysis of heterogeneous brain dynamics), informative metrics and rich visualisations, and emphasis on <strong>a)</strong> automated parameter optimisation - not requiring domain-expertise or significant prior experience with the pipeline from the user, <strong>b)</strong> data-driven model selection, <strong>c)</strong> data-modality universality and independence of heuristics/meta-knowledge throughout the entire design process, and <strong>d)</strong> availability of alternative methods and hyperparameters for key processing/modelling stages, so as to best fit the needs of the user</p> <p><strong>Limitations worth remembering:</strong></p> <ul> <li>Binarisation coarsens signals (whereas more granular discretisation becomes computationally prohibitive almost instantly for real-life data/problems)</li> <li>Results depend on the selection of binarisation thresholds, dimensionality-reduction models and target counts of obtained latent features</li> <li>For exact modelling methods, the set of possible states doubles in size with every additional feature/node/brain region</li> <li>PDA assumes \(h\approx 0\) and an SK‑like (Sherrington-Kirkpatrick) parametrisation</li> <li>ELA explores single‑spin‑flip pathways, which is a limited and simplified assumption</li> </ul> <p>To counteract the influence of initial choice of (hyper)parameters, we quantify uncertainty in the workflow, track convergence wherever applicable, offer truly data-driven and bias-free optimisation of pipeline parameters, and expose diagnostics so these choices remain transparent and testable.</p> <hr/> <h2 id="motivation-and-contribution">Motivation and contribution</h2> <p>Comparing whole-brain dynamics across individuals is hard without a common reference that preserves interpretability and quantifies uncertainty. This challenge becomes even more apparent in studies of complex brain processes spanning cognition, sensory integration, and perception; when the data are limited; or when comparing brain dynamics driven by systematically different sub-types of various neurodevelopmental, psychiatric, or neurodegenerative conditions. Aiming to address these challenges and reflecting the UniReps community’s interest in representational alignment and comparability across subjects, datasets, and models, this post demonstrates:</p> <ul> <li>a subject-alignment front-end that produces shared latent representations with stable semantics across a population, offering several alternative approaches</li> <li>a stitched, physics-grounded back-end (PMEM to ELA to PDA) that yields mechanistically interpretable descriptors and shared phase-diagram coordinates derived with our original methodology</li> <li>a robustness-first toolkit that includes custom-built consensus alignment, automated parameter selection, uncertainty quantification, diagnostics, and review-ready artefacts</li> </ul> <hr/> <h2 id="pipeline-overview">Pipeline overview</h2> <ol> <li>Preprocess and engineer: ELA-secure detrending, conservative handling of missing data, safe within-subject run concatenation, per-region standardisation.</li> <li>Population-aware alignment and dimensionality reduction: shared latents via SRM, MCCA, or Group PCA or Group ICA with consensus; automatic dimensionality selection.</li> <li>Binarisation: per-latent threshold (usually median or mean, unless domain-expertise justifies, e.g., percentile-based thresholding), yielding +/-1 time series.</li> <li>PMEM or Ising fitting: exact (small N), safeguarded pseudo-likelihood, or variational Bayes - each enriched with its adequate set of solution-stability and significance/quality assessments.</li> <li>Energy-landscape analysis: attractors, barriers, disconnectivity graph, occupancies, kinetics, and many more descriptors providing mechanistic, biologically meaningful insight into brain dynamics, as well as facilitating direct and intuitive comparisons between subjects/cohorts.</li> <li>Phase-diagram analysis: multi-observable placement on a shared reference surface with our custom cost function, reports on confidence intervals, and near-criticality indices.</li> </ol> <p><strong>Data summary</strong> (used for the development and testing of the pipeline; details not central to current discussion in themselves):</p> <p>Resting-state fUS from N=8 mice (7 Cre-lox ASD models spanning 4 subtypes; 1 control with no symptomatic manifestation modelled); 54 bilateral regions (27 L/R pairs; whole-brain collection) - unified across all the subjects; two runs per mouse (1494 frames for each recording session; TR ≈ 0.6 s); runs concatenated per subject.</p> <figure class="l-page"> <iframe src="/blog/assets/plotly/2025-10-25-phase-diagram-playbook/3D-ELA.html" title="Interactive 3D energy landscape of an example subject" loading="lazy" style="width:100%; aspect-ratio: 10 / 10; border:0;" allowfullscreen=""></iframe> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Interactive 3D energy landscape for an example mouse</strong> <br/><br/> <noscript> <a href="/blog/assets/plotly/2025-10-25-phase-diagram-playbook/3D-ELA.html"> Open the interactive figure. </a> </noscript> </figcaption> </figure> <figure class="l-page"> <iframe src="/blog/assets/plotly/2025-10-25-phase-diagram-playbook/3D-ELA2.html" title="Interactive 3D energy landscape of another example subject" loading="lazy" style="width:100%; aspect-ratio: 10 / 10; border:0;" allowfullscreen=""></iframe> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Interactive 3D energy landscape for another example mouse </strong> <br/><br/> <noscript> <a href="/blog/assets/plotly/2025-10-25-phase-diagram-playbook/3D-ELA2.html"> Open the interactive figure. </a> </noscript> </figcaption> </figure> <hr/> <h2 id="1-ela-secure-preprocessing-brief-overview">1) ELA-secure preprocessing (brief overview)</h2> <p>Aim: remove spikes, outliers, and slow global drifts while <strong>preserving the on/off switching structure</strong> that drives binarisation, PMEM fitting, and ELA/PDA. The procedure is modality-agnostic, non-invasive, and parameterised to be reproducible.</p> <ul> <li>Adaptive per-region parameters are computed from simple statistics and Welch spectra, then re-adapted after each step if requested (robust mode). <d-cite key="welch1967psd"></d-cite></li> <li>Despiking uses derivative-based detection with local, percentile-scaled replacements in short contextual windows; consecutive spikes are handled as blocks.</li> <li>Outlier removal is IQR-based with the same local, percentile-scaled replacement; an optional second pass is available.</li> <li>Population-aware detrending uses a cohort-optimised LOESS trend (fraction selected by stationarity and autocorrelation reduction)<d-cite key="cleveland1979lowess"></d-cite>. The global trend is estimated on the mean signal and scaled per region, which corrects drift without flattening transitions.</li> <li>Optional steps: light smoothing, bandpass filtering, breaking long flat runs, temporal standardisation, and spatial normalisation.</li> <li>Outputs include per-step amplitude deltas and residual checks; we also report the concordance between pre- and post-detrending binary states to ensure switching patterns are retained.</li> </ul> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20111942.png" alt="Five-step preprocessing pipeline panels for one region"/> <figcaption style="color:#f5f5f5;background:rgba(45,45,0,.1);padding:.6rem .8rem;border-radius:8px;"> Five-step preprocessing for a representative region (Striatum dorsal, R): Original → after despiking (derivative-based detection with local replacement) → after outlier removal (IQR with local context) → after universal LOESS detrending (global trend removed, transitions preserved) → after spatial normalisation to [0, 1]. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20111451.png" alt="Bars showing global linear-trend strength and magnitude across recordings"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> Cohort-wide evidence for a global trend. Top: absolute linear-trend correlation \(|r|\) for each recording with a decision threshold (red dashed line). Bottom: relative trend magnitude with its threshold. Many runs exceed both criteria, motivating a universal detrending step. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20111514.png" alt="Global signal with linear, quadratic and LOESS trends; residuals comparison"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> Model selection for detrending on an exemplar run. Top: global signal with linear, quadratic, and LOESS fits (LOESS selected). Bottom: residuals after each method. LOESS yields the most stationary, least autocorrelated residuals, hence chosen for the universal detrending step. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20111548.png" alt="Example regions: raw vs detrended signals and binary-state rasters"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> Region-wise example showing that global detrending preserves the on/off switching used downstream. Left: raw vs detrended signals for dentate gyrus (top) and thalamus (bottom). Right: binary rasters before/after median thresholding; “Concordance” in the panel titles reports the fraction of timepoints whose binary state is unchanged by detrending. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20111524.png" alt="Table of global-trend metrics and post-detrending stationarity flags for all recordings"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> Audit table across all recordings: linear-trend correlation and \(p\)-value, relative trend magnitude, and stationarity of residuals after linear, quadratic, and LOESS detrending. Most datasets achieve stationarity only with LOESS, supporting the universal-detrending choice. </figcaption> </figure> <details><summary>Click to expand: practical notes</summary> <ul> <li>Replacement never injects artificial structure: values are drawn from local percentiles and clamped to local ranges; neighbors can be skipped to avoid bleeding flagged samples.</li> <li>Block handling groups consecutive indices to avoid fragmentation; a de-blocking fix prevents long identical segments after replacements.</li> <li>Universal LOESS fraction is chosen across the cohort to balance residual stationarity, autocorrelation reduction, and variance explained; region-wise application only scales that global trend.</li> <li>All steps are seedable and config-driven; logs capture chosen parameters, max amplitude changes, and QC metrics for auditability.</li> </ul> <h4 id="remark">Remark:</h4> <p>For ELA, methods must enhance quality without compromising temporal structure. Safe, beneficial steps:</p> <ul> <li>Despiking: generally beneficial; percentile/MAD-based, no hardcoded amplitude thresholds.</li> <li>Outlier removal: single or repeated; statistics-driven safeguards prevent over-removal.</li> <li>Detrending: crucial for our data; prefer the universal LOESS approach for cross-subject/session/region comparability. Region-wise detrending should be used only with domain context; the cohort-wide analysis supports global detrending, with expert review welcomed.</li> <li>Flat-blocks breaking: apply if replacements create long identical runs.</li> <li>Smoothing: only very mild, primarily cosmetic; avoid aggressive windows that could distort binarisation.</li> </ul> <p>Bandpass filtering can be performed if not already applied to the data - many recording devices perform it automatically. Temporal standardisation and spatial normalisation are not required for ELA itself but are retained for general use; spatial normalisation is applied for the imputation pipeline to align signs and amplitudes. LOESS may shift some series below zero; this is expected and accounted for. All parameterisations are chosen to remain strictly ELA-compatible.</p> </details> <h3 id="preprocessing-supplement-i---biologically-plausible-imputation-of-time-series-for-entire-missing-brain-regions">Preprocessing Supplement I - biologically plausible imputation of time series for entire missing brain regions</h3> <details><summary>Click to expand: Imputation</summary> <h3 id="brain-region-imputation-why-this-sub-pipeline-works">Brain-region imputation: why this sub-pipeline works</h3> <p>This sub-pipeline fills in missing regional time series in whole-brain power-Doppler recordings while <strong>preserving temporal structure</strong> and providing <strong>auditable quality checks</strong>. It offers three complementary strategies and a common validation suite:</p> <h3 id="what-the-pipeline-does-high-level-summary">What the pipeline does (high-level summary)</h3> <ul> <li> <p><strong>Detect &amp; canonise</strong> Finds mice/runs with missing regions and re-indexes to a canonical (reference-mouse) ordering so every region has a stable label and posiion.</p> </li> <li> <p><strong>Quantify bilateral coupling</strong> For each left/right pair it computes change-based correlation, raw correlation, directional agreement, magnitude coupling, lagged cross-correlation (with optimal lag), and frequency-domain coherence. These metrics tell us whether a contralateral trace is a <strong>plausible shape donor</strong> and provide thresholds for safe use.</p> </li> <li> <p><strong>Offer three imputation routes</strong></p> <ol> <li> <p><strong>Bilateral (shape-copy with lag alignment):</strong> Mirrors the contralateral region after shifting by a <strong>median optimal lag</strong> estimated from reference mice. It <strong>does not scale amplitudes</strong> (we work in normalised units), preserving the on/off <strong>shape</strong> that matters for downstream binarisation / PMEM / ELA/PDA. Optional light noise can be added in a non-deterministic mode.</p> </li> <li> <p><strong>Temporal (population median):</strong> Builds the missing series from the <strong>median temporal pattern</strong> of the same region across other mice (optionally across both runs). This is robust to outliers and yields stable reconstructions; with MAD-based jitter it can reflect natural variability while staying anchored to the cohort’s typical dynamics.</p> </li> <li> <p><strong>Clustering / nearest-neighbours:</strong> Groups reference mice/runs for the same region using correlation or Euclidean distance and imputes from the <strong>cluster mean</strong> of the nearest group. This conditions the reconstruction on <strong>matched dynamics</strong>, often outperforming global medians when cohorts are heterogeneous. A 3-D PCA visualisation makes the neighbourhoods and the imputed point <strong>inspectable</strong>.</p> </li> </ol> </li> <li> <p><strong>Validate, don’t guess</strong> Every imputed series is compared to the population using:</p> <ul> <li>mean absolute deviation to the cross-mouse mean,</li> <li>correlation with that mean,</li> <li><strong>coverage</strong> within 1–2 s.d.,</li> <li>change-correlation, peak cross-correlation and optimal lag,</li> <li>mean coherence and peak-coherence frequency,</li> <li><strong>power-spectrum correlation</strong>. These metrics are printed and plotted, so each imputation carries its own <strong>provenance and QC</strong>.</li> </ul> </li> </ul> <h3 id="why-this-is-useful-for-downstream-physics-style-analyses">Why this is useful for downstream physics-style analyses</h3> <ul> <li><strong>Shape-faithful</strong>: methods preserve <strong>temporal switching</strong> structure (crucial for binarisation → PMEM → ELA/PDA).</li> <li><strong>Cohort-aware</strong>: temporal and clustering routes borrow information only from the <strong>same labelled region</strong> across other mice/runs.</li> <li><strong>Bilateral advantage</strong>: when hemispheric symmetry is strong, lag-aligned mirroring recovers realistic trajectories with minimal bias.</li> <li><strong>Transparent &amp; reproducible</strong>: seeds are fixed; thresholds are explicit; NaNs and edge cases are handled defensively; outputs are re-indexed to a canonical order.</li> <li><strong>Method flexibility</strong>: you can pick the route that best matches your biology (e.g., bilateral for symmetric circuits; clustering for diverse cohorts) and still get the <strong>same validation bundle</strong>.</li> </ul> <hr/> <h3 id="figures">Figures</h3> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20150710.png" alt="Imputed Postrhinal area (R) time series overlaid with reference mice and population mean; bilateral change-scatter with metrics"/> <figcaption style="color:var(--theme-text,#eaeaea)"> <strong>Imputation outcome and bilateral synchrony (Postrhinal area, R; Run&nbsp;1).</strong> Top: the imputed trace (red) is plotted against reference mice (light blue) and the population mean (green); the y-axis is the normalised power-Doppler signal. Bottom: bilateral <em>change</em> scatter (left minus right first differences) with the diagonal “perfect synchronisation” guide, a fitted trend line, and summary metrics (change-correlation, peak cross-correlation with its optimal lag, mean coherence). Together these panels show that the imputed series follows cohort-typical dynamics and remains temporally consistent with its contralateral partner. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20150758.png" alt="Cross-correlation vs lag and magnitude-squared coherence for the imputed region across two runs"/> <figcaption style="color:var(--theme-text,#eaeaea)"> <strong>Lag-structure and frequency-locking (Postrhinal area, R; Runs&nbsp;1–2).</strong> For each run, the left panel shows cross-correlation across lags (dashed zero line for reference), and the right panel shows magnitude-squared coherence versus frequency. Peaks indicate preferred temporal offsets and shared frequency content; stable peaks across runs support consistent reconstruction rather than overfit noise. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20150815.png" alt="Console-style validation summary listing deviation from population mean, coverage within 1–2 SD, temporal and spectral metrics"/> <figcaption style="color:var(--theme-text,#eaeaea)"> <strong>Validation summary.</strong> For each run the pipeline reports deviation from the population mean, correlation with the mean, coverage within 1–2 standard deviations, change-correlation, peak cross-correlation and optimal lag, mean coherence, and power-spectrum correlation. These values provide an audit trail for every imputation. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20150835.png" alt="Console log showing missing region detection, reference coupling metrics, and selected imputation method for each run"/> <figcaption style="color:var(--theme-text,#eaeaea)"> <strong>Pipeline provenance.</strong> The log records which regions are missing, cohort-level bilateral coupling benchmarks used as context, and the selected imputation route (here, clustering-based). This makes method choice and inputs explicit for later review. </figcaption> </figure> <div style="position:relative;width:100%;max-width:980px;height:0;padding-bottom:62%;"> <iframe src="/blog/assets/plotly/2025-10-25-phase-diagram-playbook/3D-impu.html" title="Nearest-neighbours imputation — 3D PCA view" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;" allowfullscreen=""> </iframe> </div> <p class="figure-caption" style="color:var(--theme-text,#eaeaea);margin-top:.5rem;"> <strong>Nearest-neighbours in PCA space (interactive).</strong> Each marker is a reference mouse/run for the same region, embedded by PCA of the region’s time-series vectors; colours show clusters (nearest-neighbour groups). The <em>red diamond</em> is the imputed series; the <em>blue marker(s)</em> indicate the current mouse/run. </p> </details> <h3 id="preprocessing-supplement-ii--similarity-assessment-and-conditional-concatenation-of-recordings-from-different-sessions-for-each-mouse">Preprocessing Supplement II — similarity assessment and conditional concatenation of recordings from different sessions for each mouse</h3> <details><summary>Click to expand: Concatenation</summary> <h3 id="concatenating-two-recording-runs-per-mouse">Concatenating two recording runs per mouse</h3> <p>This section documents the methodology we use to <strong>decide whether two runs are similar enough to be concatenated</strong>, how we <strong>align</strong> them when needed, and what diagnostics we plot to make the process auditable.</p> <h3 id="what-similar-enough-to-concatenate-means">What “similar enough to concatenate” means</h3> <p>Before concatenation, we compare the two runs <strong>region-by-region</strong> and turn the result into a single pass/fail decision.</p> <h3 id="1-regionwise-similarity-time-domain-patterns">1) Regionwise similarity (time-domain patterns)</h3> <p>For each region \((r)\) present in both runs, we check a scalar <strong>similarity</strong> between its run-1 series \(\mathbf{x}_r\) and run-2 series \(\mathbf{y}_r\). Several options are available:</p> <ul> <li><strong>Pearson correlation</strong>: requires \(\operatorname{corr}\big(\mathbf{x}_r,\mathbf{y}_r\big)\ \ge\ \tau_{\mathrm{corr}}.\)</li> <li><strong>Spearman</strong> (rank correlation): compute \(\rho\in[-1,1],\) map to \([0,1]\) by \(\rho_{01}=\tfrac{1}{2}(\rho+1),\) and require \(\rho_{01}\ \ge\ \tau_{\mathrm{spearman}}.\) <em>(Good for monotone but non-linear similarity)</em></li> <li><strong>Kendall’s \(\tau\)</strong>: same mapping \(\tau_{01}=\tfrac{1}{2}(\tau+1),\) require \(\tau_{01}\ \ge\ \tau_{\mathrm{kendall}}.\)</li> <li><strong>Euclidean (shape) distance</strong>: min–max normalise both series to \([0,1]\), compute \(d=\frac{\lVert \mathbf{x}_r-\mathbf{y}_r\rVert_2}{\sqrt{T}},\) and require \(d\ \le\ \tau_{\mathrm{dist}}.\) <em>(Insensitive to absolute scale; tests waveform similarity)</em></li> <li><strong>Cosine similarity</strong>: map \(\cos\theta\in[-1,1]\) to \([0,1]\) via \(c_{01}=\tfrac{1}{2}(\cos\theta+1),\) and require \(c_{01}\ \ge\ \tau_{\mathrm{cos}}.\)</li> </ul> <p>Each region yields a boolean pass/fail; the resulting vector is our <strong>regionwise mask</strong>.</p> <h3 id="2-optional-distribution-similarity">2) (Optional) Distribution similarity</h3> <p>As a distributional check, we run a <strong>two-sample Kolmogorov–Smirnov test</strong> per region and declare a pass when the \(p-value\) exceeds a threshold, i.e. the two marginal distributions are not detectably different at the chosen level.</p> <h3 id="3-aggregating-to-a-single-gate">3) Aggregating to a single gate</h3> <p>We fuse regionwise pass/fail results into a final score using one of:</p> <ul> <li><strong>fraction</strong> (default): share of regions that pass; concatenate if \(\mathrm{fraction}\ \ge\ \tau_{\mathrm{agg}}.\)</li> <li><strong>weighted</strong>: same as above but weights each region by amplitude or variance.</li> <li><strong>median / any / all</strong>: robust/lenient/strict alternates.</li> </ul> <p>The gate is reported as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[check_run_similarity] aggregator='&lt;mode&gt;', final_score=&lt;value&gt;, pass=&lt;True|False&gt;
</code></pre></div></div> <p>Only if it <strong>passes</strong>, or if <code class="language-plaintext highlighter-rouge">force_concatenate=True</code>, do we proceed.</p> <h3 id="alignment-making-run-2-comparable-to-run-1">Alignment: making run-2 comparable to run-1</h3> <p>If concatenation proceeds, we align <strong>levels</strong> of run-2 to run-1 <strong>per region</strong> (no temporal warping):</p> <ul> <li><code class="language-plaintext highlighter-rouge">alignment_mode='match_medians'</code> (default): for each region \((r)\), \(\mathbf{y}_r \leftarrow \mathbf{y}_r + \big(\operatorname{median}(\mathbf{x}_r)-\operatorname{median}(\mathbf{y}_r)\big)\)</li> <li>Alternatively, <code class="language-plaintext highlighter-rouge">match_means</code> uses the mean instead of the median.</li> </ul> <p><strong>Why this is safe:</strong> the ELA/PDA pipeline is driven by <strong>on/off switching and relative changes</strong>. Shifting a time series by a constant to match central tendency <strong>does not</strong> distort the temporal structure we use downstream.</p> <h3 id="preprocessing-before-the-gate-optional-but-recommended">Preprocessing before the gate (optional but recommended)</h3> <p>Both runs can first pass through a light, ELA-secure preprocessing stack (despiking, robust outlier handling, LOESS detrending, mild smoothing). Parameters are seedable and adapt across regions. This improves the reliability of similarity estimates without changing the switching dynamics that matter later.</p> <h3 id="concatenation-step">Concatenation step</h3> <p>After alignment, we intersect region indices and <strong>horizontally concatenate</strong> the two runs (time dimension doubles). An optional last <strong>outlier smoothing</strong> can be applied to the concatenated trace using a robust IQR rule.</p> <h3 id="diagnostics-and-what-the-plots-show">Diagnostics and what the plots show</h3> <p>The helper <code class="language-plaintext highlighter-rouge">show_intermediate_concat_plots(...)</code> produces a <strong>3×2</strong> panel (if preprocessing is enabled):</p> <ul> <li><strong>Row 1:</strong> Run-1 RAW (left) and Run-2 RAW (right). Orange dashed line = mean; bright-green dashed line = median.</li> <li><strong>Row 2:</strong> Run-1 Preprocessed (left) and Run-2 Preprocessed (right) with the same guides.</li> <li><strong>Row 3:</strong> <strong>Aligned Run-2</strong> (left; grey dashed = pre-alignment, green = aligned) and <strong>Final Concatenated</strong> (right; with mean/median guides).</li> </ul> <p>A shared legend clarifies the mean/median guides. These views make the gating, alignment, and final result fully inspectable.</p> <hr/> <h3 id="figures">Figures</h3> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20154328.png" alt="Console readout of regionwise similarity gate for multiple mice"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Run-similarity gate (console readout).</strong> For each mouse, we compute a per-region similarity mask and aggregate it to a single decision. The line shows the chosen aggregator, the final score, and pass/fail. Only runs that pass are aligned and concatenated; failing pairs are skipped to avoid mixing incompatible sessions (although similarity checks can be manually overridden and the user could force concatenation anyway). </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20154306.png" alt="Six-panel plot illustrating raw runs, preprocessed runs, alignment of run 2 to run 1, and final concatenation with mean/median guides"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Concatenation diagnostics for a representative region.</strong> <em>Top row:</em> raw run-1 (left, blue) and raw run-2 (right, red) with orange mean and bright-green median lines. <em>Middle row:</em> the same region after preprocessing. <em>Bottom-left:</em> alignment step—run-2 before (grey dashed) and after (green) median-matching to run-1. <em>Bottom-right:</em> final concatenated trace with mean/median guides. These panels document that the two runs are comparable, that level alignment has worked as intended, and that the final time series is suitable for downstream analyses. </figcaption> </figure> <hr/> <h2 id="why-this-approach-is-robust-and-useful">Why this approach is robust and useful</h2> <ul> <li><strong>Prevents spurious mixing.</strong> The gate stops concatenation when the two runs <strong>do not</strong> tell a consistent story for most regions. This protects subsequent ELA/PDA stages from artefactual discontinuities.</li> <li><strong>Flexible similarity notions.</strong> You can choose correlation-based (linear or rank), cosine (directional), or Euclidean-shape metrics, depending on whether absolute scale or monotonicity matters most for your data The rank-based options (Spearman/Kendall) are especially <strong>stable</strong> for neural time series, where amplitude changes can be non-linear or non-stationary.</li> <li><strong>Scale-safe alignment.</strong> Median/mean level-matching fixes global offsets without altering temporal structure, keeping <strong>on/off</strong> switching intact—the key for binarisation and PMEM fitting.</li> <li><strong>Transparent diagnostics.</strong> The console summary and 3×2 figure make each decision inspectable. If a pair fails, you immediately see why; if it passes, you can verify that alignment has not introduced distortions.</li> <li><strong>Configurable strictness.</strong> Thresholds \((\tau_{\mathrm{corr}},\ \tau_{\mathrm{spearman}},\ \tau_{\mathrm{dist}},\ \dots,\ \tau_{\mathrm{agg}})\) and the aggregation rule control how strict the gate is. You can be conservative for clinical datasets and more permissive for exploratory work.</li> </ul> <hr/> <h3 id="minimal-recipe-what-the-functions-do">Minimal recipe (what the functions do)</h3> <ol> <li><strong>Preprocess</strong> each run (optional, recommended).</li> <li><strong>Compute regionwise similarity</strong> with your chosen method.</li> <li><strong>Aggregate</strong> passes into a final gate score; stop if it fails.</li> <li><strong>Align levels</strong> (<code class="language-plaintext highlighter-rouge">match_medians</code>/<code class="language-plaintext highlighter-rouge">match_means</code>) per region.</li> <li><strong>Concatenate</strong> the two runs (time axis).</li> <li><strong>(Optional) Final outlier pass</strong> on the concatenated series.</li> <li><strong>Plot diagnostics</strong> for audit and reports.</li> </ol> <p>This procedure is <strong>seeded, reproducible, and auditable</strong>, and it keeps exactly the aspects of the signal that matter for your subsequent ELA/PDA pipeline.</p> </details> <hr/> <h2 id="2-population-universal-shared-latent-space">2) Population-universal shared latent space</h2> <p>Goal: obtain shared, ELA-secure latent components whose semantics are stable across subjects and runs, so downstream binarisation → PMEM → ELA/PDA is directly comparable and computationally tractable.</p> <h4 id="novelty-factor-at-a-glance">Novelty factor at a glance:</h4> <ul> <li>Pick-and-mix reductions: (Group PCA, Group ICA, SRM, MCCA) with robust tweaks (varimax, multi-restart ICA, PCA-initialised SRM, whitened MCCA).</li> <li>A population-aware, multi-metric objective (structural + temporal + method-specific) → auto-selection of dimensionality and hyperparameters.</li> <li>Alignment toolkit (orthogonal Procrustes with column-wise sign fixes, Hungarian matching, and a neuro-Procrustes consensus).</li> <li>Synergy-weighted consensus across methods with dominance checks, stability (RV) analysis, and per-component contribution audits <d-cite key="robert1976rv"></d-cite>.</li> <li>Efficient, reproducible compute (SVD on averaged covariances, fast downsampling for temporal metrics, parallel grid search, seeded randomness).</li> </ul> <hr/> <h3 id="21-methods-population-aware-ela-secure">2.1 Methods (population-aware, ELA-secure)</h3> <p>All methods preserve temporal ordering (only linear projections or orthogonal transforms over channels), and we quantify temporal faithfulness (trustworthiness/continuity, autocorrelation preservation) <d-cite key="venna2006localmds"></d-cite>.</p> <ul> <li> <p>Group PCA (variance capture + varimax): eigenvectors of the average subject covariance; optional varimax keeps components sparse/interpretable without breaking orthogonality <d-cite key="SMITH2014738,kaiser1958varimax"></d-cite>. <em>Extras:</em> elbow detection, subject-wise EVR, reconstruction error, and post-rotation low-variance pruning.</p> </li> <li> <p>Group ICA (shared independent subspaces): subject-wise PCA → concatenation → FastICA <d-cite key="hyvarinen1999fastica"></d-cite> with multi-restart selection (best negentropy proxy via kurtosis deviation), independence verification (mean \(\lvert\mathrm{corr}\rvert\), kNN mutual information <d-cite key="kraskov2004mi"></d-cite>, squared-corr checks), sign harmonisation of subject contributions before averaging.</p> </li> <li> <p>SRM (shared timecourses with subject-specific maps): iterative orthogonal subject mappings \((W_i)\) and shared response \((S)\), PCA-based initialisation, Hungarian-matched alignment of mappings across subjects, orthogonality diagnostics, shared variance quantification <d-cite key="NIPS2015_b3967a0e"></d-cite>.</p> </li> <li> <p>MCCA (maximising cross-subject correlation): per-subject whitening → SVD on concatenated features (SUMCOR-style) → iterative refinement of subject loadings \((a_i)\) <d-cite key="60b14841-3c7e-3751-88f2-15308f78bf55,decheveigne2019mcca,correa2010mcca"></d-cite>. We report cross-subject alignment, canonical correlations to shared response, orthogonality in whitened space, and shared variance in native space.</p> </li> </ul> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20180713.png" alt="Group PCA scree: individual and cumulative explained variance with elbow and totals"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Group PCA scree.</strong> Light-blue bars show individual explained-variance ratio (EVR) per component; the black line is cumulative EVR. The dotted vertical line marks the elbow (here at the 1st component), and the title reports total variance explained (65.6%). This plot guides the initial range for dimensionality selection before our multi-metric model choice. </figcaption> </figure> <figure class="l-page"> <div style="display:flex; gap:1rem; flex-wrap:wrap;"> <img style="flex:1 1 360px; max-width:49%;" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181009.png" alt="SRM consensus mapping W: region-by-component loadings heatmap"/> <img style="flex:1 1 360px; max-width:49%;" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181039.png" alt="SRM example subject: reduced correlation between latent components"/> </div> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>SRM latent space.</strong> <em>Left:</em> consensus SRM mapping \(W\) (regions × components). Colours indicate signed loadings (arbitrary overall sign but harmonised across subjects), highlighting stable spatial patterns shared across the cohort. <em>Right:</em> example subject’s correlation matrix between SRM latent time series. Light (near-white) off-diagonals indicate low cross-component correlation, i.e. little superfluous overlap—evidence of efficient representation and dimensionality reduction. </figcaption> </figure> <figure class="l-page"> <div style="display:flex; gap:1rem; flex-wrap:wrap;"> <img style="flex:1 1 360px; max-width:49%;" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181248.png" alt="MCCA example subject: reduced correlation between latent components"/> <img style="flex:1 1 360px; max-width:49%;" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181218.png" alt="MCCA subject projection matrix a: region-by-component loadings"/> </div> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>MCCA latent space.</strong> <em>Left:</em> example subject’s correlation matrix between MCCA components—again, light off-diagonals reflect low redundancy across latents. <em>Right:</em> subject-specific MCCA projection matrix \(a\) (regions × components), showing how each brain region contributes to each shared component; structured bands point to interpretable, population-aligned patterns. </figcaption> </figure> <blockquote> <p>Why four methods? They trade off interpretability, independence, correlation sharing, and variance capture. We score them with the same unified, ELA-aware metric suite and fuse them, yielding a stable, population-universal latent space.</p> </blockquote> <hr/> <h3 id="22-alignment--sign-consistency-critical-for-comparability">2.2 Alignment &amp; sign consistency (critical for comparability)</h3> <ul> <li>Orthogonal Procrustes with column-wise sign checks (flip a component if it anticorrelates with the reference) <d-cite key="schonemann1966procrustes"></d-cite>.</li> <li>Hungarian matching aligns component order across methods/subjects by maximal absolute correlations <d-cite key="kuhn1955hungarian"></d-cite>.</li> <li>Neuro-Procrustes consensus: iterative, order-robust alignment across methods (SVD-based reference) with final sign harmonisation.</li> <li>Optional biological sign protocol (baseline-anchored flips) to stabilise polarities across datasets.</li> </ul> <hr/> <h3 id="23-metrics--selection-multi-objective-normalised-to-01">2.3 Metrics &amp; selection (multi-objective, normalised to [0,1])</h3> <p>Structural fidelity (RSA-style): correlation between original region-by-region correlation structure and that reconstructed from latents; sign preservation weighted by \(\lvert\mathrm{corr}\rvert\); Procrustes disparity; Kruskal stress <d-cite key="kruskal1964nmds1"></d-cite> on inter-region distances.</p> <p>Temporal faithfulness: trustworthiness/continuity of neighbour relations, temporal neighbourhood hits, autocorrelation preservation at task-relevant lags.</p> <p>Method-specific:</p> <ul> <li>PCA: mean subject EVR, reconstruction error.</li> <li>ICA: independence (mean \(\lvert\mathrm{corr}\rvert\)↓, Mutual Information (MI)↓), kurtosis (≠3), sparsity.</li> <li>SRM: shared alignment (Hungarian-matched), orthogonality, shared variance.</li> <li>MCCA: cross-subject alignment, shared variance, canonical correlations, orthogonality (whitened).</li> </ul> <p>Normalisation uses principled maps (e.g., \(x \mapsto \tfrac{1}{1+x}\) for “smaller-is-better”, linear \([-1,1]\to[0,1]\) for correlations). Composite score: weighted average (user- or default weights), then auto-optimise \((k)\) and hyperparameters via seeded, parallel grid search.</p> <details><summary>Click to expand: metric formulas</summary> <p>Kruskal stress (upper-triangle, variance-matched):</p> \[\mathrm{Stress}_1 =\min_{b&gt;0}\sqrt{\frac{\sum_{i&lt;j}\big(d_{ij}-b\,\hat d_{ij}\big)^2} {\sum_{i&lt;j} d_{ij}^2}},\qquad b^\star=\frac{\sum_{i&lt;j} d_{ij}\,\hat d_{ij}}{\sum_{i&lt;j} \hat d_{ij}^2}\] <p>Procrustes (disparity surrogate):</p> \[R^\star=\arg\min_{R\in O(k)}||A-BR||_F,\qquad with \quad B^\top A = U\Sigma V^\top, \quad R^\star = UV^\top, \quad A,B\in\mathbb{R}^{n\times k}\] <p>RV stability (consensus vs method): \(\mathrm{RV}(A,B)=\frac{\operatorname{tr}(A^\top B)}{\sqrt{\operatorname{tr}(A^\top A)\operatorname{tr}(B^\top B)}} \quad\)</p> <p>(illustrative RV formulation for comparing matrices of the same size: recording sites X number of observations)</p> <p>Composite (normalised metrics \(\tilde m\in[0,1]\)): \(J=\frac{\sum_m w_m\tilde m}{\sum_m w_m}\)</p> </details> <hr/> <h3 id="24-consensus-across-methods-synergy-weighted-stability-checked">2.4 Consensus across methods (synergy-weighted, stability-checked)</h3> <p>We build a weighted consensus matrix after cross-method alignment:</p> <ol> <li>Align each method’s components (Hungarian or neuro-Procrustes).</li> <li> <p>Synergy weights per method: \(\tilde w_m \propto \big(\mathrm{composite}_{\mathrm{specific},m}\big)^{\alpha}\, \big(\mathrm{composite}_{\mathrm{common},m}\big)^{1-\alpha},\qquad w_m=\frac{\tilde w_m}{\sum_j \tilde w_j}\)</p> </li> <li>Weighted sum of aligned components → column L2 re-normalisation.</li> <li>Validation: RSA metrics, Procrustes disparity, variance explained, RV stability, reconstruction consistency with each method, and a dominance index (Gini/CV/TV-distance) to ensure no method overwhelms the fusion.</li> </ol> <details><summary>Click to expand: consensus diagnostics &amp; thresholds</summary> <ul> <li>Component alignment quality: mean inter-method \(\lvert\mathrm{corr}\rvert\) per component, with permutation and bootstrap thresholds, plus a data-driven clustering criterion for bimodal quality profiles.</li> <li>Subject consistency: mean cross-subject \(\lvert\mathrm{corr}\rvert\) of per-component timecourses, with phase-randomised surrogate thresholds (preserve autocorrelation).</li> <li>Dominance (balance of synergy weights): we report normalised Gini, CV, TVD, and min/max ratio (all mapped to [0,1]).</li> </ul> </details> <figure class="l-page"> <img loading="lazy" alt="Bar chart of cross-subject component consistency with statistical thresholds" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181523.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Cross-subject consistency of shared components.</strong> Each bar is the mean absolute correlation of the same component across subjects (higher = more reproducible). Dashed/dotted lines show permutation and 95% CI thresholds. Components above both guides generalise well across mice and are kept for the consensus latent space. </figcaption> </figure> <figure class="l-page"> <img loading="lazy" alt="Bar chart of cross-method alignment quality with thresholds" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181355.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Alignment quality across methods.</strong> Bars show how well each consensus component matches its counterparts from Group PCA, Group ICA, SRM and MCCA after Procrustes + Hungarian alignment (mean |corr|). Higher values and crossing the dashed/dotted thresholds indicate robust cross-method agreement, supporting the fused consensus basis. </figcaption> </figure> <figure class="l-page"> <img loading="lazy" alt="Stacked bars showing method contributions to each consensus component" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20181438.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Method contributions to the consensus components.</strong> Stacked bars report the weighted influence of each method (Group PCA / Group ICA / SRM / MCCA) on every consensus component. Balanced contributions mean the final space is not dominated by a single method and retains structure that is consistent across approaches. </figcaption> </figure> <figure class="l-page"> <img loading="lazy" alt="Heatmap of Group PCA transformation matrix after alignment" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20183547.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Example aligned loadings: Group PCA.</strong> Brain regions × components matrix (unit-normalised) after cross-subject/method alignment. Warmer/colder cells mark stronger positive/negative loadings; light colours near zero indicate sparse, non-overlapping contributions—useful for efficient representation and reduced redundancy. </figcaption> </figure> <figure class="l-page"> <img loading="lazy" alt="Triangular heatmap of cross-method component correlations after alignment" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20183612.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Cross-method component correlations (post-alignment).</strong> Each block compares components across the four methods. A sharp diagonal with light off-diagonals shows one-to-one matches and little superfluous overlap, validating that different methods recover consistent latent directions. </figcaption> </figure> <figure class="l-page"> <img loading="lazy" alt="Heatmap of the final consensus transformation matrix" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-08%20083629.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Final consensus transformation.</strong> Regions × components loadings that define the population-universal latent space used for all subjects. The mapping is sign-harmonised and unit-normalised so that latents have consistent semantics across mice and runs. </figcaption> </figure> <figure class="l-page"> <img loading="lazy" alt="Correlation matrix of consensus component time series for one subject" src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20183651.png"/> <figcaption style="color:#f5f5f5;background:rgba(0,0,0,.45);padding:.6rem .8rem;border-radius:8px;"> <strong>Consensus latent space — within-subject orthogonality.</strong> Correlation matrix of consensus component time courses for an example mouse. Near-zero off-diagonals (light colours) indicate low redundancy between latents, which aids binarisation and stabilises the downstream PMEM/ELA/PDA steps. </figcaption> </figure> <hr/> <details><summary>Click to expand: Mathematical underpinnings</summary> <h3 id="25-mathematical-cores-rigorous-but-compact">2.5 Mathematical cores (rigorous but compact)</h3> <p><strong>SRM (orthogonal, correlation‑maximising form)</strong></p> <p>For subjects \(X_i \in \mathbb{R}^{T \times d}\), find \(W_i \in \mathbb{R}^{d \times k}\) with \(W_i^\top W_i = I\) and a shared response \(S \in \mathbb{R}^{T \times k}\):</p> \[\min_{\{W_i\},\, S}\; \sum_i \lVert X_i W_i - S \rVert_F^2 \;\Longleftrightarrow\; \max_{\{W_i\},\, S}\; \sum_i \operatorname{tr}\!\big(S^\top X_i W_i\big) \quad \text{s.t. } W_i^\top W_i = I .\] <p>Updates:</p> \[S \leftarrow \frac{1}{n} \sum_i X_i W_i ,\] <p>then z‑score \(S\) column‑wise.</p> \[W_i \leftarrow U V^\top, \qquad \text{where } U \Sigma V^\top = \operatorname{SVD}\!\big(X_i^\top S\big).\] <p>*The above formulation reflects our shared-T assumption - since all the subjects from the dataset used for developing the pipeline had the exact same number of frames in their time series.</p> <hr/> <p><strong>MCCA (SUMCOR‑style, whitened)</strong></p> <p>Let \(X_i\) be centred and whitened to \(\tilde{X}_i\). Find \(a_i \in \mathbb{R}^{d \times k}\) maximising total cross‑correlation:</p> \[\max_{\{a_i\}} \; \sum_{i&lt;j} \operatorname{tr}\!\big((\tilde{X}_i a_i)^\top (\tilde{X}_j a_j)\big) \quad \text{s.t. } a_i^\top a_i = I .\] <p>Solution via SVD on the concatenation:</p> \[\operatorname{SVD}\!\left(\big[\, \tilde{X}_1 \ \ \tilde{X}_2 \ \ \cdots \ \ \tilde{X}_n \,\big]\right),\] <p>then map back with subject‑specific unwhitening.</p> <hr/> <p><strong>Group PCA (population‑aware)</strong></p> <p>With subject covariances \(\Sigma_i \in \mathbb{R}^{d \times d}\), form the mean covariance \(\bar{\Sigma}=\frac{1}{n}\sum_{i=1}^{n}\Sigma_i .\)</p> <p>Eigen‑decompose the mean covariance \(\bar{\Sigma} = Q \Lambda Q^\top,\) where the columns of \(Q=[q_1,\dots,q_d]\) are orthonormal eigenvectors and \(\Lambda=\operatorname{diag}(\lambda_1\ge\dots\ge\lambda_d)\) .</p> <p>Select the top‑\(k\) eigenvectors \(E := [q_1\ \cdots\ q_k] \in \mathbb{R}^{d \times k}.\)</p> <p>(Optional) apply an orthogonal rotation \(R\in\mathbb{R}^{k\times k}\) (e.g., varimax) to improve sparsity/interpretability.</p> <p>The rotated loadings are \(L = ER, \text{ with } R^\top R = I.\)</p> <p>A standard varimax objective (orthomax with parameter \(\gamma\in[0,1]\)) is: \(R^\star = \arg\max_{R^\top R=I} \sum_{j=1}^k \left( \sum_{p=1}^d L_{pj}^4 - \frac{\gamma}{d}\left(\sum_{p=1}^d L_{pj}^2\right)^2 \right), \quad \text{for loadings } L = E R .\)</p> <p>Per‑subject time‑series data \(X_i \in \mathbb{R}^{T_i \times d}\) are projected to latent timecourses via:</p> \[Z_i = X_iL \in \mathbb{R}^{T_i \times k},\] <p>or, without rotation: \(Z_i = X_iE\).</p> <p><strong>Notes:</strong></p> <ul> <li>Using the average covariance \(\bar{\Sigma}\) ensures the components reflect population structure.</li> <li>Rotation is optional and keeps components orthogonal (since \(R\) is orthogonal). If we prefer unrotated principal components, use \(L=E\).</li> </ul> <hr/> <p><strong>Group ICA (robust)</strong></p> <p>Subject PCA → concatenation → FastICA. Restart with multiple seeds; select the run with the largest negentropy proxy (mean \(\lvert\mathrm{kurtosis} - 3\rvert\)), and verify independence (low mean \(\lvert\operatorname{corr}\rvert\), low kNN‑MI, low mean squared correlation).</p> </details> <hr/> <h3 id="26-practicalities-efficiency-reproducibility">2.6 Practicalities (efficiency, reproducibility)</h3> <ul> <li>Efficient linear algebra: SVD on averaged covariances (Group PCA), batched downsampling for temporal metrics, parallel grid search with seeded restarts.</li> <li>Diagnostics at each stage: subject EVR, reconstruction error, independence checks, alignment scores, consensus stability (RV), method dominance, and retention rationale for kept/dropped components.</li> <li>ELA-secure by design: all transformations are linear in space (no temporal warping), detrending was handled upstream, and temporal metrics explicitly guard the switching dynamics used by ELA.</li> </ul> <hr/> <h3 id="27-outputs-what-to-expect">2.7 Outputs (what to expect)</h3> <ul> <li>Method-specific models (components/loadings), aligned across methods.</li> <li>Consensus model (columns = population-universal latents), with weights and stability report.</li> <li>Per-subject projections (time × components) ready for binarisation → ELA/PDA.</li> <li>QC bundle: metric table, thresholded alignment &amp; consistency plots, dominance index, and retained-component justifications.</li> </ul> <hr/> <details><summary>Click to expand: implementation highlights &amp; safeguards</summary> <ul> <li>RSA-style structure preservation reconstructs regions from components (per-region least squares) to compare correlation matrices before/after, reporting Pearson on vectorised upper triangles, Frobenius/mean diffs, and weighted sign preservation.</li> <li>Temporal metrics (trustworthiness/continuity, neighbourhood hit) use downsampled representations for speed without losing neighbourhood signal; autocorr preservation checked at task-relevant lags.</li> <li>Robust sign handling: column-wise correlation checks after Procrustes; optional baseline-anchored flipping (percentile-based) prevents biologically implausible polarity swaps.</li> <li>Dominance index offers four normalised variants (Gini/CV/TVD/min-max) to ensure balanced consensus.</li> <li>Permutation/phase-randomisation thresholds provide statistical guardrails for declaring alignment/consistency “good enough.”</li> </ul> </details> <hr/> <h2 id="3-binarisation-of-latent-time-series">3) Binarisation of latent time series</h2> <p>After alignment and dimensionality reduction, each latent \(x_i(t)\) is thresholded per-latent using median or mean and mapped to \(s_i(t) \in \{-1,+1\}\).</p> <p>This respects component-specific baselines, keeps PMEM tractable, and standardises inputs for inter-subject comparability.</p> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-26%20183815.png" alt="Proportion of +1 (active) binary states across latents over time with linear trend, min and max markers"/> <figcaption style="color:rgba(255, 255, 255, 0.84);padding:.6rem .8rem;border-radius:8px;"> <strong>Binarisation sanity check: activity fraction over time.</strong> The blue trace shows, at each time step, the proportion of latents in the +1 state after per-latent median thresholding. The grey dashed line marks 0.5; orange dotted lines show the observed range (min ≈ 0.20, max ≈ 0.80). The red dashed line is the fitted linear trend (+0.052 percentage points per 100 steps; total change ≈ 1.57 pp). Near-stationary behaviour centred around 0.5 indicates balanced on/off usage and supports downstream PMEM fitting; large drifts would flag thresholding issues or residual global trends. </figcaption> </figure> <hr/> <h2 id="4-pairwise-maximum-entropy-ising-fitting">4) Pairwise maximum-entropy (Ising) fitting</h2> <p>We model each time‑point’s binary latent vector \(\mathbf{s}\in\{-1,+1\}^N\) with the pairwise maximum‑entropy (PMEM/Ising) distribution: \(P(\mathbf{s}) \propto \exp\Big(\sum_{i} h_i s_i + \sum_{i&lt;j} J_{ij} s_i s_j\Big), \qquad E(\mathbf{s}) = -\sum_{i} h_i s_i - \tfrac12 \sum_{i\ne j} J_{ij} s_i s_j\)</p> <p>PMEM matches the empirical first and second moments with minimal assumptions while remaining expressive for mesoscopic neural populations (as well as on the scale of entire networks of brain regions, and extending naturally to dynamical systems beyond neuroscience) <d-cite key="jaynes1957maxent,schneidman2006nature"></d-cite>.</p> <h3 id="inference-routes-complementary-scaleaware">Inference routes (complementary, scale‑aware):</h3> <ul> <li><strong>Exact (small \(N\))</strong>: Enumerate all \(2^N\) states to obtain the exact log‑likelihood and moments for gold‑standard checks</li> <li> <p><strong>Pseudo‑likelihood (PL)</strong>: Optimise the sum of node‑wise logistic conditionals with L2 penalties and a safeguarded Armijo line‑search; enforce symmetry \(J_{ij}=J_{ji}, J_{ii}=0\) and use a relative‑norm stopping rule (scale‑free, robust).</p> <p><strong>Local field:</strong></p> \[f_i^{(t)} = h_i + \sum_{j\neq i} J_{ij}s_j^{(t)} .\] <p><strong>Node-wise conditional and log-conditional:</strong></p> \[p\left(s_i^{(t)}\mid \mathbf s_{\setminus i}^{(t)}\right) = \frac{\exp\big(s_i^{(t)} f_i^{(t)}\big)}{2\cosh f_i^{(t)}}, \qquad \log p\left(s_i^{(t)}\mid \mathbf s_{\setminus i}^{(t)}\right) = s_i^{(t)} f_i^{(t)}-\log\big(2\cosh f_i^{(t)}\big).\] <p><strong>PL objective with L2 penalties (optimise over \((h)\) and the upper-triangle \(\{J_{ij}\}_{i&lt;j}\) ):</strong></p> \[\mathcal L_{\mathrm{PL}}(h,J) = \overline{ \sum_{i=1}^N \Big[s_i f_i - \log\big(2\cosh f_i\big)\Big] } - \frac{\lambda_h}{2}||h||_2^2 - \frac{\lambda_J}{2}\sum_{i&lt;j} J_{ij}^2 , \qquad J_{ij}=J_{ji}, \qquad J_{ii}=0 .\] </li> </ul> <hr/> <details><summary>Click to expand: Gradients</summary> \[\nabla_{h_i}\mathcal L_{\mathrm{PL}} = \overline{s_i - \tanh f_i,}-\lambda_h h_i\] \[\nabla_{J_{ij}}\mathcal L_{\mathrm{PL}} = \overline{2s_i s_j - s_j \tanh f_i - s_i \tanh f_j,} -\lambda_J J_{ij}, \qquad i&lt;j\] <p>(Bars denote averages over time; gradients include L2 terms)</p> </details> <hr/> <ul> <li> <p><strong>Variational Bayes (VB):</strong> Gaussian prior on \((h,J)\) with separate precisions for fields/couplings; a quadratic bound on the log‑partition yields closed‑form majorise–minimise updates:</p> \[\begin{aligned} \Sigma^{-1} &amp;= \Lambda_0 + T\,C_\eta,\\ \mu &amp;= \theta_0 + \Sigma\,T\big(\bar{\Phi} - m_\eta\big), \end{aligned}\] <p>where \(m_\eta = \mathbb{E}_{p_\eta}[\Phi], \qquad C_\eta = \operatorname{Cov}_{p_\eta}(\Phi),\) are the model moments/curvature evaluated at the current anchor \(\eta=\mu\).</p> <p>Credible intervals come from \(\Sigma\); optional Gamma hyper‑priors give ARD‑style shrinkage.</p> </li> </ul> <hr/> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20192129.png" alt="Variational Bayes quality report with posterior standard deviations, top z-scores, and coefficient-of-variation of couplings" style="width:100%;height:auto;border:1px solid rgba(255, 255, 255, 0.84); border-radius:8px;" w=""/> <figcaption style="color:#e5e7eb"> <strong>VB-Ising diagnostics.</strong> Iteration summary and posterior uncertainties for fields/couplings. Large \(|z|=\lvert\mu\rvert/\sigma\) edges are strongly supported; high coefficient-of-variation flags potentially unstable couplings. Use these readouts to prioritise robust interactions when interpreting circuit-level dependencies. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20234938.png" alt="Variational Bayes Ising diagnostics: uncertainty vs magnitude, posterior sd(J) maps, ELBO trajectory, field uncertainties, ARD precision spectrum, data–model probability agreement, CV histogram, fit-quality indices"/> <figcaption style="color:#f5f7ff; text-shadow:0 1px 2px rgba(255, 255, 255, 0.84);"> <strong>Variational-Bayes PMEM: uncertainty and fit checks.</strong> Panels summarise one VB run: (i) posterior uncertainty vs coupling magnitude; (ii, v) heatmaps of posterior s.d. for couplings (diagonal masked; log-scale variant); (iii) (negative) ELBO trajectory decreasing across iterations (convergence); (iv) histogram of field uncertainties; (vi) ARD precision spectrum indicating data-driven shrinkage; (vii) model-vs-empirical state probabilities (log–log; closer to the diagonal is better); (viii) histogram of coupling coefficient-of-variation \( \mathrm{CV}=\sigma/|\mu| \); (ix) two fit-quality indices (moment-matching accuracy). Together these quantify parameter credibility and goodness-of-fit before ELA/PDA. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20234218.png" alt="Energy–probability diagnostic: empirical probability vs shifted energy with basin colours, histograms and slope fit"/> <figcaption style="color:#f5f7ff; text-shadow:0 1px 2px rgba(255, 255, 255, 0.84);"> <strong>Energy–probability diagnostic.</strong> Empirical pattern probabilities \( P_{\mathrm{emp}}(\sigma) \) vs shifted energies \( E(\sigma)-E_{\min} \). An approximately linear trend in log-probability vs energy (dashed fit; slope and \( R^2 \) shown) is consistent with Boltzmann structure. Points are coloured by basin; the circled marker denotes the global minimum. Marginal histograms summarise energy and count distributions. Deviations at the extremes flag rare states and help identify outliers before landscape and phase-diagram analyses. </figcaption> </figure> <h3 id="fit-quality--sanity">Fit quality &amp; sanity</h3> <p>We report (i) moment matching (means and pairwise correlations), (ii) multi‑information explained and KL‑reduction vs. independence, and (iii) empirical‑vs‑Boltzmann pattern agreement.</p> <p>For Monte‑Carlo checks we use multi‑chain sampling<d-cite key="metropolis1953mcmc"></d-cite> with <strong>R̂</strong>/effective sample size (ESS) <d-cite key="vehtari2021rhat"></d-cite> diagnostics and estimate observables (magnetisation \(m\), Edwards–Anderson \(q\), spin‑glass and uniform susceptibilities \(\chi_{\mathrm{SG}},\chi_{\mathrm{Uni}}\), specific heat \(C\)).</p> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20192518.png" alt="Console log of pseudo-likelihood fit, correlation to empirical distribution, baseline vs PL error, BM-Metropolis sampling time" style="width:100%;height:auto;border:1px solid ;border-radius:8px;"/> <figcaption style="color:#f5f7ff"> <strong>Pseudo-likelihood fit and sampling check.</strong> The I2/IN ratio and correlation \(r\) quantify global match to empirical statistics; the PL error markedly improves the independent baseline. BM–Metropolis runtimes confirm that the fitted model is tractable for sampling-based validation and energy-landscape analyses. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20192606.png" alt="Console showing null-based QC thresholds and a table of m1_error, C_error, passed_QC for each mouse" style="width:100%;height:auto;border:1px solid rgba(255, 255, 255, 0.84); border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Null-based quality control.</strong> Thresholds for first-moment error \(m_1\) and configuration-error \(C\) are estimated from nulls; all shown mice pass. This guards against over-fitting and ensures that downstream landscape metrics (barriers, relaxation) rest on statistically credible fits. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20193023.png" alt="Console listing bootstrap accuracy estimates with 95% confidence intervals for each mouse/run" style="width:100%;height:auto;border:1px solid rgba(255, 255, 255, 0.88); border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Bootstrap accuracy with 95% CIs.</strong> For each mouse we report resampled accuracies and confidence intervals, quantifying the stability of the fitted model against sampling noise — a practical measure of reliability for comparative neuroscience. </figcaption> </figure> <h3 id="implementation-highlights">Implementation highlights</h3> <p>PL uses mean‑field initialisation, symmetric updates, Armijo backtracking and a relative gradient test; VB stores posterior precisions and ELBO traces for convergence auditing. (See robustness notes in Section Robustness, uncertainty and diagnostics.)</p> <hr/> <h2 id="5-energy-landscape-analysis-ela-descriptors-and-kinetics">5) Energy-Landscape Analysis (ELA): descriptors and kinetics</h2> <p>Once \((h,J)\) are fitted, the Ising energy</p> \[E(\mathbf{s})=-\sum_i h_i s_i-\tfrac{1}{2}\sum_{i\neq j}J_{ij} s_i s_j\] <p>induces a rugged energy landscape over \(\{-1,+1\}^N\).</p> <p>We compute:</p> <ul> <li><strong>Attractors and basins:</strong> Local minima are states whose single‑spin flips all increase energy. Every state is assigned to a basin by steepest‑descent (or best‑improving) paths. We summarise basins by occupancies and a disconnectivity graph <d-cite key="becker1997disconnectivity,wales2006jpcb"></d-cite>.</li> <li><strong>Barriers and disconnectivity:</strong> The barrier between basins \((\alpha,\alpha')\) is the minimum, over all paths connecting them, of the maximum energy along the path; the disconnectivity graph visualises these heights. Denoting the (symmetrised) minimal saddle by \(\overline{E}_{\alpha\alpha'}\),</li> </ul> \[\overline{E}_{\alpha\alpha'} = \min_{\gamma: \alpha\to\alpha'}\ \max_{\mathbf{s}\in\gamma} E(\mathbf{s}).\] <ul> <li>We also estimate a depth threshold (null‑model percentile) to guard against spurious minima.</li> </ul> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20193305.png" alt="Histogram and KDE of pairwise basin barrier heights with median shown" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Distribution of pairwise barrier heights.</strong> Histogram of inferred barrier heights \(\overline{E}_{\alpha,\alpha'}\) between metastable basins in the fitted landscape; the dashed line marks the sample median. Mechanistically, more negative/lower barriers indicate easier inter-basin switches, whereas rarer higher barriers point to protected transitions. The spread quantifies heterogeneity of switching difficulty across the mouse’s brain-state repertoire. </figcaption> </figure> <p><strong>Kinetics (Markov view):</strong> Build a single-spin-flip Metropolis chain with proposal “flip one spin uniformly” <d-cite key="metropolis1953mcmc"></d-cite> and transition probability:</p> \[p(\mathbf{s}\to\mathbf{s}^{(i)}) = \frac{1}{N}\min\{1,\ \exp\big(E(\mathbf{s})-E(\mathbf{s}^{(i)})\big)\},\] <p>where \(\mathbf{s}^{(i)}\) is \(\mathbf{s}\) with spin \(i\) flipped. This yields a \(2^N\times 2^N\) transition matrix \(P\) (or a restriction to the visited subgraph).</p> <p><strong>From \(P\) we derive:</strong></p> <ul> <li>Stationary distribution \(\pi\), dwell-time distributions, and basin occupancy.</li> <li>Mean first-passage times (MFPT): from a set \(A\) to \(B\) via the fundamental matrix \(Z=(I-Q)^{-1}\) of the transient block <d-cite key="kemeny1960finite"></d-cite>.</li> <li>Committors \(q_{AB}\) solving \((I-Q),q=b\), where \(b\) collects transitions into \(B\) <d-cite key="e2006tpt"></d-cite>.</li> <li>Relaxation spectrum (mixing time-scales): non-unit eigenvalues \(\lambda_i(P)\) with \(\tau_i=-1/\log\lvert\lambda_i\rvert\), and the Kemeny constant (mean mixing time) \(K=\sum_{i\ge 2}\frac{1}{1-\lambda_i}\).</li> </ul> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20191607.png" alt="Log-PDF of dwell times in steps" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Dwell-time distribution (empirical).</strong> The heavy right tail on a log-PDF axis is consistent with near-geometric escape from basins. Longer dwells reflect stabilised neural configurations (metastability), while short dwells reflect rapid exploration; the slope encodes an effective leaving rate. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20191630.png" alt="Stem plot of slow relaxation times versus mode index" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Slow relaxation spectrum.</strong> Relaxation times \(tau_i\) (from the fitted Markov operator) quantify how quickly perturbations along each mode decay. A dominant \(tau_1\) and a gap to subsequent modes signal slow inter-basin exchange and long memory in the dynamics — a hallmark of metastable neural regimes. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20191557.png" alt="Histogram of empirical committor values from A to B" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Empirical committor \(q_i\) for \(A=[150]\to B=[1309]\).</strong> The committor is the forward-commitment probability that a state first hits \(B\) before \(A\). Mass near \(q_i\approx 0\) indicates most visited states lie closer to \(A\); states with \(q_i\approx 0.5\) are transition-like and highlight probable dynamical bottlenecks in the brain’s state graph. </figcaption> </figure> <p><strong>Read-outs:</strong> (i) attractor maps (patterns + labels), (ii) disconnectivity graphs, (iii) barrier distributions, (iv) transition/reachability matrices (one-step and multi-step), and (v) kinetic summaries (MFPT heatmaps, committor fields, relaxation spectra, Kemeny constants). These quantify stability, switching propensity, and heterogeneity of access between states.</p> <p>Crucially, these mechanistic and interpretable descriptors and metrics provide an additional high-level framework for comparing brain dynamics across different individuals, or even cohorts with systematically divergent patterns of neural activity - a discrete and more intuitive alternative to classic means for unifying/juxtaposing representations in computational systems.</p> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-22%20225317.png" alt="Energy Landscape Analysis (ELA) panel with attractor patterns, 3D energy surface with basins and paths, transition matrices, disconnectivity graph, basin visit counts and basin sizes"/> <figcaption style="color:#f5f7ff; text-shadow:0 1px 2px rgba(0,0,0,.55);"> <strong>Energy-Landscape Analysis (ELA) — summary descriptors.</strong> The composite figure illustrates the standard read-outs used downstream of the fitted Ising model: <br/> <em>(A)</em> <strong>Local-minimum patterns</strong> (binary states for each attractor); <em>(B)</em> <strong>3-D energy surface</strong> with labelled minima (white dots) and most-probable transition paths (white arrows); <em>(C)</em> <strong>Direct transition counts</strong> between minima (Metropolis single-flip kernel); <em>(D)</em> <strong>Disconnectivity graph</strong> showing barrier heights that separate basins; <em>(E)</em> <strong>Basin visit frequencies</strong> (empirical occupancy); <em>(F)</em> <strong>Basin sizes</strong> (number of micro-states per basin in state-space); <em>(G)</em> <strong>Direct/indirect transition counts</strong> summarising multi-step reachability. Deeper basins and higher barriers indicate more stable, harder-to-leave states; denser transition lanes point to preferred switching routes. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-11%20161144.png" alt="Basin graph: mosaics of microstates grouped by attractor label with an energy colour bar"/> <figcaption style="color:#eaeaea;"> <strong>Basin graph (alternative A: mosaics).</strong> Each panel corresponds to one attractor (State 1–18). Circles denote individual binary microstates assigned to that basin; circle colour encodes Ising energy (cooler = lower). This compact view shows how densely each basin occupies nearby configurations and highlights within-basin heterogeneity (broader colour spread ⇒ greater internal energy variance). </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-11%20161034.png" alt="Directed neighbourhood graphs for selected basins with node colour=energy and arrowed transitions"/> <figcaption style="color:#eaeaea;"> <strong>Basin graph (alternative B: directed neighbourhoods).</strong> For selected attractors (States 5–8), nodes are microstates (colour = energy) and arrows indicate admissible single-spin-flip moves under the Metropolis kernel. The layered, fan-shaped structure reflects typical downhill funnels into each attractor; sparse cross-links indicate rarer exits via saddles. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-11%20160912.png" alt="3D energy surface with numbered minima and white transition skeleton overlaid; colour bar shows energy"/> <figcaption style="color:#eaeaea;"> <strong>3-D energy landscape.</strong> A continuous rendering of the fitted Ising energy with numbered minima (basins) and a white transition skeleton connecting them through low-saddle routes. Valleys (cool colours) are deep, stable basins; ridges quantify barrier heights that regulate switching. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-11%20160656.png" alt="2D contour map of energy with the same transition skeleton between minima; colour bar shows energy levels"/> <figcaption style="color:#eaeaea;"> <strong>2-D energy landscape.</strong> The same landscape as a contour map. This top-down view makes it easy to read relative heights along paths and to spot alternative routes between basins (branch points near saddles). Together with the 3-D view, it provides complementary intuition about basin depth (3-D) and path geometry (2-D). </figcaption> </figure> <figure class="l-page" style="margin:1.2rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20234516.png" alt="Mean first-passage time (MFPT) matrix across all discrete states, sorted by basin index" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Mean first-passage times (MFPT).</strong> Heatmap of expected steps to reach each <em>target</em> state from each <em>start</em> state (both sorted by basin index). The bright diagonal reflects near-zero self-passage; block structure and asymmetric bands reveal easy vs hard cross-basin routes. White gridlines mark basin boundaries; the colour bar is in steps. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20191024.png" alt="Per-basin dwell-time distributions (violins with overlaid points and summary markers)" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Dwell-time distributions per basin.</strong> For each attractor basin, the violin shows the full distribution of residence times (frames) from the single-spin-flip dynamics; points display individual visits. Wider violins and higher medians indicate kinetically stable basins; narrow shapes near 1–2 frames indicate transient basins. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20191326.png" alt="Time-stripe raster showing the sequence of visited basins over the recording" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Basin-visit raster over time.</strong> Colour-coded stripe plot of the visited basin label across the recording. Long same-colour blocks correspond to sustained dwell periods; frequent colour changes indicate rapid switching. This readout complements MFPT and dwell-time summaries by exposing the temporal ordering of visits. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20191723.png" alt="Console summary with accuracies, stationary entropy, relaxation times, spectral gap, Kemeny constant" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Model-fit quality and global kinetics.</strong> Reported are fit accuracies, stationary entropy \(H(\pi)\) (spread of state use), slow \(\tau_i\), spectral gap \(\lambda_1-\lambda_2\) (mixing speed), and the Kemeny constant (mean hitting time averaged over targets). Together they summarise how well the model matches the data and how swiftly the brain’s state dynamics explore the landscape. </figcaption> </figure> <figure class="l-page" style="margin:1.6rem 0"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-11-01%20191957.png" alt="Boltzmann rank plot of empirical probabilities by energy rank with basin colouring and fitted slope" style="width:100%;height:auto;border:1px solid rgba(255,255,255,0.15);border-radius:8px;"/> <figcaption style="color:#e5e7eb"> <strong>Boltzmann rank plot (basin-coloured).</strong> Empirical probabilities versus energy rank (log-scale) assess Boltzmann-like ordering: high-probability states sit among the lowest-energy configurations. Basin colours reveal which attractors dominate the high-probability tail; the dashed fit summarises the overall decay. </figcaption> </figure> <hr/> <h2 id="6-phase-diagram-analysis-pda-multi-observable-placement">6) Phase-Diagram Analysis (PDA): multi-observable placement</h2> <p><strong>Goal:</strong> Place every subject on a <em>shared</em> Sherrington–Kirkpatrick‑like \((\mu,\sigma)\) phase surface using <em>multiple</em> observables at once, with uncertainty, so that cohorts become directly comparable without needing a fixed “healthy baseline”. PDA sits downstream of our shared‑latent → binarisation → Ising (PMEM) fit, and is designed to be robust, auditable, and reproducible from end to end. <d-cite key="edwards1975ea,sherrington1975sk,ezaki2020critical"></d-cite></p> <figure class="l-page"> <iframe src="/blog/assets/plotly/2025-10-25-phase-diagram-playbook/3D-PDA.html" title="Interactive 3D surface for an example phase diagram metric with cohort-aware placements" loading="lazy" style="width:100%; aspect-ratio: 10 / 10; border:0;" allowfullscreen=""></iframe> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Interactive 3D phase diagram with robust multi-subject positioning - illustrated for C as the example metric </strong> <br/><br/> <noscript> <a href="/blog/assets/plotly/2025-10-25-phase-diagram-playbook/3D-PDA.html"> Open the interactive figure. </a> </noscript> </figcaption> </figure> <hr/> <h3 id="61--what-pda-estimates-observables-and-phase-coordinates">6.1 What PDA estimates (observables and phase coordinates)</h3> <p>For a fitted Ising model on binary latents \(\mathbf{s}\in\{-1,+1\}^N\) with fields \(h_i\) and couplings \(J_{ij}\), PDA uses macroscopic observables:</p> <ul> <li> <p><strong>Magnetisation</strong><br/> \(m=\frac{1}{N}\sum_i \langle s_i\rangle\)</p> </li> <li> <p><strong>Edwards–Anderson order</strong><br/> \(q=\frac{1}{N}\sum_i \langle s_i\rangle^2\)</p> </li> <li> <p><strong>Spin‑glass susceptibility</strong> (using the eigenvalues \(\{\lambda_k\}\) of the spin covariance)<br/> \(\chi_{\mathrm{SG}}=\frac{1}{N}\sum_{k=1}^{N}\lambda_k^2\)</p> </li> <li> <p><strong>Uniform susceptibility</strong><br/> \(\chi_{\mathrm{Uni}}=\frac{1}{N}\sum_{i\neq j}\mathrm{Cov}(s_i,s_j)\)</p> </li> <li> <p><strong>Specific heat</strong> (from energy variance)<br/> \(C=\frac{\langle E^2\rangle-\langle E\rangle^2}{N},\qquad E(\mathbf{s})=-\sum_i h_i s_i-\frac{1}{2}\sum_{i\neq j}J_{ij}s_is_j\)</p> </li> </ul> <p>In practice, we compute these from Monte‑Carlo samples of the fitted model (with automatic convergence checks). For quick diagnostics, the first four are also obtainable directly from the binarised data.</p> <hr/> <h3 id="62--the-reference-phase-surface-musigmamapstomqchi_mathrmsgchi_mathrmunic">6.2 The reference phase surface \((\mu,\sigma)\mapsto\{m,q,\chi_{\mathrm{SG}},\chi_{\mathrm{Uni}},C\}\)</h3> <p>We construct a <em>reference</em> coupling matrix \(J_{\mathrm{ref}}\) (either <em>pooled</em> across the cohort or <em>control</em>‑only), then generate a dense grid over target \((\mu,\sigma)\) by <em>affinely transforming the off‑diagonal entries</em> of \(J_{\mathrm{ref}}\). Let \(\mu_{\text{old}}\) and \(\sigma_{\text{old}}\) be the mean and std of off‑diagonal entries of \(J_{\mathrm{ref}}\). For a target \((\mu,\sigma)\):</p> \[J_{ij}^{(\mu,\sigma)}= \begin{cases} \big(J_{ij}-\mu_{\text{old}}\big)\dfrac{\sigma}{\sigma_{\text{old}}+\varepsilon}+\mu, &amp; i\neq j,\\[6pt] 0, &amp; i=j~, \end{cases}\] <p>with all <strong>fields zeroed</strong> \(h_i\equiv 0\) (diagonal remains 0) to recover the canonical spin‑glass phase structure. For each grid point we Monte‑Carlo sample the observables above and cache five surfaces \(\{\mathrm{m},\mathrm{q},\chi_{\mathrm{SG}},\chi_{\mathrm{Uni}},C\}\).</p> <p><strong>Working vs display grids:</strong> We build a high‑resolution <em>working</em> grid used for optimisation/placement and optionally a wider <em>display</em> grid for visuals. We automatically pick \((\mu,\sigma)\) limits by running quick PL fits per subject to estimate native \((\hat\mu,\hat\sigma)\), then expand by a safety factor; we avoid overly coarse grids (e.g., if \(\Delta\mu\) or \(\Delta\sigma&gt;0.01\)).</p> <p><strong>MC convergence safeguards:</strong> We run multiple chains with increasing sweep budgets until <em>all</em> observables reach \(\hat R&lt;1.05\) and an effective sample size threshold, or a maximum sweep cap is hit (in which case a warning is issued).</p> <hr/> <h3 id="63--multiobservable-placement-cost-projection-with-balanced-weights">6.3 Multi‑observable placement (cost projection with balanced weights)</h3> <p>Given a subject’s <em>observed</em> \((m_o,q_o,\chi^{o}_{\mathrm{SG}},\chi^{o}_{\mathrm{Uni}})\) from their binarised latents, we <strong>project</strong> onto the reference surface by minimising a <em>variance‑balanced</em> squared error:</p> \[\underset{\mu,\sigma}{\arg\min}\;\sum_{k\in\{\mathrm{m},\mathrm{q},\chi_{\mathrm{SG}},\chi_{\mathrm{Uni}}\}} w_k\big(O_k^{\text{obs}}-\widehat{O}_k(\mu,\sigma)\big)^2,\] <p>where \(\widehat{O}_k(\mu,\sigma)\) is obtained by regular‑grid interpolation of the precomputed surfaces and weights \(w_k\) default to the inverse <em>range</em> of each surface (less sensitive than \(1/\mathrm{var}\)). Optimisation uses L‑BFGS‑B on the working grid bounds; we also provide an <strong>iso‑curve</strong> fallback (intersect level sets of \(\chi_{\mathrm{SG}}\) and \(\chi_{\mathrm{Uni}}\) with a simple border‑safety check) and a brute‑force grid fallback.</p> <p>We return \((\hat\mu,\hat\sigma)\), the final cost value, and the method used (“cost_minimisation”, “iso_curves”, or “fallback_grid”).</p> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20235654.png" alt="3-D surface of specific heat C over sigma and mu with subject markers and a dashed ridge"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Specific-heat landscape \(C(\sigma,\mu)\) with cohort placements.</strong> The dashed ridge marks a near-critical band where responsiveness inflates. Subjects cluster on the gentler slope below the ridge; <em>the control mouse consistently occupies the lowest-\(\sigma\)</em> (least heterogeneous couplings), as expected. This panel anchors the multi-observable placement on a shared surface. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-10-29%20134043.png" alt="3-D observable surface with two 2-D panels for q and chiSG over sigma and mu; subject points overlaid"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Cross-checks across observables.</strong> A complementary 3-D view (top) with 2-D slices for <em>q</em> (left) and <em>\(\chi_{\mathrm{SG}}\)</em> (right). Consistent subject ordering across panels indicates that placements are not driven by a single metric. The control remains the lowest-\(\sigma\) point on the pooled reference. </figcaption> </figure> <hr/> <h3 id="64--uncertainty-robustness-and-diagnostics">6.4 Uncertainty, robustness, and diagnostics</h3> <p><strong>Bootstrap CIs:</strong> For each subject we run a <em>circular block bootstrap</em> along time, refit the Ising per resample, and recompute \((\mu,\sigma)\) or \((m,q,\chi_{\mathrm{SG}},\chi_{\mathrm{Uni}})\) as needed to report 95% intervals. Specific heat \(C\) can be bootstrapped likewise via short MC runs per resample.</p> <p><strong>Control shrinkage (optional):</strong> When a control is available, we stabilise its estimate by bootstrapping its \(J\), pooling across the cohort, and forming a convex combination \(J^{\text{ctrl,shrunk}}=(1-\lambda)J^{\text{ctrl}}+\lambda J^{\text{pooled}}\); we then summarise \((\mu_c,\sigma_c)\) from the shrunk \(J\).</p> <p><strong>Cost bowls:</strong> Around each optimum we display the <em>local cost landscape</em> (2‑D filled contour and 3‑D surface) to judge identifiability; narrow, well‑curved bowls suggest precise placement.</p> <p><strong>Group‑level tests:</strong> Small helpers allow groupwise comparisons in \(\sigma\) (e.g., Welch ANOVA and permutation tests) using the bootstrapped distributions.</p> <p><strong>Critical structure:</strong> We plot <em>critical contours</em> (e.g., a fixed fraction of the maximum of an observable) on the display surfaces; a simple near‑criticality index is the minimal Euclidean distance from \((\hat\mu,\hat\sigma)\) to the chosen contour.</p> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20235914.png" alt="Bootstrap means with 95% ellipses on the sigma–mu plane for each mouse"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>\(\mu\)–\(\sigma\) placements with uncertainty.</strong> Dots show bootstrap means; ellipses are 95% confidence regions from circular block-bootstrap resamples. Groupings separate along \(\sigma\) (heterogeneity), and the control consistently shows the smallest \(\sigma\) on the pooled reference — a sanity check aligned with biological expectations. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20235737.png" alt="2-D filled contour of the PDA objective around the optimum with the optimum marked"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Local 2-D cost surface (“identifiability map”).</strong> Filled contours of the variance-balanced multi-observable discrepancy around the optimum in \((\sigma,\mu)\). A tight, symmetric bowl indicates precise, well-conditioned placement; mild elongation reflects correlated trade-offs between observables. </figcaption> </figure> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20235742.png" alt="3-D surface view of the PDA objective around the optimum forming a convex bowl"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>3-D “cost bowl”.</strong> The same objective shown as a 3-D surface. Clear curvature corroborates convergence and local identifiability. (Together with the 2-D map above, this rules out flat minima or boundary locking.) </figcaption> </figure> <hr/> <h3 id="65--practical-recipe-what-the-code-actually-does">6.5 Practical recipe (what the code actually does)</h3> <p>1) <strong>Prepare data</strong>: shared latents → binarise per latent to \(\pm1\).<br/> 2) <strong>Rough bounds</strong>: quick PL fits to get subject‑wise \((\hat\mu,\hat\sigma)\); expand to working ranges; verify resolution is fine enough.<br/> 3) <strong>Build reference surface</strong>: choose <code class="language-plaintext highlighter-rouge">reference_mode ∈ {pooled, control}</code>, set \(h\equiv 0\), sweep \((\mu,\sigma)\) by affine off‑diagonal transforms of \(J_{\mathrm{ref}}\), run multi‑chain MC with convergence checks, and cache \(\{\mathrm{m},\mathrm{q},\chi_{\mathrm{SG}},\chi_{\mathrm{Uni}},C\}\) surfaces.<br/> 4) <strong>Place subjects</strong>: minimise the balanced multi‑observable cost (or use the iso‑curve / grid fallbacks with border guard).<br/> 5) <strong>Uncertainty</strong>: bootstrap along time to report CIs; optionally shrink the control to summarise \((\mu_c,\sigma_c)\).<br/> 6) <strong>Visuals</strong>: 2‑D contour panels and interactive 3‑D surfaces, with group‑colours and critical lines; “cost bowls” per subject for identifiability.</p> <hr/> <h3 id="66--mathematical-and-implementation-notes-exact-to-this-workflow">6.6 Mathematical and implementation notes (exact to this workflow)</h3> <ul> <li><strong>Affine mapping of \(J\) to \((\mu,\sigma)\)</strong> modifies <em>only</em> off‑diagonals and preserves \(J_{ii}=0\). This avoids artefactual self‑coupling and keeps the spin‑glass structure intact. Fields \(h\) are explicitly zeroed for the surface.</li> <li><strong>Observables from data vs model.</strong> Fast “data‑side” \(m,q,\chi\) provide immediate checks, while MC‑side estimates (including \(C\)) are used to build the surface; both routes are available.</li> <li><strong>Balanced weights</strong> \(w_k\) default to inverse <em>range</em> (normalises sensitivity without over‑penalising heavy‑tailed metrics). Equal weights are also supported.</li> <li><strong>Convergence gating</strong> uses multi‑chain Gelman-Rubin \(\hat R\) and a conservative ESS check with geometric back‑off of sweeps up to a cap; we report if the cap is reached.</li> <li><strong>Border guard</strong> prevents pathological “corner locking” when intersecting \(\chi\)‑iso‑curves on coarse grids.</li> </ul> <hr/> <h3 id="67--interpretation-tips">6.7 Interpretation tips</h3> <h3 id="metric-glossary">Metric glossary:</h3> <p><strong>\(\sigma\) — network heterogeneity (dispersion of couplings)</strong><br/> <strong>Meaning:</strong> standard deviation of off‑diagonal \(J_{ij}\) on the reference surface.<br/> <strong>High:</strong> uneven, subnetwork‑biased interactions; rugged energy landscape with many competing basins; dynamics readily reconfigured by <em>local</em> nudges.<br/> <strong>Low:</strong> broadly uniform interactions; smoother coordination; fewer competing minima (less glassy).</p> <p><strong>\(\mu\) — net coupling / co‑activation balance (mean coupling)</strong><br/> <strong>Meaning:</strong> mean of off‑diagonal \(J_{ij}\) (with \(h\approx 0\) on the surface).<br/> <strong>High (positive):</strong> stronger global ordering/co‑activation; coherent whole‑system shifts are easy.<br/> <strong>Low/negative:</strong> interactions cancel or oppose; regions act more independently/segregate.</p> <p><strong>\(m\) — magnetisation (whole‑brain activation bias)</strong><br/> <strong>Definition:</strong> \(m=\tfrac{1}{N}\sum_i \langle s_i\rangle\).<br/> <strong>Large \(|m|\):</strong> prolonged hypo‑ or hyper‑activation (tonic bias; long runs in one sign).<br/> <strong>Near 0:</strong> balanced on/off usage.<br/> <strong>Note:</strong> on the reference surface \(h=0\), so any non‑zero \(m\) reflects ordering from \(\mu\) (or thresholding bias when computed directly from data).</p> <p><strong>\(q\) — Edwards–Anderson order (pattern rigidity)</strong><br/> <strong>Definition:</strong> \(q=\tfrac{1}{N}\sum_i \langle s_i\rangle^2\) (persistence per unit regardless of sign).<br/> <strong>High:</strong> recurring, “frozen” configurations; can be rigid even when \(m\approx 0\) (symmetry‑related states).<br/> <strong>Low:</strong> flexible/exploratory dynamics with weak per‑unit bias.</p> <p><strong>\(\chi_{\mathrm{SG}}\) — spin‑glass susceptibility (sensitivity to <em>local</em> perturbations)</strong><br/> <strong>Meaning:</strong> response to heterogeneous, small nudges; equals the sum of squared eigenvalues of the spin covariance (normalised by \(N\)).<br/> <strong>High:</strong> small local changes can reconfigure the network; hallmark of glassy, high‑\(\sigma\) regimes with many shallow basins.<br/> <strong>Low:</strong> locally robust; topology resists piecemeal perturbations.</p> <p><strong>\(\chi_{\mathrm{Uni}}\) — uniform susceptibility (sensitivity to a <em>global</em> nudge)</strong><br/> <strong>Meaning:</strong> response when all units are pushed equally (normalised sum of pairwise covariances).<br/> <strong>High:</strong> easy, coherent whole‑brain shift (often increases with \(\mu\), decreases with \(\sigma\)).<br/> <strong>Low:</strong> globally inertial/decoupled.</p> <p><strong>\(C\) — specific heat (breadth of accessible repertoire)</strong><br/> <strong>Definition:</strong> \(C=\big(\langle E^2\rangle-\langle E\rangle^2\big)/N\), i.e., energy variance per unit.<br/> <strong>High:</strong> wide repertoire; peaks near phase boundaries/critical bands (heightened responsiveness).<br/> <strong>Low:</strong> narrow variability; stereotyped dynamics.</p> <hr/> <h3 id="typical-regimes-reading-combinations">Typical regimes (reading combinations):</h3> <ul> <li><strong>Ferromagnetic‑like:</strong> \(\mu\) high, \(\sigma\) low → large \(\|m\|\), high \(q\), high \(\chi_{\mathrm{Uni}}\), low \(\chi_{\mathrm{SG}}\); \(C\) can peak near the ordering boundary.</li> <li><strong>Spin‑glass‑like:</strong> \(\mu\approx 0\), \(\sigma\) high → \(m\approx 0\) but \(q\) elevated, \(\chi_{\mathrm{SG}}\) high, \(\chi_{\mathrm{Uni}}\) low; many metastable basins and irregular switching.</li> <li><strong>Paramagnetic‑like:</strong> \(\mu\approx 0\), \(\sigma\) low → \(m\approx 0\), low \(q\), both susceptibilities low, \(C\) low; weak coordination, noise‑like exploration.</li> <li><strong>Near‑critical band:</strong> \(C\) high with elevated susceptibilities; large fluctuations and long correlation lengths.</li> </ul> <hr/> <h3 id="practical-notes">Practical notes:</h3> <ul> <li>PDA surfaces set \(h\approx 0\); placements therefore reflect coupling structure \(\big(\mu,\sigma\big)\) rather than tonic biases.</li> <li>Compare subjects on the <strong>same</strong> reference (pooled vs control can shift absolute positions). Use uncertainty ellipses/cost bowls to judge identifiability.</li> <li>Large \(\|m\|\) in data space can arise from binarisation thresholds or residual trends—validate preprocessing.</li> <li>PDA gives macroscopic coordinates; pair with ELA for basin‑level mechanisms (attractors, barriers, kinetics).</li> </ul> <hr/> <figure class="l-page"> <img src="/blog/assets/img/2025-10-25-phase-diagram-playbook/Screenshot%202025-09-09%20135822.png" alt="Five reference surfaces (m, q, chiSG, chiUni, C) over sigma and mu with subject placements"/> <figcaption style="color:var(--theme-text, #eaeaea)"> <strong>Multi-observable phase surfaces with subject placements.</strong> Each panel shows an observable over \( (\sigma,\mu) \): top row — magnetisation \( m \), Edwards–Anderson order \( q \), spin-glass susceptibility \( \chi_{\mathrm{SG}} \); bottom row — uniform susceptibility \( \chi_{\mathrm{Uni}} \) and specific heat \( C \). Points mark each subject’s placement on the pooled reference. The steep wing indicates a near-critical band; the control occupies the smallest \( \sigma \). </figcaption> </figure> <hr/> <h3 id="68--reproducibility-knobs-defaults">6.8 Reproducibility knobs (defaults)</h3> <ul> <li>MC: chains = 12, start sweeps = 8k, burn‑in = 1k, cap = 128k, \(\hat R\) tol = 1.05.</li> <li>Working grid: \(140\times140\) by default; guard: \(\max(\Delta\mu,\Delta\sigma)\le 0.01\).</li> <li>Objective: weights = “balanced” (inverse range), optimiser = L‑BFGS‑B within bounds.</li> <li>Bootstrap: circular blocks, user‑set size; control shrinkage parameter \(\lambda\in[0,1]\).</li> </ul> <hr/> <h3 id="69--illustrative-pseudocode-orientation-only--code-is-not-released-here">6.9 Illustrative pseudocode (orientation only — code is not released here)</h3> <blockquote> <p>The snippet below mirrors the sequence used to generate the figures; it is descriptive rather than an API.</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1) Build the reference phase surfaces (pooled or control)
</span><span class="n">phase</span> <span class="o">=</span> <span class="nf">build_phase_surfaces</span><span class="p">(</span>
    <span class="n">binaries_pm1</span> <span class="o">=</span> <span class="n">BINARIES_PM1_OR_01</span><span class="p">,</span>         <span class="c1"># dict: subject → DataFrame (±1)
</span>    <span class="n">reference</span>    <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">mode</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">pooled</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">subject</span><span class="sh">"</span><span class="p">:</span> <span class="bp">None</span><span class="p">},</span>  <span class="c1"># or {"mode": "control","subject": "ID"}
</span>    <span class="n">grid_size</span>    <span class="o">=</span> <span class="mi">150</span><span class="p">,</span>                         <span class="c1"># working grid resolution
</span>    <span class="n">mc_settings</span>  <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">chains</span><span class="sh">"</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span> <span class="sh">"</span><span class="s">burn_in</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span> <span class="sh">"</span><span class="s">start_sweeps</span><span class="sh">"</span><span class="p">:</span> <span class="mi">8000</span><span class="p">}</span>
<span class="p">)</span>

<span class="c1"># 2) Place subjects via multi-observable cost (balanced weights by default)
</span><span class="n">positions</span> <span class="o">=</span> <span class="nf">place_subjects_on_surface</span><span class="p">(</span>
    <span class="n">binaries_pm1</span> <span class="o">=</span> <span class="n">BINARIES_PM1_OR_01</span><span class="p">,</span>
    <span class="n">phase</span>        <span class="o">=</span> <span class="n">phase</span><span class="p">,</span>
    <span class="n">method</span>       <span class="o">=</span> <span class="sh">"</span><span class="s">cost</span><span class="sh">"</span><span class="p">,</span>              <span class="c1"># or "iso_curves"
</span>    <span class="n">weights_mode</span> <span class="o">=</span> <span class="sh">"</span><span class="s">balanced</span><span class="sh">"</span>
<span class="p">)</span>

<span class="c1"># 3) Inspect local identifiability (“cost bowl”) for one subject
</span><span class="n">w</span> <span class="o">=</span> <span class="nf">objective_weights</span><span class="p">(</span><span class="n">phase</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">balanced</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">plot_cost_landscape</span><span class="p">(</span>
    <span class="n">subject_id</span> <span class="o">=</span> <span class="sh">"</span><span class="s">SUBJECT_ID</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">positions</span>  <span class="o">=</span> <span class="n">positions</span><span class="p">,</span>
    <span class="n">phase</span>      <span class="o">=</span> <span class="n">phase</span><span class="p">,</span>
    <span class="n">window</span>     <span class="o">=</span> <span class="mf">0.06</span><span class="p">,</span>
    <span class="n">steps</span>      <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">weights</span>    <span class="o">=</span> <span class="n">w</span>
<span class="p">)</span>

</code></pre></div></div> <hr/> <h3 id="610--caveats-specific-to-pda">6.10 Caveats specific to PDA</h3> <ul> <li>PDA assumes an SK‑like parameterisation (coupling distribution matters); it analyses <strong>macrostates</strong> and does not replace ELA’s mechanistic, basin‑level descriptors.</li> <li>The choice of reference surface (pooled vs control) can shift placements; we therefore expose the cost bowls and allow both options to be reported.</li> <li>Grid resolution and MC budgets matter near sharp boundaries; guards and diagnostics make this explicit.</li> </ul> <hr/> <h2 id="results-at-a-glance">Results at a glance</h2> <p>On resting-state functional ultrasound (fUS) recordings (mesoscopic, whole-brain), we observe low placement residuals \((10^{-6}–10^{-4})\), tight bootstrap confidence regions, convergent models, and stable ordering under pooled vs subgroup phase references; example estimates span <strong>σ ≈ 0.15–0.32</strong> and <strong>μ ≈ −0.01 to +0.03</strong>. The outputs - susceptibility to perturbations, ordering vs glassiness, transition propensity - form compact, biologically meaningful fingerprints.</p> <p>For experimentalists, this is a mechanistic dashboard (what states exist, how deep, how likely to switch); for theorists, it anchors subjects in a physics-grounded phase space with interpretable axes.</p> <p>Overall, the multi-observable placement and the combination of shared embeddings with ELA/PDA provide reliability-minded, comparable, and interpretable read-outs that support discovery, phenotyping, and model-based hypothesis generation across cohorts, data modalities, tasks, and species.</p> <hr/> <h2 id="robustness-uncertainty-and-diagnostics">Robustness, uncertainty and diagnostics</h2> <ul> <li>Uncertainty via variational-Bayes posteriors for h and J; bootstrap intervals for mu and sigma and for near-criticality; block bootstrap for autocorrelation <d-cite key="politis1992cbb,politis2004blocklength"></d-cite></li> <li>Convergence checks including MCMC R̂ and effective sample size <d-cite key="vehtari2021rhat"></d-cite> where applicable, pseudo-likelihood relative-norm stopping, and ELBO improvements for variational Bayes</li> <li>Quality-control gates including permutation or null thresholds for spurious minima, component-stability filters, and sensitivity to number of latents and alignment choice</li> <li>Ablations: pooled versus subgroup reference surfaces; method toggles among SRM, MCCA, and ICA; median versus mean thresholds</li> </ul> <hr/> <h2 id="reproducibility-and-artefacts">Reproducibility and artefacts</h2> <p>We report key settings and diagnostics to make the computational provenance clear, even though the code is not released with this post. Runs are seed-controlled with machine-parsable configuration files; convergence (e.g., R̂, ESS) and grid resolution checks are documented in the figure captions and text.</p> <p><strong>Example configuration stub (indicative):</strong></p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># example-config.yaml</span>
<span class="na">seed</span><span class="pi">:</span> <span class="m">123</span>
<span class="na">preprocess</span><span class="pi">:</span>
  <span class="na">detrend</span><span class="pi">:</span> <span class="s">conservative</span>
  <span class="na">concat_runs</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">impute</span><span class="pi">:</span> <span class="s">short_gaps_only</span>
<span class="na">alignment</span><span class="pi">:</span>
  <span class="na">methods</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">SRM</span><span class="pi">,</span> <span class="nv">MCCA</span><span class="pi">,</span> <span class="nv">GroupPCA</span><span class="pi">,</span> <span class="nv">GroupICA</span><span class="pi">]</span>
  <span class="na">consensus</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">select_dim</span><span class="pi">:</span> <span class="s">auto</span>
<span class="na">binarise</span><span class="pi">:</span>
  <span class="na">threshold</span><span class="pi">:</span> <span class="s">median</span>
<span class="na">ising</span><span class="pi">:</span>
  <span class="na">mode</span><span class="pi">:</span> <span class="s">PL</span>            <span class="c1"># {EXACT|PL|VB}</span>
  <span class="na">l2_h</span><span class="pi">:</span> <span class="s">1e-5</span>
  <span class="na">l2_J</span><span class="pi">:</span> <span class="s">1e-4</span>
  <span class="na">pl_tol</span><span class="pi">:</span> <span class="s">1e-6</span>        <span class="c1"># relative-norm stopping</span>
  <span class="na">vb</span><span class="pi">:</span>
    <span class="na">prior_prec_h</span><span class="pi">:</span> <span class="m">6.0</span>
    <span class="na">prior_prec_J</span><span class="pi">:</span> <span class="m">30.0</span>
<span class="na">ela</span><span class="pi">:</span>
  <span class="na">minima_search</span><span class="pi">:</span> <span class="s">exhaustive</span>
  <span class="na">kinetics</span><span class="pi">:</span> <span class="kc">true</span>
<span class="na">pda</span><span class="pi">:</span>
  <span class="na">observables</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">m</span><span class="pi">,</span> <span class="nv">q</span><span class="pi">,</span> <span class="nv">chiSG</span><span class="pi">,</span> <span class="nv">chiUni</span><span class="pi">,</span> <span class="nv">C</span><span class="pi">]</span>
  <span class="na">bootstrap</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">ci_level</span><span class="pi">:</span> <span class="m">0.95</span>
<span class="na">reports</span><span class="pi">:</span>
  <span class="na">phase_report</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">landscape_report</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div> <hr/> <h2 id="limitations-and-scope">Limitations and scope</h2> <ul> <li>Binarisation coarsens signals, but enables interpretable PMEM fitting and stable cross-subject comparability</li> <li>Latent selection is influential; consensus and metric-guided selection mitigate but semantics remain partly model-dependent</li> <li>The choice of reference surface affects PDA; we quantify sensitivity and expose cost bowls for transparency</li> <li>Designed for resting or task neural time series; extension to other binarisable dynamical systems is often straightforward</li> </ul> <hr/> <h2 id="outlook">Outlook</h2> <p>Population-universal latents combined with physics-grounded descriptors provide a shared language for multi-subject brain dynamics that is portable across modalities, tasks, and species, and a bridge to mechanistic interpretation and clinical translation. Planned extensions include multi-modal fusion, alignment-aware causal probes, truly dynamic/directional extensions of the methodology (e.g., by incorporating Langevin-based methods and attractor networks), developing modular workflows for modelling the state-spaces of consecutive processing stages in the brain under cognitive tasks, and targeted clinical studies.</p> <hr/> <h2 id="appendix-mathematical-details">Appendix: Mathematical details</h2> <p>Below are the core mathematical underpinnings of the pseudo‑likelihood and variational‑Bayes alternatives to the exact‑likelihood pairwise maximum‑entropy model (PMEM / Ising), as well as of the relevant auxiliary methods (e.g., for VB, the ARD/shrinkage or convergence diagnostics).</p> <details><summary>Click to expand the entire Appendix</summary> <h2 id="notation-common-to-pl-and-vb">Notation (common to PL and VB)</h2> <ul> <li>Binary state \(\mathbf{s}\in\{-1,+1\}^N\) with samples \(\{\mathbf{s}^{(t)}\}_{t=1}^T\).</li> <li>Parameters \(\theta = \big[h_1,\ldots,h_N,\ \{J_{ij}\}_{i&lt;j}\big]^\top \in \mathbb{R}^{P}\), where \(P=N+N(N-1)/2\).</li> <li>Feature map (sufficient statistics) \(\Phi(\mathbf{s}) = \big[s_1,\ldots,s_N,\ \{s_i s_j\}_{i&lt;j}\big]^\top \in \mathbb{R}^{P}\).</li> <li>Ising / PMEM: \(p_\theta(\mathbf{s}) \propto \exp\!\big(\theta^\top \Phi(\mathbf{s})\big), \qquad A(\theta)=\log Z(\theta)=\log \sum_{\mathbf{s}}\exp\!\big(\theta^\top \Phi(\mathbf{s})\big).\)</li> <li>Empirical feature mean: \(\bar{\Phi}=\frac{1}{T}\sum_{t=1}^T \Phi\!\big(\mathbf{s}^{(t)}\big).\)</li> <li>Model moments at parameter \(\theta\): \(m_\theta=\mathbb{E}_{p_\theta}[\Phi], \qquad C_\theta=\operatorname{Cov}_{p_\theta}(\Phi)=\nabla^2 A(\theta).\) <em>(Here \(C_\theta\) is the Fisher information.)</em></li> </ul> <hr/> <h2 id="1-pseudolikelihood-pl-pmem">1) Pseudo‑likelihood (PL) PMEM</h2> <p><strong>Node‑wise conditional</strong> (logistic form). For \(f_i(\mathbf{s}_{\setminus i}) = h_i + \sum_{j\neq i} J_{ij}\, s_j,\) we have \(\log p\!\big(s_i \mid \mathbf{s}_{\setminus i}\big) = s_i f_i - \log\!\big(2\cosh f_i\big), \qquad p\!\big(s_i \mid \mathbf{s}_{\setminus i}\big)=\frac{\exp(s_i f_i)}{2\cosh f_i}.\)</p> <p><strong>Objective with L2</strong> (ridge on \(h\) and off‑diagonal \(J\)): \(\mathcal{L}_{\mathrm{PL}}(h,J) =\sum_{t=1}^T\sum_{i=1}^N \log p\!\big(s_i^{(t)}\mid \mathbf{s}_{\setminus i}^{(t)}\big) -\frac{\lambda_h}{2}\lVert h\rVert_2^2 -\frac{\lambda_J}{2}\lVert J\rVert_F^2,\) with \(J_{ii}=0,\quad J_{ij}=J_{ji}.\)</p> <p><strong>Gradients</strong> (for L‑BFGS/CG): \(\frac{\partial \mathcal{L}_{\mathrm{PL}}}{\partial h_i} =\sum_{t}\!\big[s_i^{(t)}-\tanh f_i^{(t)}\big]-\lambda_h h_i,\) \(\frac{\partial \mathcal{L}_{\mathrm{PL}}}{\partial J_{ij}} =\sum_{t}\!\big[s_i^{(t)}s_j^{(t)}-s_j^{(t)}\tanh f_i^{(t)}-s_i^{(t)}\tanh f_j^{(t)}\big]-\lambda_J J_{ij}.\)</p> <p><strong>Practical symmetry step.</strong> Fit \(N\) independent logistic regressions, then <strong>symmetrise</strong> \(J\): \(J_{ij}\leftarrow \tfrac{1}{2}\big(\hat\beta^{(i)}_j+\hat\beta^{(j)}_i\big),\qquad J_{ii}=0.\)</p> <hr/> <h2 id="2-variational-bayes-vb-pmem--gaussian-posterior-over-theta">2) Variational Bayes (VB) PMEM — Gaussian posterior over \(\theta\)</h2> <h3 id="a-prior-and-variational-family">(a) Prior and variational family</h3> <p>Stack \(\theta=[h;\, J_{i&lt;j}]\). Use zero‑mean Gaussian prior with separate precisions for fields and couplings: \(p(\theta)=\mathcal{N}\!\big(\theta\mid \theta_0,\ \Lambda_0^{-1}\big),\qquad \theta_0=\mathbf{0},\qquad \Lambda_0=\operatorname{diag}\!\big(\tau_h \mathbf{I}_N,\ \tau_J \mathbf{I}_{P-N}\big).\) Variational posterior: \(q(\theta)=\mathcal{N}\!\big(\theta\mid \mu,\ \Sigma\big).\)</p> <h3 id="b-quadratic-bound-on-the-logpartition">(b) Quadratic bound on the log‑partition</h3> <p>By convexity of \(A(\theta)\), for any anchor point \(\eta\) (majorisation parameter), \(A(\theta)\ \le\ A(\eta) + m_\eta^\top(\theta-\eta) +\tfrac{1}{2}(\theta-\eta)^\top C_\eta (\theta-\eta).\) This turns the intractable \(\mathbb{E}_q[A(\theta)]\) into a tractable quadratic form.</p> <h3 id="c-elbo-under-the-quadratic-bound">(c) ELBO under the quadratic bound</h3> <p>Let \(\mathcal{F}(\mu,\Sigma\mid\eta)\) denote the bound on the ELBO: \(\begin{aligned} \mathcal{F}(\mu,\Sigma\mid\eta) &amp;= T\Big[\mu^\top \bar{\Phi} - A(\eta) - m_\eta^\top(\mu-\eta) -\tfrac{1}{2}\operatorname{tr}(C_\eta \Sigma) -\tfrac{1}{2}(\mu-\eta)^\top C_\eta (\mu-\eta)\Big] \\ &amp;\quad -\tfrac{1}{2}\Big[(\mu-\theta_0)^\top \Lambda_0 (\mu-\theta_0) + \operatorname{tr}(\Lambda_0\Sigma)\Big] +\tfrac{1}{2}\log\!\det(2\pi e\,\Sigma). \end{aligned}\)</p> <h3 id="d-stationarypoint-updates-closed-form">(d) Stationary‑point updates (closed form)</h3> <p>Maximising \(\mathcal{F}\) w.r.t. \(\mu,\Sigma\) with \(\eta\) fixed gives \(\boxed{\ \ \Sigma^{-1} = \Lambda_0 + T\, C_\eta\ \ } \qquad \boxed{\ \ \mu = \theta_0 + \Sigma\, T\big(\bar{\Phi} - m_\eta\big)\ \ }.\) A convenient step form (with \(\theta_0=\mathbf{0}\)) is \(\mu \leftarrow \eta + \Sigma\, T\big(\bar{\Phi}-m_\eta\big).\) Then set \(\eta \leftarrow \mu\) and repeat (majorise‑minimise) until convergence.</p> <h3 id="e-moments-needed-in-d">(e) Moments needed in (d)</h3> <ul> <li> \[m_\eta=\mathbb{E}_{p_\eta}[\Phi] = \big[\langle s_i\rangle_\eta,\ \langle s_i s_j\rangle_\eta\big].\] </li> <li>\(C_\eta=\operatorname{Cov}_{p_\eta}(\Phi)\) (Fisher matrix).</li> </ul> <p>They can be obtained <strong>exactly</strong> by state enumeration for small \(N\), or <strong>approximately</strong> for larger \(N\) via Monte Carlo: \(\hat m_\eta=\frac{1}{M}\sum_{m=1}^M \Phi(\mathbf{s}^{[m]}),\qquad \hat C_\eta=\frac{1}{M-1}\sum_{m=1}^M \big(\Phi(\mathbf{s}^{[m]})-\hat m_\eta\big)\big(\Phi(\mathbf{s}^{[m]})-\hat m_\eta\big)^\top,\) where \(\mathbf{s}^{[m]} \sim p_\eta\) (e.g., Gibbs/Metropolis).</p> <h3 id="f-optional-ard--shrinkage-on-precisions">(f) Optional ARD / shrinkage on precisions</h3> <p>With Gamma hyperpriors \(\tau_h\sim\operatorname{Gamma}(a_h,b_h)\) and \(\tau_J\sim\operatorname{Gamma}(a_J,b_J)\) (type‑II ML / evidence updates), \(\tau_h \leftarrow \frac{N/2 + a_h - 1}{\tfrac{1}{2}\big(\lVert \mu_h\rVert_2^2 + \operatorname{tr}\Sigma_{hh}\big) + b_h},\qquad \tau_J \leftarrow \frac{(P-N)/2 + a_J - 1}{\tfrac{1}{2}\big(\lVert \mu_J\rVert_2^2 + \operatorname{tr}\Sigma_{JJ}\big) + b_J}.\)</p> <h3 id="g-convergence--diagnostics">(g) Convergence &amp; diagnostics</h3> <ul> <li>Relative change: \(\max\!\Big(\tfrac{\lVert \mu^{(t)}-\mu^{(t-1)}\rVert}{\lVert \mu^{(t-1)}\rVert},\ \tfrac{\lVert \Sigma^{(t)}-\Sigma^{(t-1)}\rVert_F}{\lVert \Sigma^{(t-1)}\rVert_F}\Big) &lt; \varepsilon\).</li> <li>Monotone ascent of \(\mathcal{F}(\mu,\Sigma\mid \eta)\) (re‑compute with \(\eta=\mu\)).</li> <li>Posterior standard deviations from \(\Sigma\) provide credible intervals for \(h\) and \(J\).</li> </ul> <h3 id="h-what-each-core-formula-captures-vb-intuition">(h) What each core formula captures (VB intuition)</h3> <ul> <li>\(\bar{\Phi}\) — the <strong>empirical</strong> first/second moments of the data.</li> <li>\(m_\eta\) — the <strong>model</strong> moments at the current parameter anchor; mismatch \(\bar{\Phi}-m_\eta\) drives the mean update.</li> <li>\(C_\eta\) — the curvature (Fisher) of the log‑partition; it <strong>tempers</strong> the update and sets the posterior covariance.</li> <li>\(\Sigma^{-1} = \Lambda_0 + T C_\eta\) — precision adds <strong>prior precision</strong> and <strong>data precision</strong> (information additivity).</li> <li>\(\mu = \theta_0 + \Sigma T(\bar{\Phi}-m_\eta)\) — mean moves in the direction that reduces <strong>moment mismatch</strong>.</li> <li>Hyper‑precisions \(\tau_h, \tau_J\) — control shrinkage of fields vs couplings (can be learned).</li> </ul> <h3 id="i-minimal-algorithm-majoriseminimise-vb">(i) Minimal algorithm (majorise‑minimise VB)</h3> <ol> <li>Initialise \(\mu\) (e.g., PL‑MAP), choose \(\Lambda_0\) (or \(\tau_h,\tau_J\)).</li> <li>Repeat: <ul> <li>Compute \(m_\eta, C_\eta\) at \(\eta=\mu\) (exact/MCMC/mean‑field).</li> <li>Update \(\Sigma^{-1}=\Lambda_0+T C_\eta\).</li> <li>Update \(\mu=\theta_0+\Sigma T(\bar{\Phi}-m_\eta)\) (or step form above).</li> <li>Optionally update \(\tau_h,\tau_J\); check ELBO and relative change.</li> </ul> </li> <li>Output \(q(\theta)=\mathcal{N}(\mu,\Sigma)\) and credible intervals.</li> </ol> </details> <hr/>]]></content><author><name>Julian Kędys</name></author><category term="shared latent representations"/><category term="tutorial"/><category term="subject alignment"/><category term="neural state-spaces"/><category term="energy landscapes"/><category term="intersubject comparability"/><category term="state transitions"/><category term="interpretable descriptors"/><category term="brain dynamics"/><summary type="html"><![CDATA[Overview (TL;DR)]]></summary></entry><entry><title type="html">Do Sparse Autoencoders (SAEs) transfer across base and finetuned language models?</title><link href="https://unireps.org//blog/2025/do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/" rel="alternate" type="text/html" title="Do Sparse Autoencoders (SAEs) transfer across base and finetuned language models?"/><published>2025-02-06T00:00:00+00:00</published><updated>2025-02-06T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models</id><content type="html" xml:base="https://unireps.org//blog/2025/do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/"><![CDATA[<h1 id="tldr-executive-summary"><strong>TLDR</strong> (Executive Summary)</h1> <ul> <li>We explored <strong>whether Sparse Autoencoders (SAEs)</strong> can effectively transfer from base language models to their finetuned counterparts, focusing on two base models: <a href="https://huggingface.co/google/gemma-2b">Gemma-2b</a> <d-cite key="gemmateam2024gemmaopenmodelsbased"></d-cite> and <a href="https://huggingface.co/mistralai/Mistral-7B-v0.1">Mistral-7B-V0.1</a> <d-cite key="jiang2023mistral7b"></d-cite> (we tested finetuned versions for coding and mathematics respectively)</li> <li>In particular, we split our analysis into three steps: <ol> <li>We analysed the similarity (<strong>Cosine and Euclidian Distance</strong>) of the residual activations, which was <strong>highly correlated with the resulting transferability of the SAEs</strong> for the two models.</li> <li>We computed several performance metrics (L0 Loss, Reconstruction CE Loss, Variance Explained) of the base SAEs on the fine-tuned models. Almost all metrics agreed on a <strong>significant degradation of the SAE performance for the Gemma-2b</strong> model, and <strong>remained within a reasonable range for the Mistral-7B model</strong>, indicating a much better transferability.</li> <li>We took a further step by operationalizing the idea of transferability of SAE from base models to fine-tuned models by applying an <a href="https://transformer-circuits.pub/2023/monosemantic-features#phenomenology-universality">approach from Towards Monosemanticity</a><d-cite key="bricken2023monosemanticity"></d-cite> for studying feature universality through <strong>feature activation similarity</strong> and <strong>feature logit similarity</strong>. These similarity scores were mostly consistent with the results from the previous step, albeit with one caveat for the Gemma-2b model, suggesting that <strong>some SAE features may still transfer</strong> even if the overall SAE performance is poor for the finetuned model.</li> </ol> </li> <li>Overall, our results agree with <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">previous work that studied Instruct models</a><d-cite key="sae_finetuning"></d-cite>. That is, SAEs transferability seems to be model-dependent and sensitive to the finetuning process.</li> <li>We make our <a href="https://github.com/tommasomncttn/SAE-Transferability">code repository public</a> to facilitate future work in this direction.</li> </ul> <hr/> <h1 id="1-introduction-and-motivation">1. Introduction and motivation</h1> <h2 id="11-what-are-saes-and-why-do-we-care-about-them">1.1 What are SAEs and why do we care about them</h2> <p>We find ourselves in a world where we have machines that speak fluently dozens of languages, can do a wide variety of tasks like programming at a reasonable level, <strong>and we have no idea how they do it!</strong> This is a standard <strong>mechanistic interpretability</strong> (a.k.a. mech interp) pitch - a field that is trying to <strong>express neural networks’ behaviours as human-understandable algorithms</strong>, i.e. <strong>reverse engineer</strong> algorithms learned by a neural network (or a model, in short). The main motivation is that even though we know the exact form of computation being done by the model to transform the input (e.g. text prompt) to the output (e.g. text answer), we don’t know <em>why</em> this computation is doing what it’s doing, and this is a major concern from a standpoint of AI Safety. The model can perform the computation because it’s genuinely trained to perform the task well, or because it learned that doing the task well correlates with its other learned goals like gaining more power and resources. Without understanding the computation, we have no direct way of distinguishing between the two.</p> <p>The solution proposed by mechanistic interpretability is closely analogous to reverse engineering ordinary computer programs from their compiled binaries. In both cases, we have an intrinsically non-interpretable model of computation - a sequence of binary instructions performed on a string of 0s and 1s, and the (mathematical) function of the neural network’s architecture applied with its learned parameters (weights)<d-footnote>This is a pretty important analogy to understand and you can read more about it in [this Anthropic post](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)<d-cite key="Olah_2022"></d-cite> where it's explained better. </d-footnote>. Programmers know that a natural way to think about computer programs is mapping <strong><em>variables</em></strong> into other variables (or new states of existing variables), starting from some pre-initialized state. So, reverse engineering complied binaries boils down to (oversimplifying) identifying binary memory segments that correspond to variables, tracking how these segments change as the program is being executed, coming up with the explanations of the purpose of these variables, and ultimately arriving at the replication of the program source code - a sequence of human-understandable instructions.</p> <p>But what makes us think that the same is possible for neural networks, especially the ones as large as the current Large Language Models (LLMs)? In particular, why should we even expect that neural networks solve tasks similarly to humans, and thus adopt the same “variable-centered” model of computation? While the proof-of-existence for the first question appeared relatively early (see <a href="https://distill.pub/2020/circuits/zoom-in/">Circuits thread by Chris Olah et al.</a><d-cite key="olah2020zoom"></d-cite> for CNNs or a <a href="https://arxiv.org/abs/2301.05217">more recent work by Neel Nanda et al.</a><d-cite key="nanda2023progressmeasuresgrokkingmechanistic"></d-cite> for language models), the second question is a more general claim, and thus requires more general evidence. The first fundamental work that provided such evidence was the <a href="https://transformer-circuits.pub/2023/monosemantic-features">“Towards Monosemanticity” paper by Anthropic</a><d-cite key="bricken2023monosemanticity"></d-cite>, which introduced Sparse Autoencoders (SAEs) for interpreting the language models’ activations. The activations are any intermediate state of the models’ computation, such as residual stream, MLP layers etc. and can be seen as analogous to a program’s memory state. And just as the program’s memory state can be decomposed into variables, the <strong>main purpose of SAEs is to decompose models’ activations into features</strong>.</p> <p>A feature, in general, is a fuzzy term, and you can find some good attempts to define it <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=BQds7CQ8ytq2rolt7p0XQPbt">here</a><d-cite key="nanda_2022"></d-cite>. For this post we’ll use the analogy with variables and link it to a very general definition of a feature as “<em>a</em> <em>property of the input</em>”. The link is pretty natural: <strong>the types and the number of variables a programmer needs to solve a task depends on the task itself</strong> (i.e. on the problem input). So for a model it would seem reasonable if it used different kinds of variables/features depending on its input: you don’t need a feature “this line is inside a for-loop in Python” in a poetry text, or a feature “this word rhymes with ‘sunset’” in the Python code. And given that models have a finite amount of parameters (which limits a total number of variables they can use), we should expect that they will utilize this kind of input-specificity to use as many unique features as they need to perform a specific task.</p> <p>Why are sparse autoencoders called sparse? It’s actually deeply linked with the idea from the previous paragraph: if you want to use many features in a limited activation space (limited by a number of neurons), you have to exploit the fact that <strong>for any input, most of the features will not be there</strong>. So given that modern language models are trained to predict a next token in a huge variety of possible inputs, we should expect that any feature learned by the model will be <strong>sparse</strong>, i.e. it <strong>will be used by the model only for a small fraction of all possible inputs</strong>.</p> <p>But wait, how is it even possible for a model to learn input-specific features if it has a low-dimensional activations space (where dimension equals the number of neurons) but a very high-dimensional input space? The answer is <strong><em>superposition</em></strong> - an idea of exploiting feature sparsity to store more features than dimensions in the activation space. It has a rich mathematical background and we invite an unfamiliar reader to learn more about it in the <a href="https://transformer-circuits.pub/2022/toy_model/index.html">“Toy Models of Superposition” paper by Elhage et al.</a><d-cite key="elhage2022superposition"></d-cite></p> <p>Coming back to SAEs, they were introduced with all of these ideas in mind to <em>solve superposition</em>, i.e. to recover more than <em>n</em> features in an <em>n</em>-dimensional activation space of a model. How are they supposed to do it? The answer is once again in the name - <em>autoencoders</em>, which means that SAEs are neural networks with the “autoencoder” architecture, which is illustrated in a diagram below (borrowed from the great <a href="https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html">Adam Karvonen’s post</a><d-cite key="Karvonen_2024"></d-cite>):</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/sae.png" alt="My Image" width="500"/> </div> <p>So the model activations are “encoded” into a high-dimensional vector of feature activations (top right, note that it always has many more elements than the model’s input), and this high-dimensional vector (a.k.a. “code”) is “decoded” back to reconstruct the input, hence the name “autoencoder”. We advise the reader to take a quick look at the <a href="https://transformer-circuits.pub/2023/monosemantic-features#appendix-autoencoder">“Towards monosematicity” appendix</a><d-cite key="bricken2023monosemanticity"></d-cite> where this architecture is presented mathematically<d-footnote>Note that it's different from the diagram in two ways: adding biases vectors **b** and using a transposed encoder/decoder matrix compared to what is seen in the diagram.</d-footnote>, but the core point to understand is that we’re interested in the right part of the above diagram: <strong>how the reconstructed activations are decomposed into a linear combination of feature vectors</strong> from the Decoder matrix (with the weights of a linear combination equal to SAE <em>feature activations</em>, due to how matrix-vector multiplication works). Mathematically, it means that for each input \(x^j\) (which is the model’s activation vector at the place where we ‘attach’ the SAE - residual layer, hidden head activations etc.), we’re looking to express it in the following form:</p> \[\mathbf{x}^j \approx \mathbf{b} + \sum_i f_i(\mathbf{x}^j) \mathbf{d}_i\] <p>where \(f_i(\mathbf{x}) = \text{ReLU}\left( \mathbf{W}_{enc} \mathbf{x} + \mathbf{b}_{enc} \right)_i\) are the feature activations that are computed in the left (“encoder”) part of the diagram, and \(\mathbf{d}_i\) are the rows of the decoder matrix (or columns, if you take the transpose and multiply from the other side). Note that the diagram omits bias vectors \(\mathbf{b}\) for simplicity, but conceptually they don’t change much: instead of decomposing the activation space, we’re decomposing a translation of that space by a fixed vector (because this is just easier for an SAE to learn).</p> <p>If you think about it, it’s exactly what we hoped to do in an analogy with decomposing program memory into variable names! The variables are now features - <strong>vectors (directions) in the activation space</strong>. And <em>if</em> the autoencoder is doing a good job at reconstructing the input, we can expect that this decomposition (and hence the features) to make sense!</p> <p>The last part is tricky though. Unlike variables that are deliberately used by humans to write sensible algorithms, there is no reason to expect that the features we recover with an SAE will be <em>interpretable</em> in a sense that a human can understand on which inputs they activate and can predict their “roles” based on that (e.g. which tokens they help to predict). But this is where the <em>sparsity</em> condition comes in: we don’t only want an SAE to reconstruct the input from a high-dimensional feature-activation representation, <strong>but we also want this representation to be sparse</strong>, i.e. have only a handful of non-zero feature activations at a time. We already touched on the reason for this - the hope is that we’ll be able to recover the “true” features used by the model in this way<d-footnote>It's quite a slippery area to consider the logical relationship between the feature quality of being "truly used" by the model (analogously to correctly recovered variables from the compiled binary) and its interpretability. If the model came up with some genius way to solve a particular task using features no human can comprehend, would they still be considered as interpretable? The answer can vary from "no" to "kind of yes", because it can be argued that humans with their evolutionally developed problem-solving skills can eventually understand (i.e. interpret) how things work, even though it may not be obvious at a first glance. It's also discussed by Neel Nanda [here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=dzkF4Sh89hg1GUJj5h2TiGVx)<d-cite key="nanda_2022"></d-cite> </d-footnote>. And the way this is achieved is by imposing an L1-loss penalty on the feature activation vector, which intuitively incentivizes the model to not learn any features unless they are really useful in reconstructing the input<d-footnote>There's also a better justified mathematical reason for sparsity, greatly explained [here](http://ufldl.stanford.edu/tutorial/unsupervised/SparseCoding/)<d-cite key="Ng"></d-cite>. Essentially, by learning to decompose the model's activation space into feature activations, we're trying to find an overcomplete basis of feature directions (a basis with more than n vectors in an n-dimensional space), which is impossible to do without imposing some additional criteria. The ["Toy Models of Superposition"](https://transformer-circuits.pub/2022/toy_model/index.html)<d-cite key="elhage2022superposition"></d-cite> is also incredibly helpful to refine one's intuition about this. </d-footnote>.</p> <h3 id="111-sae-features-for-ai-safety">1.1.1 SAE features for AI Safety</h3> <p>The traditional view in mech interp has been that <strong>one cannot interpret the model’s weights if one cannot interpret the neurons that the weights are connecting</strong>. But due to the <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=RDddls6iedarJZiVvLWwnaYI">neurons polysemanticity</a><d-cite key="nanda_2022"></d-cite> (a consequence of superposition), interpreting individual neurons in the language model is extremely hard if at all possible. That’s where SAEs come to the rescue: by revealing the directions in the neuron activation space (i.e. features) that have a clear, interpretable meaning, they allow for a new form of <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=GeeSfnALcakOYfxQcKaAwV6x">circuits</a><d-cite key="nanda_2022"></d-cite> analysis: instead of interpreting weights between neurons, we can instead interpret weights connecting features. Thus the SAE features potentially serve as a new “basis” for circuit analysis, and some of the recent work e.g. by <a href="https://arxiv.org/abs/2403.19647">Marks et al.</a><d-cite key="marks2024sparsefeaturecircuitsdiscovering"></d-cite> and <a href="https://transformer-circuits.pub/2024/march-update/index.html#feature-heads">Batson et al.</a><d-cite key="Batson_Chen_Jones_2024"></d-cite> has already started exploring this idea and producing the first results.</p> <p>So what does this mean for AI Safety? We’ll cite the Anthropic team’s view on this topic (layed out in their <a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html#safety">“Interpretability Dreams”</a><d-cite key="Olah_2023"></d-cite> post and in the <a href="https://transformer-circuits.pub/2022/toy_model/index.html#strategic">“Strategic Picture” section</a><d-cite key="elhage2022superposition"></d-cite> of the Toy Models paper):</p> <blockquote> <p>We’d like a way to have confidence that models will never do certain behaviors such as “deliberately deceive” or “manipulate.” Today, it’s unclear how one might show this, but we believe a promising tool would be the ability to identify and enumerate over all features.</p> </blockquote> <blockquote> <p>Ultimately we want to say that a model doesn’t implement some class of behaviors. Enumerating over all features makes it easy to say a feature doesn’t exist (e.g. “there is no ‘deceptive behavior’ feature”) but that isn’t quite what we want. We expect models that need to represent the world to represent unsavory behaviors. But it may be possible to build more subtle claims such as “all ‘deceptive behavior’ features do not participate in circuits X, Y and Z.”</p> </blockquote> <p>Summarizing, the hope is to be able to prove statements of the following form:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/eq.png" alt="My Image" width="500"/> </div> <h2 id="12-finetuning-models-is-a-challenge-to-ai-safety---saes-to-the-rescue">1.2 Finetuning models is a challenge to AI safety - SAEs to the rescue?</h2> <p>After outlining the procedure behind SAE-interpretability, we can answer a more general question: why is it relevant to translate the matrix language of neural networks (not more understandable to us than binary code) into a human-readable algorithmic language? There are several reasons, but, among the others, once we are able to do so, we can understand what features of an input a model identifies before predicting an answer. This can allow us to identify when a model is learning to deploy features spuriously correlated with the actual labels (an intuitive example <a href="https://ar5iv.labs.arxiv.org/html/1712.02950#:~:text=these%20image%20domains.-,2,Hidden%20Information,-We%20begin%20with">here</a><d-cite key="DBLP:journals/corr/abs-1712-02950"></d-cite>) or when the model is even <a href="https://arxiv.org/abs/2310.06824">lying to us</a><d-cite key="marks2024geometrytruthemergentlinear"></d-cite>. In both of these cases, it is a primary safety concern that these behaviors are not occurring in our model when used in production. Moreover, SAE-interpretability allows us to gain some insight into solving these problems precisely!</p> <p>Nevertheless, reality is often rougher than abstraction, and mechanistic interpretability suffers from one big problem: once we crack the interpretation of a model, we are only able to decode what is going on inside <strong>a singular, particular model, and not all models with the same architecture and different weights</strong>. Luckily, to have a model that shows emergent abilities, <a href="https://epochai.org/blog/compute-trends">we need a lot of compute</a><d-cite key="computetrends"></d-cite>, which remarkably restricts the Pareto frontier of competitive models and therefore the number of pre-trained models that we need to interpret. Therefore, one could think that if we manage to get some good SAE-interpreters for these few, we will be done. This may not be true! While indeed there are few state-of-the-art models, there are tons of finetuned versions of them (<a href="https://twitter.com/ClementDelangue/status/1839375655688884305?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1839375655688884305%7Ctwgr%5Eb537ad0b54dfc2d9ec69e2b01a337c5b0ce9d4e9%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Freadwrite.com%2Fai-startup-hugging-face-reaches-one-million-downloadable-ai-models-thats-a-lot-you-have-never-heard-of%2F">hugging face reached 1 million of models</a>), which are quite cheap to obtain compared to pretraining. <strong>If a simple finetuning will make the model uninterpretable, then we might be in danger</strong>. This could be the case, as <a href="https://arxiv.org/abs/2310.02949">previous studies</a><d-cite key="yang2023shadowalignmenteasesubverting"></d-cite> showed that alignment can be erased with a small finetuning. Then we ask ourselves:</p> <p><em>Is the interpretability of a model as weak as alignment to finetuning?</em></p> <p>In this post, we try to answer these questions and extend the positive results derived from a similar study by <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite>, where SAEs for the residual stream have been shown to be easily transferable (at the cost of some finetuning).</p> <p>Lastly, we want to remark how this kind of study derives its importance from the weakness of outer alignment forced by some ad-hoc finetuning. Indeed, if interpretability is more resistant to being broken than alignment, the path towards AI safety could be reached via <a href="https://www.cold-takes.com/high-level-hopes-for-ai-alignment/">digital neuroscience</a><d-cite key="Karnofsky_2023"></d-cite>, rather than simply through external finetuning.</p> <hr/> <h1 id="2-problem-setup">2. Problem setup</h1> <p>In choosing finetuned models to work with, we tried to strike a balance between the potential relevance of these models (how many people will actually use similar models), and the availability of pre-trained SAEs from the <a href="https://jbloomaus.github.io/SAELens/">SAELens</a><d-cite key="bloom2024saetrainingcodebase"></d-cite> library we used. So, we arrived at the following models and their finetunes:</p> <ol> <li>Gemma-2b (v1) -&gt; <a href="https://huggingface.co/shahdishank/gemma-2b-it-finetune-python-codes">Gemma-2b-it-finetuned-python-codes</a><d-cite key="gemmateam2024gemmaopenmodelsbased"></d-cite> finetune on <strong>Python code</strong> by Dishank Shah.</li> <li>Mistral-7B (v0.1) -&gt; <a href="https://huggingface.co/meta-math/MetaMath-Mistral-7B">MetaMath-Mistral-7B</a><d-cite key="jiang2023mistral7b"></d-cite> finetune on <strong>math problems</strong> by Meta from their <a href="https://arxiv.org/abs/2309.12284">MetaMath paper</a><d-cite key="yu2024metamath"></d-cite> by Yu et al.</li> </ol> <p>We then loaded the following SAEs for these models from SAELens (SAE layer numbering starts from 0):</p> <table style="margin: auto; text-align: center;"> <thead> <tr> <th>Model</th> <th>SAE Release</th> <th>SAE Layer</th> <th>N Features</th> </tr> </thead> <tbody> <tr> <td>Gemma-2b (v1)</td> <td>gemma-2b-res-jb by Joseph Bloom</td> <td>Residual layer #6</td> <td>16384</td> </tr> <tr> <td>Mistral-7B (v0.1)</td> <td>mistral-7b-res-wg by Josh Engels</td> <td>Residual layer #8</td> <td>65536</td> </tr> </tbody> </table> <p>Two important things to note:</p> <ul> <li>Gemma-2b SAE was trained on the <em>base</em> Gemma-2b model, while our Gemma-2b finetune was obtained from the <em>instruct</em> model, so there was one more “finetuning step” compared to the Mistral-7B case.</li> <li>Both finetunes that we used are <em>full</em> finetunes (with respect to the base model), i.e. no layer was frozen during the finetuning process. This is important for our SAE study, because all SAEs would trivially generalize (in terms of their reconstruction quality) if they were applied at the layer where activations are not affected a priori by the finetuning process.</li> </ul> <h2 id="21-studying-default-transferability">2.1 Studying “default” transferability</h2> <p>Similarly to what <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite> did with the instruct models, we’ll study the SAE transferability “by default”. That is, we’ll take an SAE trained on the base model, and apply it to the finetuned model to see if it maintains its performance (operationalized below). We won’t do any additional finetuning of our SAEs (on the activations from the finetune model), but as the same results from <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite> indicate: even when SAEs do not transfer by default, they can be finetuned relatively cheaply to recover their performance.</p> <p>Prior to evaluating the SAEs’ performance, we computed different similarity metrics for residual stream activations at the specific layer our SAEs are used for. The goal was to obtain some sort of a prior probability that our SAEs will transfer to the finetune model: the more similar the activations are, the higher is the (expected) probability that our SAEs will transfer. On the one hand, this analysis can be used as a <em>first step to select a fine-tuned model</em> from the thousands available on Hugging-Face. On the other hand, further studies can try to analyze <em>whether the phenomenon of SAE transferability actually correlates with the difference between activations</em> of the base and fine-tuned models (which we treat here only as an unproven heuristic).</p> <h2 id="22-evaluating-saes-performance">2.2 Evaluating SAEs performance</h2> <p>Designing rigorous approaches to evaluate the SAEs’ performance is an open problem in mechanistic interpretability. The main complicating factor is that we’re interested not so much in the SAEs reconstructed output, but rather in <strong>the SAE feature activations and feature vectors</strong>. However, measuring whether the SAEs features are interpretable or whether the features “are truly used by the model” is not straightforward. For our work, we’ll just start with computing standard evaluation metrics proposed either in the original “Towards monosemanticity” paper, or used in the later work, <a href="https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream">e.g. this one by Joseph Bloom</a><d-cite key="jbloom_lesswrong"></d-cite>:</p> <ol> <li><strong>L0 loss</strong>, namely the number of non-zero values in the feature activations vector. If the features retain their sparsity, we should expect L0 loss to be low compared to the total number of features, with the fraction being usually less than 1% (\(\frac{L_0}{N_{\text{features}}} &lt; 0.01\))</li> <li><strong>Reconstruction Cross-Entropy (CE) loss</strong> (a.k.a. substitution loss) which is computed as follows: <ol> <li>Run the model up to the layer where we apply the SAE, get this layer’s activations</li> <li>Run the activations through the SAEs, obtaining the reconstructions</li> <li><strong>Substitute</strong> the original activations with the reconstructed activations, continue the forward pass of the model, and get the corresponding cross-entropy loss</li> </ol> </li> <li><strong>Variance explained</strong>, is one of the standard ways to measure the difference of original activations and the activations reconstructed by the SAE. Specifically, we’ll use \(R^2\) score a.k.a. <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">Coefficient of determination</a></li> <li><strong>Feature density histograms</strong>: <a href="https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream#Why_can_training_Sparse_AutoEncoders_be_difficult__">as explained by Joseph Bloom</a><d-cite key="jbloom_lesswrong"></d-cite>, ideally the features should be “within good sparsity range”: <strong>not too sparse</strong> (e.g. when the features are “dead” and never activate) and <strong>not too dense</strong> (e.g. activating in more than 10% of the inputs). In both edge cases, anecdotally the features are mostly uninterpretable. One (rather qualitative) way to check this is to plot feature histograms: <ol> <li>Run a given sample of tokens through the model, and get the SAE feature activations.</li> <li>For each feature, record the number of times (tokens) it had a non-zero activation.</li> <li>Divide by the total number of tokens to get the fraction, and take the log10 of it (adding some epsilon value to avoid log-of-zero)</li> <li>Plot the histogram of the resulting log-10 fractions (the number of histogram samples equals to the number of features)</li> </ol> </li> </ol> <p>We’ll compute these metrics first for the base model and its SAE to get a baseline, then for the finetuned model with the same SAE, and compare the resulting metrics against the baseline<d-footnote>Even though density histograms are not technically a metric, we can infer quantitative metrics from them like the number of dead features</d-footnote>. The dataset used in both cases is the original training dataset of the corresponding SAE:</p> <ol> <li><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb">Fineweb</a><d-cite key="fineweb"></d-cite> dataset for Gemma-2b.</li> <li><a href="https://huggingface.co/datasets/monology/pile-uncopyrighted">The Pile</a><d-cite key="thepile"></d-cite> dataset for Mistral-7B.</li> </ol> <p>Based on the feature density histograms, we additionally zoomed in on individual features to see how well they transfer using <a href="https://transformer-circuits.pub/2023/monosemantic-features#phenomenology-universality">feature activation similarity and logit weight similarity</a><d-cite key="bricken2023monosemanticity"></d-cite>, as elaborated in the later section of this post.</p> <hr/> <h1 id="3-how-similar-are-residual-activations-of-finetuned-models">3. How similar are residual activations of finetuned models?</h1> <p>Before analyzing the SAE metrics on the finetuned models, we will visualize some easier computations on the <strong>residual</strong> activations (at the residual stream of the layer where we apply the corresponding SAE) to get a sense of the SAE transferability. Specifically, we are interested in the similarities between the base and finetuned model activations. We consider two metrics: the Cosine Similarity and the Euclidian Distance, for the model and datasets specified above with the <a href="https://huggingface.co/shahdishank/gemma-2b-it-finetune-python-codes">Gemma-2b Python-codes</a> and <a href="https://huggingface.co/meta-math/MetaMath-Mistral-7B">Mistral-7b MetaMath</a><d-cite key="yu2024metamath"></d-cite> finetunes respectively.</p> <p>Computing the Cosine Similarities and Euclidian Distances of the activations yields a tensor of shape <code class="language-plaintext highlighter-rouge">[N_BATCH, N_CONTEXT]</code> (each token position is determined by its batch number and position in the context). A simple metric to start with is to consider the global mean of the Cosine Similarities of the activations across both batch and context dimensions, giving a single scalar representing the overall similarity. This can be seen in the following table:</p> <table> <thead> <tr> <th>Model/Finetune</th> <th>Global Mean (Cosine) Similarity</th> </tr> </thead> <tbody> <tr> <td>Gemma-2b/Gemma-2b-Python-codes</td> <td>0.6691</td> </tr> <tr> <td>Mistral-7b/Mistral-7b-MetaMath</td> <td>0.9648</td> </tr> </tbody> </table> <p>This already suggests much better transferability of the Mistral-7b SAE for its MetaMath finetune. For a more fine-grained comparison, we flatten the similarities into a <code class="language-plaintext highlighter-rouge">N_BATCH * N_CONTEXT</code> vector and plot the histogram across all tokens:</p> <p>Gemma-2b - Cosine Similarity Histogram</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.1.png" alt="My Image" width="700"/> </div> <p>Mistral-7b - Cosine Similarity Histogram</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.2.png" alt="My Image" width="700"/> </div> <p>Gemma-2b - Euclidian Distance Histogram</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.3.png" alt="My Image" width="700"/> </div> <p>Mistral-7b - Euclidian Distance Histogram</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.4.png" alt="My Image" width="700"/> </div> <p>We can see how the Cosine Similarities for Mistral-7b are concentrated around a value close to 1, whereas the Gemma-2b similarities are more spread around the mean of 0.66 (higher variance). The Euclidian Distances histogram shows a similar distinction, with the Gemma-2b distances being spread around a mean of around 120, while the bulk of Mistral-7b distances stay at a low value.</p> <p>We also visualize the per-context mean of Cosine Similarities and Euclidian Distances. We compute the mean across batches but preserve the context dimension, giving a tensor of shape <code class="language-plaintext highlighter-rouge">[N_CONTEXT]</code>, which reflects how similarity changes over the context length.</p> <p>Gemma-2b - Cosine Similarity Context Line</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.5.png" alt="My Image" width="700"/> </div> <p>Mistral-7b - Cosine Similarity Context Line</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.6.png" alt="My Image" width="700"/> </div> <p>Gemma-2b - Euclidian Distance Context Line</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.7.png" alt="My Image" width="700"/> </div> <p>Mistral-7b - Euclidian Distance Context Line</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/3.8.png" alt="My Image" width="700"/> </div> <p>In the above, we can see how the similarities and distances stabilise quickly after a few tokens of context, albeit around different values. Both models start with close to 1 similarity for the first token, and then stabilize after a few tokens.</p> <p>These results already anticipate a considerable difference in the transferability of the SAEs for the two models, which will be explored more in-depth in the following section.</p> <hr/> <h1 id="4-how-well-do-the-base-saes-work-on-the-finetuned-models">4. How well do the base SAEs work on the finetuned models?</h1> <h2 id="41-methodology">4.1 Methodology</h2> <p>In this section, we’ll compute a set of standard SAE metrics for base and finetuned models, using the same base SAE in both scenarios (i.e., the SAE that was trained on the base model activations):</p> <ol> <li>For the <strong>base model</strong>: <ol> <li>we sample input tokens from the <strong>original SAE training dataset</strong></li> <li>pass the tokens through the base model to get <strong>the model’s activations</strong></li> <li>pass the activations through the SAE to <strong>get the feature activations</strong></li> <li>complete the forward pass of the base model to <strong>get the final loss</strong> (used afterward for the reconstructed loss)</li> </ol> </li> <li>Then we repeat the same steps for the <strong>finetuned</strong> <strong>model</strong>, using the same tokens dataset</li> <li>Finally, we compute the metrics mentioned in the Evaluating SAEs performance section.</li> </ol> <h2 id="42-technical-details">4.2 Technical Details</h2> <p>Before delving deeper into the results, we want to point out three technical details:</p> <ol> <li>The sample size used across nearly all experiments is <strong>256K tokens</strong></li> <li> <p>Similarly to <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite> we observed a major numerical instability when computing our reconstruction loss and variance explained metrics. As the authors noted:</p> <blockquote> <p>SAEs fail to reconstruct activations from the opposite model that have outlier norms (e.g. BOS tokens). These account for less than 1% of the total activations, but cause cascading errors, so we need to filter these out in much of our analysis.</p> </blockquote> </li> <li> <p>To solve this problem we used a similar outlier filtering technique, where an outlier is defined as <em>an activation vector whose (L2) norm exceeds a given threshold</em>. We tried several ways to find a “good” threshold and arrived at values similar to those used by <em>Kissane et al</em>:</p> <ul> <li><strong>290 norm value</strong> for the Gemma-2b model</li> <li><strong>200 norm value</strong> for the Mistral-7B model</li> </ul> <p>Using these threshold values, we found that <strong>only 0.24% activations are classified as outliers in the Gemma-2b model</strong>, and <strong>0.7% in the Mistral-7B</strong>, agreeing with the Kissane et al. result that these outliers account for less than 1% of activations. It should be noticed, however, that we <em>only used this outlier filtering technique for our reconstruction loss &amp; variance explained</em> experiments to avoid numerical errors. In practice, it means that for this experiment the true sample size was a little smaller than for the other experiments, equal to \(\left( 1 - \text{outlier_fraction} \right) \times 256{,}000\) with the \(\text{outlier_fraction}\) defined above.</p> </li> </ol> <h2 id="43-results">4.3 Results</h2> <p>In the following table, we report the results for the first experiment with the <strong>Mistral</strong> model pair:</p> <table style="margin: auto; text-align: center;"> <thead> <tr> <th>Model\\Metric</th> <th>L0 Loss</th> <th>Clean CE Loss</th> <th>Reconstruction CE Loss</th> <th>Loss Delta</th> <th>$$R^2$$ Score (Variance Explained)</th> <th>Dead Features (%)</th> </tr> </thead> <tbody> <tr> <td>Mistral-7B</td> <td>83.37</td> <td>1.78</td> <td>1.93</td> <td><b>0.15</b></td> <td>0.68</td> <td>0.76%</td> </tr> <tr> <td>Mistral-7B MetaMath</td> <td>90.22</td> <td>1.94</td> <td>2.1</td> <td><b>0.16</b></td> <td>0.58</td> <td>0.64%</td> </tr> </tbody> </table> <p>As you can see, the L0-Loss of the features and variance explained increase a bit, but the reconstruction loss delta is almost the same! It suggests that our Mistral SAE may still transfer after finetuning, although with a slightly worse reconstruction quality. Let’s compare these results with the Gemma-2b and its Python finetune:</p> <table style="margin: auto; text-align: center;"> <thead> <tr> <th>Model\\Metric</th> <th>L0 Loss</th> <th>Clean CE Loss</th> <th>Reconstruction CE Loss</th> <th>Loss Delta</th> <th>$$R^2$$ Score (Variance Explained)</th> <th>Dead Features (%)</th> </tr> </thead> <tbody> <tr> <td>Gemma-2b Base</td> <td>53.59</td> <td>2.65</td> <td>3.16</td> <td>0.51</td> <td>0.97</td> <td>48.1%</td> </tr> <tr> <td>Gemma-2b Python-codes</td> <td>84.74</td> <td>3.29</td> <td><b>7.5</b></td> <td><b>4.21</b></td> <td><b>-10.27</b></td> <td>0.1%</td> </tr> </tbody> </table> <p>Now, this is what <em>bad</em> SAE transferability looks like! But actually this should come as no surprise after the <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al.</a><d-cite key="sae_finetuning"></d-cite> result: they concluded that Gemma-2b SAEs do not transfer even between the base and the <em>instruct</em> models, so when you add an additional finetuning step on top of the instruct, it’s completely expected that the metrics will get even worse. The authors explain this behavior with an abnormal weights deviation in the instruct model:</p> <blockquote> <p>Here we show that the weights for Gemma v1 2B base vs chat models are unusually different, explaining this phenomenon (credit to Tom Lieberum for finding and sharing this result):</p> </blockquote> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.1.png" alt="My Image" width="700"/> </div> <p>But what effect does this have on the SAE features? Well, we could expect that if an SAE is no longer able to reconstruct the input activations, it will always “hallucinate” - any features it “detects” will not make any sense. Let’s see if this expectation holds in practice for the Gemma-2b model.</p> <p>We’ll start with the feature activations histogram plot. In general, this kind of histogram gives little insight since you will always have a large mode at 0 due to feature sparsity, and some kind of log-normal distribution at non-zero activations. Indeed, this is what happens in the base Gemma-2b model, when we plot its log10 feature activations histogram:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.2.png" alt="My Image" width="700"/> </div> <p>Two things to note:</p> <ul> <li>The first bar’s count value is <strong>clipped</strong> - it’s much larger than 900k, equal to more than 6 million.</li> <li>We used a smaller sample size for this experiment due to the need to store all the feature activations in memory to plot the histogram - here the sample size is equal to <strong>128K</strong>.</li> </ul> <p>With this in mind, let’s compare it with the same kind of histogram for our Gemma-2b finetune (where the features are given by the same SAE):</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.3.png" alt="My Image" width="700"/> </div> <p>If that’s not a characterization for “cursed”, we don’t know what is! Instead of a nice bell curve, we now have some sort of a 3-mode monster in the non-zero activations section. To be clear - nothing like that was present when we repeated this experiment for the Mistral-7B: we obtained the well-expected bell curves with similar mean and standard deviation for both base and finetuned models. We don’t have a good explanation for this Gemma-2b anomaly, but we’ll try to give some deeper insight into what happens with the SAE features in the next section.</p> <p>Let’s move on to the feature densities plot, which was produced as described in the Evaluating SAEs Performance section. Starting from Gemma-2b:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.4.png" alt="My Image" width="700"/> </div> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.5.png" alt="My Image" width="700"/> </div> <p>As expected from the above results, the two plots have little in common. We see that most of our dead features (in the base model) turn alive in the finetuned one! To see where exactly these dead feature densities land in the finetuned model (what are their new densities), we also made a parallel coordinate plot (below we show two versions of the same plot: with different density ranges highlighted):</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.6.png" alt="My Image" width="700"/> </div> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.7.png" alt="My Image" width="700"/> </div> <p>So it looks like the dead features spread out quite widely in the finetuned model, contributing to more probability mass before the -3 log-density. As for the dense features (-4 to -1 log density) in the base model, their density interval gets squeezed to (-3, -1) in the finetuned model, causing a sharp mode near the -2.5 log-density value.</p> <p>We’ll continue the Gemma-2b investigation in the next chapter, and conclude this section with the Mistral-7B feature density histograms:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.8.png" alt="My Image" width="700"/> </div> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.9.png" alt="My Image" width="700"/> </div> <p>We can see that for Mistral the feature densities distribution almost doesn’t change after the model finetuning! The only slight difference is in the number of dead features: the finetuned Mistral has around 80 dead features less than the base one. To zoom in closer, we also show the parallel coordinate plot:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.10.png" alt="My Image" width="700"/> </div> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/4.11.png" alt="My Image" width="700"/> </div> <p>So yes, a small number of features do turn alive, but also some features (even a smaller amount) turn dead in the finetuned model! Overall though, the feature densities look very similar, with the Pearson correlation of their log10 densities equal to 0.94 (versus 0.47 for the Gemma-2b case).</p> <hr/> <h1 id="5-do-the-base-sae-features-transfer-to-the-finetuned-model">5. Do the base SAE features transfer to the finetuned model?</h1> <p>We want to motivate this section with a more thoughtful consideration of the question <strong>what is the best way to operationalize SAE transferability</strong>. In the previous section, we simply checked the standard SAE evaluation metrics to see how well they reconstruct the activations. But this doesn’t necessarily reflect the main goal of using SAEs - <strong>interpreting the model.</strong></p> <p>As noted in the SAE features for AI Safety section of our post, the end goal of using SAEs for interpretability is to be able to <strong>use features as the basis for circuit analysis</strong>. And if we assume that some kind of circuit analysis has been done for the base model to prove that it doesn’t implement certain undesirable behaviors, the most ambitious operationalization of SAE transferability (for AI Safety) would be the ability to apply <strong>the same kind of circuit analysis with the same SAE</strong> (or the finetuned one) <strong>to prove or disprove that the finetuned model is safe.</strong></p> <p>In our case of studying transferability “by default”, the better way to demonstrate it is to show that our SAE features “stay relevant” in the finetuned model, so that we can expect that they still potentially serve as the basis for circuit analysis. Showing this rigorously would be a really difficult task (partly because there’s no standard way to do circuit analysis in the SAE basis yet) and it’s out of scope for this blog post. What we did instead is apply an <a href="https://transformer-circuits.pub/2023/monosemantic-features#phenomenology-universality">approach from Towards Monosemanticity</a><d-cite key="bricken2023monosemanticity"></d-cite> for studying features <strong>universality</strong>:</p> <ul> <li>Normally to study if a feature from model A is conceptually the same (has the same “role” in the model) as another feature in the model B, one can compute <ul> <li><strong>feature activation similarity</strong>: represent a feature as a vector of its activations across a given sample of tokens, obtaining a <em>feature activations vector →</em> do it for model A’s feature, model B’s feature and compute a <strong>correlation between their activations vectors</strong>.</li> <li><strong>feature logits similarity:</strong> represent a feature as a vector of its <a href="https://transformer-circuits.pub/2023/monosemantic-features#feature-arabic-effect">logit weights</a><d-cite key="bricken2023monosemanticity"></d-cite> (for each token of the vocab a logit weight is the relative probability of that token as predicted by the feature direct effect), obtaining a <em>feature logit vector→</em> do it for model A’s feature, model B’s feature and compute a <strong>correlation between their logit vectors</strong>.</li> </ul> </li> <li>So, we call model A our base model, model B - the corresponding finetune, and compute feature activation similarity and logits similarity for a given sample of the SAE features (which are the same for the base and finetuned models).</li> </ul> <p>This can be seen as a (very) rough proxy for “the feature is doing the same job in the finetuned model”, and we call it the “<strong>feature transferability test</strong>”.</p> <h2 id="51-feature-selection-procedures">5.1 Feature Selection Procedures</h2> <p>Conceptually, dead features are completely different from the ordinary features: <a href="https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream">as explained by Joseph Bloom</a><d-cite key="jbloom_lesswrong"></d-cite>, they represent permanently lost capacity in an SAE and thus are merely an artifact of the SAE training<d-footnote>Essentially, an SAE is saying “If I cannot find relevant features for reconstructing my input anymore, I’m going to learn a direction(s) in the activation space that is orthogonal to all the inputs I’ve seen, so that I get zero activations for the features I cannot learn and thus I’m no longer penalized by sparsity, at least”. If a feature was dead in the base model but is no longer dead in the finetuned one, it implies a distributional shift in the activation space (for which the SAE was not adapted, but could potentially be adapted by finetuning)</d-footnote>. So we decided to make a separate analysis of dead features and “<strong>regular</strong>” features, that we defined as <strong>features with a log10 density between -5 and -1.</strong></p> <p>By dead features, we mean features that are <strong>exclusively</strong> dead (never activating across our entire 256K sample of tokens), i.e. <strong>dead only in one of the models</strong>:</p> <ul> <li>a “dead base” feature is a feature that is dead in the base model, but not in the finetuned one</li> <li>a “dead finetune” feature is a feature that is dead in the finetuned model, but not in the base one.</li> </ul> <p>We observe that only a handful of features are dead in both models, so we think our definitions give more information on what we’re analysing.</p> <p>Then, our approach for the rest of this section looks as follows:</p> <ol> <li>We sample max 100 exclusively dead features and 1000 regular features using our density histogram values for each base model and its finetune.</li> <li>We convert these features to their activation vector and logit vector representations for both the base model and its finetune.</li> <li>For each regular feature, we compute their <strong>activation similarity</strong> and the <strong>logits similarity</strong> with respect to the corresponding finetune, and for the exclusively dead features - their <strong>activation error:</strong> <ul> <li>We cannot really compute the activation similarity as a correlation score if one of the feature’s activation vectors is constantly 0, i.e. the feature is dead. In this case we take the log10 of these activation vectors (with <code class="language-plaintext highlighter-rouge">1e-10</code> as the epsilon value to avoid a log of zero), take the <a href="https://en.wikipedia.org/wiki/Mean_absolute_error">Mean Absolute Error</a> of the resulting vectors and call it <strong>activation error</strong><d-footnote>It makes little sense to compute dead features logit similarity: if the feature never activates, it doesn’t matter what its logit effect is - it will never manifest itself in the model. </d-footnote>.</li> </ul> </li> <li>Additionally, we plot a <strong>histogram of similarities</strong> for each feature type, since we observed a significant deviation of the similarity score (mainly activation similarity) in some experiments.</li> </ol> <h2 id="52-gemma-2b-features-transferability-test">5.2 Gemma-2b features transferability test</h2> <p>One could say that in the Gemma-2b case, it’s obvious from the previous results that our SAE doesn’t transfer. But we could imagine a case where <em>some</em> (perhaps a tiny fraction) of our SAE features from the regular density interval do still transfer, so we decided to conduct this experiment anyway.</p> <p>Starting with the features that are exclusively dead in the <em>base</em> model, their mean activation error for Gemma-2b and Gemma-2b python-codes finetune is <strong>0.025</strong>. A histogram of these 100 activation errors is given below:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.1.png" alt="My Image" width="700"/> </div> <p>This made us think that “dead features turning alive” anomaly is not so much of an anomaly, because the dead features activate only (very) slightly in the finetuned model. The max activation value across all 100 dead features in the finetuned model was <strong>1.1,</strong> indicating that our “dead feature direction” is only slightly off in the finetuned model, and can be easily adjusted by SAE finetuning.</p> <p>As for the features that are exclusively dead in the <em>finetune</em> model, Gemma-2b had only two of them on our sample, with the activation error equal to 0.34 and 3.19, which is considerably higher than in the previous case.</p> <p>Moving on to the regular features, we expected to see a much more drastic dissimilarity of their activations. Indeed, the <strong>mean activation similarity for our sample of Gemma-2b regular feature is 0.39</strong>. Let’s check the histogram of these similarity scores:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.2.png" alt="My Image" width="700"/> </div> <p>Interestingly, we see that a small fraction of features (~10%) have an activation similarity above 0.8! This implies that if these features were interpretable in the base model, they will most likely stay interpretable in the finetune model<d-footnote>We didn’t try to manually interpret these features’ activations to verify this claim, and it would be interesting to see future works in this direction</d-footnote>. But we’re not sure about the significance of this result: this could just as well be noise, so we invite further research in this area.</p> <p>As for the logit similarity of these regular features, it turns out it’s much higher than our activation similarity, with a mean value of <strong>0.952.</strong> Looking at the logit similarity scores histogram, it’s also much more concentrated towards the end of the interval:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.3.png" alt="My Image" width="700"/> </div> <p>However, it’s easy to be misled by the mean logits similarity score. What it’s really saying is that our unembedding matrix (which is multiplied by the feature direction to get the logits similarity) hasn’t changed that much after finetuning (with a Frobenius norm ratio equal to 1.117 as we checked for our Gemma finetune). So <em>if the feature has still the same direction, we can indeed say that the “direct feature effect” hasn’t changed in the finetuned model, but we never checked this premise!</em> All we know is that there exist ~10% of features which have reasonably high activation similarity scores with the features from the base model. <em>The key point is that the latter is a statement about the feature’s encoder direction</em> (one that is used to project onto to get the feature’s activation, <a href="https://www.lesswrong.com/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s">explained by Neel Nanda here</a><d-cite key="Nanda_2023"></d-cite>), <em>not the decoder one -</em> which is what we mean when we talk about <em>feature directions. So it could be the case that the feature is still there but changed its direction</em> as discussed in <a href="https://www.lesswrong.com/posts/bsXPTiAhhwt5nwBW3/do-sparse-autoencoders-saes-transfer-across-base-and?commentId=pJHfoZ2GLD8neS57g">this comment,</a><d-cite key="sae_finetuning"></d-cite> it could also be that some features change their directions and the others don’t - it’s impossible to tell when the reconstruction score (e.g. variance explained) is as poor as in the Gemma-2b case.</p> <h2 id="53-mistral-7b-features-transferability-test">5.3 Mistral-7B features transferability test</h2> <p>Here we repeat all the same experiments for Mistral-7B and its MetaMath finetune, and compare the result with the Gemma-2b case.</p> <p>Let’s start with the features that are exclusively dead in the Mistral base model. Their mean activation error is 0.0003, which is almost <em>two orders of magnitude</em> lower than in the Gemma-2b case. The corresponding histogram looks like this:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.4.png" alt="My Image" width="700"/> </div> <p>Once again, the results suggest <em>that even though the dead features in the base model are no longer dead in the finetuned one</em>, they activate really weakly on average, so it should be easy to adjust them with a cheap SAE finetuning.</p> <p>The activation error for the features exclusively dead in the finetuned model tells a similar story:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.5.png" alt="My Image" width="700"/> </div> <p>Here the error is even smaller, implying that even though some features stopped activating after finetuning, their corresponding activation values in the base model were really low. And the features are often uninterpretable in the lowest activation intervals anyway, so it should have a minor overall effect on SAEs transferability.</p> <p>Let’s conclude this section with an analysis of our regular features. As expected from the results of the last section, the activation similarity of these features is quite high, with a mean value of <strong>0.958</strong>. As for the activation scores histogram:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.6.png" alt="My Image" width="700"/> </div> <p>As we can see, the distribution of the scores is strongly attracted to the 0.9-1.0 correlation interval, so we can conclude that SAE feature transferability is significantly high in this case. This is also backed up by the mean logits similarity of 0.9996, and a rather straightforward logits similarity histogram:</p> <div style="text-align: center;"> <img src="/blog/assets/img/2025-02-06-do-sparse-autoencoders-saes-transfer-across-base-and-finetuned-language-models/5.7.png" alt="My Image" width="700"/> </div> <hr/> <h1 id="6-conclusions--limitations">6. Conclusions &amp; Limitations</h1> <h2 id="61-conclusions">6.1 Conclusions</h2> <p>Going back to our original question of <em>“Do SAEs trained on a base model transfer to the finetuned one?”</em>, the most obvious answer that comes to mind now is - it depends! We got drastically different results for our Gemma-2b-python-codes and Mistral-7B-MetaMath finetunes. However, <strong>it seems possible that one could estimate the “degree of transferability” in advance<em>.</em></strong> One method is to compute various weight deviation metrics, such as the one used by <a href="https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models">Kissane et al</a><d-cite key="sae_finetuning"></d-cite> for Gemma-2b, and another method that we used - to compute activation similarities of the model that are fed into an SAE. Both of these anecdotally correlate with the results of our transferability experiments, but a more thorough study is definitely needed.</p> <p>Another takeaway we’ve had after finishing this post is that <strong>“SAE transferability” can mean different things</strong>. One can utilize the standard SAE evaluation metric to get a high-level evaluation of the SAE quality on the finetuned model, but it doesn’t always give a deeper insight into what happens with the SAE feature once we zoom in (which may be more interesting for the real SAE applications in mech interp). Our Gemma-2b results suggest that some SAE features may still be interpretable, even when finetuning has completely rendered the SAE incapable of reconstructing the input. And although the significance of this result can be rightly questioned, we still think it is interesting to investigate further.</p> <h2 id="62-limitations">6.2 Limitations</h2> <p>The main limitations we see in our work are the following:</p> <ul> <li>It’s not clear how our results will generalize to other finetunes. A more principled approach would be to use a custom finetuning setup, where one could e.g. study the relationship between the amount of compute put into finetuning and some key SAE transferability metrics like the reconstruction loss etc. <ul> <li>Our finetuned models also had almost the same dictionaries as the base model (with the exception of a single padding token), so it’s also not clear whether our results generalize to the finetuned model with significantly modified dictionaries (e.g. language finetunes for languages that were not in the original training dataset of the base model)</li> </ul> </li> <li>We only studied SAEs for a single residual layer for Gemma-2b and Mistral-7B models. A more thorough study is needed to see how these results will vary when considering different layers and different SAE activations, e.g. MLP or hidden head activations.</li> <li>All our experiments were performed on the training dataset of the base SAE, i.e. on the original training distribution of the base models. But the finetuned models are mostly used for tasks that they have been finetuned on, so we definitely need some future work here to extend these results to a more specific setting of finetuned models.</li> <li>Our analysis of SAE features transferability was somewhat superfluous, because we didn’t do a thorough investigation of the interpretability of our features after the finetuning. An even more representative study would be to replicate some kind of circuit analysis in the SAE basis to rigorously prove if (at least some) features are still involved in the same computation of the finetuned model.</li> </ul> <hr/> <h1 id="appendix">Appendix</h1> <p>All code is available on <a href="https://github.com/tommasomncttn/SAE-Transferability">github</a></p>]]></content><author><name>Taras Kutsyk</name></author><category term="sae"/><category term="mechanistic interpretability"/><category term="model diffing"/><summary type="html"><![CDATA[TLDR (Executive Summary)]]></summary></entry><entry><title type="html">UniReps 2024 Awards</title><link href="https://unireps.org//blog/2025/unireps2024awards/" rel="alternate" type="text/html" title="UniReps 2024 Awards"/><published>2025-01-11T00:00:00+00:00</published><updated>2025-01-11T00:00:00+00:00</updated><id>https://unireps.org//blog/2025/unireps2024awards</id><content type="html" xml:base="https://unireps.org//blog/2025/unireps2024awards/"><![CDATA[<p>The second edition of the UniReps Workshop at NeurIPS 2024 was a huge success! Building on the momentum from last year, we were thrilled to welcome over 1,000 participants from academia and industry for a day packed with engaging talks, lively discussions, and an exciting poster session. We received 106 submissions this year, including 25 fantastic proceedings papers that will soon be published in the workshop volume.</p> <p>We couldn’t have done it without all of you! A big thank you to the authors, our amazing program committee of 181 members, the participants who made the event so special, and our sponsors for their incredible support.</p> <p>We were also proud to highlight some exceptional work through our awards. Here’s a look at the winners and their inspiring contributions.</p> <hr/> <h2 id="best-paper-awards">Best Paper Awards</h2> <h3 id="proceedings-track">Proceedings Track</h3> <p><strong>Authors</strong>: <a href="https://sarahharvey.github.io">Sarah Harvey</a>, <a href="https://sites.google.com/view/lipshutz/home">David Lipshutz</a>, and <a href="https://alexhwilliams.info">Alex H. Williams</a></p> <p><strong>Title</strong>: <em>“What Representational Similarity Measures Imply about Decodable Information.”</em> <d-cite key="harvey2024what"></d-cite></p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_best_proceedings_2024.jpg" alt="Best Paper Proceedings Track Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <hr/> <h3 id="extended-abstracts-track">Extended Abstracts Track</h3> <p><strong>Authors</strong>: <a href="https://scholar.google.com/citations?user=EtEVFLoAAAAJ">Richard Antonello</a> and <a href="https://chengemily1.github.io">Emily Shana Cheng</a></p> <p><strong>Title</strong>: <em>“Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models.”</em> <d-cite key="antonello2024evidence"></d-cite></p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_best_abstract_2024.jpg" alt="Best Paper Extended Abstracts Track Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <hr/> <h2 id="honorable-mentions">Honorable Mentions</h2> <h3 id="proceedings-track-1">Proceedings Track</h3> <p><strong>Author</strong>: <a href="https://alexhwilliams.info">Alex H. Williams</a></p> <p><strong>Title</strong>: <em>“Equivalence between Representational Similarity Analysis, Centered Kernel Alignment, and Canonical Correlations Analysis.”</em> <d-cite key="williams2024equivalence"></d-cite></p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_honorable_proceedings_2024.jpg" alt="Honorable Mention Proceedings Track Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <h3 id="extended-abstracts-track-1">Extended Abstracts Track</h3> <p><strong>Authors</strong>: <a href="https://chenyuwang-monica.github.io">Chenyu Wang</a>, <a href="https://www.mit.edu/~sharut/">Sharut Gupta</a>, <a href="https://scholar.google.com/citations?user=2gU9PYQAAAAJ">Xinyi Zhang</a>, <a href="https://www.cs.toronto.edu/~stonekaboni/">Sana Tonekaboni</a>, <a href="https://scholar.google.ch/citations?user=gTWUZlsAAAAJ">Stefanie Jegelka</a>, <a href="https://people.csail.mit.edu/tommi/">Tommi Jaakkola</a>, and <a href="https://www.carolineuhler.com/caroline-uhler">Caroline Uhler</a></p> <p><strong>Title</strong>: <em>“An Information Criterion for Controlled Disentanglement of Multimodal Data.”</em> <d-cite key="wang2024an"></d-cite></p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_honorable_abstract_2024.jpg" alt="Honorable Mention Extended Abstracts Track Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <hr/> <h2 id="best-reviewer-award">Best Reviewer Award</h2> <p>A special <strong>Best Reviewer Award</strong> was given to <a href="https://akamboj2.github.io">Abhi Kamboj</a> for their exceptional feedback.</p> <div class="l-page-outset"> <img src="/blog/assets/img/2025-01-11-unireps2024awards/unireps_best_reviewer_2024.jpg" alt="Best Reviewer Award Ceremony" style="max-width: 60%; height: auto; display: block; margin: 0 auto;"/> </div> <hr/> <h2 id="acknowledgments">Acknowledgments</h2> <p>🔴 Congratulations to all the award recipients for their outstanding work, and a heartfelt thank you to everyone who participated in the workshop. The UniReps community keeps growing! We look forward to seeing you again next year! 🔵</p>]]></content><author><name>UniReps Organizing Team</name></author><category term="NeurIPS,"/><category term="awards,"/><category term="2024,"/><category term="UniReps"/><summary type="html"><![CDATA[A gallery of the UniReps 2024 Awards winners and photos]]></summary></entry><entry><title type="html">Failures in Perspective-Taking of Multimodal AI Systems</title><link href="https://unireps.org//blog/2024/failures_perspectivetaking/" rel="alternate" type="text/html" title="Failures in Perspective-Taking of Multimodal AI Systems"/><published>2024-11-20T00:00:00+00:00</published><updated>2024-11-20T00:00:00+00:00</updated><id>https://unireps.org//blog/2024/failures_perspectivetaking</id><content type="html" xml:base="https://unireps.org//blog/2024/failures_perspectivetaking/"><![CDATA[<div class="caption"> Listen to the AI-generated podcast based on our preprint or check out the benchmark paper and project on GitHub: </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/blog/assets/audio/2024-11-20-failures_perspectivetaking/podcast.mp3" controls=""/> </figure> </div> </div> <div style="display: flex; align-items: center;"> <a href="https://github.com/bridgetleonard2/perspectiveTaking" style="margin-left: 70px; margin-right: 80px; display: inline-block;"> <img src="/blog/assets/img/2024-11-20-failures_perspectivetaking/github-mark.png" alt="GitHub" class="logo" width="40"/> </a> <a href="https://arxiv.org/abs/2409.13929" style="display: inline-block;"> <img src="/blog/assets/img/2024-11-20-failures_perspectivetaking/arxiv-logomark-small@2x.png" alt="arXiv" class="logo" width="30"/> </a> </div> <h2 id="introduction">Introduction</h2> <p>Recent research in AI has exposed a critical limitation: the inability of current models to effectively perform spatial reasoning tasks. Despite their impressive visual perception capabilities, these models struggle to understand spatial relationships and make inferences about them. While previous research has explored aspects of spatial cognition in AI, it often lacks the specificity characteristic of human spatial cognition studies. In cognitive psychology, tasks are carefully designed to isolate distinct processes, enabling precise measurement and minimizing bias or reliance on alternative strategies. To bridge the gap between cognitive science and artificial intelligence, we focus on a fundamental aspect of human spatial reasoning: visual perspective-taking.</p> <blockquote> <p><strong>Visual perspective-taking</strong> is the ability to mentally simulate a viewpoint other than one’s own. It allows us to understand the relationship between objects and how we might have to manipulate a scene to align with our perspective, which is essential for tasks like navigation and social interaction.</p> </blockquote> <p>By leveraging established methodologies, we can rigorously evaluate AI’s spatial cognition, starting with perspective-taking. The extensive human literature on spatial reasoning offers a valuable benchmark, enabling comparisons between model performance and the human developmental trajectory. This comparison helps identify critical gaps and opportunities for enhancing AI models.</p> <p>Our aim was to create a targeted perspective-taking benchmark for multimodal AI systems, probing various levels and components of the cognitive process.</p> <h3 id="definitions-and-terminology">Definitions and Terminology</h3> <ul> <li> <p><strong>Level 1 Perspective-taking</strong> refers to knowing that a person may be able to see something another person does not</p> </li> <li> <p><strong>Level 2 Perspective-taking</strong> refers to the ability to represent how a scene would look from a different perspective</p> </li> <li> <p><strong>Mental Rotation</strong> where one imagines an object or scene rotating in space to align with a perspective</p> </li> <li> <p><strong>Spatial vs Visual Judgments</strong> responding to queries about the spatial orientations of objects or their non-spatial visual characteristics</p> </li> </ul> <details><summary>Click here to learn more about perspective-taking</summary> <p>In the human developmental literature, perspective-taking has been stratified into two levels, defined above. Based on developmental literature, level 1 perspective-taking appears fully developed by the age of two <d-cite key="moll2006level1"></d-cite>. In contrast, although success on some simple Level 2 tasks is first seen around age 4 <d-cite key="newcombe1992children"></d-cite>, Level 2 perspective-taking continues to develop into middle childhood <d-cite key="surtees2012egocentrism"></d-cite> and even into young adulthood <d-cite key="dumontheil2010online"></d-cite>. In terms of measurement, a common Level 1 task might ask if an object is viewable (or positioned to the front or back) of a person or avatar in a scene. Level 2 is often measured by having subjects assess the spatial relationship between objects.</p> <p>A more specific cognitive process, <strong>mental rotation</strong>, where one imagines an object or scene rotating in space to align with a perspective, plays an important role in perspective-taking. Surtees et al. <d-cite key="surtees2013similarities"></d-cite> experimentally manipulated Level 1 and Level 2 perspective-taking by presenting participants with tasks where they viewed numbers or blocks relative to an avatar. Different stimuli were used to elicit visual and spatial judgments, like whether the number was a “6” or a “9” from the person’s perspective, or if the block was to the person’s right or left. Level 1 tasks involved indicating whether the number/block was visible to the avatar, while Level 2 involved reporting either the number seen by the avatar or whether it was to the avatar’s left or right (Level 2). For both visual and spatial judgments, response times were longer for Level 2 tasks as the angular difference between the avatar and the participant increased, while response times remained unaffected by the angle in Level 1 tasks. This increase in response time when the participant’s view was unaligned with the avatar’s perspective is attributed to the mental rotation process, either rotating the scene or rotating one’s own reference frame to align with the avatar.</p> </details> <hr/> <h3 id="creating-a-new-benchmark">Creating a New Benchmark</h3> <h4 id="limitations-of-current-benchmarks">Limitations of Current Benchmarks</h4> <p>There are two main limitations current AI spatial cognition assessment:</p> <details><summary>Reasoning with language alone can inflate performance on spatial benchmarks</summary> <p>Text-only GPT-4 achieves a score of 31.4, while multimodal GPT-4v achieves a score of 42.6 on the spatial understanding category of Meta’s openEQA episodic memory task <d-cite key="majumdar2024openeqa"></d-cite>. The strong baseline score achieved by the text-only GPT-4 suggests that many “real-world” questions based on visual scenes can be deduced linguistically. Additionally, the limited improvement when moving from a blind LLM to a multimodal one suggests that vision models do not gain a significant understanding of space beyond what can be inferred through language.</p> </details> <details><summary>Benchmark scores can be hard to interpret since models often perform poorly</summary> <p>BLINK <d-cite key="fu2024blink"></d-cite>, a benchmark more specifically focused on visual perception capabilities, contains categories related to spatial cognition, such as relative depth and multi-view reasoning. On this benchmark, GPT-4v achieved an accuracy of 51.26%, only 13.17% higher than random guessing and 44.44% lower than human performance. When benchmarks are highly focused on visuospatial tasks, the significant shortcomings of multimodal models suggest that further advancements are needed before these models can reliably perform in real-world scenarios. Even within specific categories, it is often difficult to determine <em>why</em> models fail on certain tasks while succeeding on others, as these failures cannot be easily linked to the absence of a particular cognitive process.</p> </details> <p>To target some of these issues, we apply established tasks in cognitive psychology that measure spatial cognition in a precise manner. By applying these tasks to AI systems, we gain not only improved measurement precision but also the ability to compare AI performance with human development, providing clear insights into model limitations and areas for improvement.</p> <h4 id="perspective-taking-benchmark">Perspective Taking Benchmark</h4> <p>Leveraging the distinction between Level 1 and Level 2 perspective-taking <d-cite key="surtees2013similarities"></d-cite>, we propose a small perspective-taking benchmark that assesses multimodal model capabilities across three tasks: Level 1, Level 2 with spatial judgments, and Level 2 with visual judgments. Although human performance remains stable regardless of judgment type, we include this differentiation of Level 2 stimuli to examine potential egocentric biases that may arise in multimodal models when interpreting spatial relations compared to optical character recognition (OCR). This benchmark aims to address gaps in current AI spatial cognition measures by increasing process specificity, limiting language-based solutions, and offering straightforward comparisons to human cognition.</p> <hr/> <h2 id="methods">Methods</h2> <p>Our study utilized GPT-4o (“gpt-4o-2024-05-13” via OpenAI’s API) to conduct a series of perspective-taking experiments designed to capture the system’s spatial reasoning abilities. We kept <code class="language-plaintext highlighter-rouge">top_p = 0.5</code> to restrict the model from choosing from the top 50% of words that could come next in its response.</p> <p>Our experimental design was inspired by previous studies that evaluated viewpoint dependence using targets like toy photographers [2] and avatars with blocks [12]. In our study, we used an avatar as a target and different stimuli, either cubes with numbers and letters or cubes and spheres, to investigate the influence of visual and spatial judgments on model performance. Each task consisted of 16 trial types, featuring images at 8 different angles (0°, 45°, 90°, 135°, 180°, 225°, 270°, 315°) with 2 response options for each task (e.g., cube in front or behind, 6/9 or M/W on the cube, and cube left or right).</p> <p>Ten iterations of each image were passed through the model to calculate the percentage of correct responses.</p> <div class="table-responsive"> <table class="table table-striped table-bordered align-middle"> <thead class="table-light"> <tr> <th scope="col">Task</th> <th scope="col">Example Stimulus</th> <th scope="col">Prompt</th> </tr> </thead> <tbody> <tr> <td><strong>Level 1</strong></td> <td> <img src="/blog/assets/img/2024-11-20-failures_perspectivetaking/infront_behind_ex.jpg" alt="Level 1: &quot;IN FRONT&quot; 45°" class="img-fluid rounded shadow-sm" style="max-width: 300px;"/> </td> <td>For the following images respond with <em>in front</em> or <em>behind</em> to indicate if the cube is in front or behind from the perspective of the person.</td> </tr> <tr> <td><strong>Level 2: Spatial Judgment</strong></td> <td> <img src="/blog/assets/img/2024-11-20-failures_perspectivetaking/left_right_ex.jpg" alt="Level 2 Spatial: &quot;RIGHT&quot; 225°" class="img-fluid rounded shadow-sm" style="max-width: 300px;"/> </td> <td>For the following images respond with <em>left</em> or <em>right</em> to indicate if the cube is to the left or to the right from the perspective of the person.</td> </tr> <tr> <td><strong>Level 2: Visual Judgment</strong></td> <td> <img src="/blog/assets/img/2024-11-20-failures_perspectivetaking/number_ex.jpg" alt="Level 2 Visual: &quot;6&quot; 90°" class="img-fluid rounded shadow-sm" style="max-width: 300px;"/> </td> <td>For the following images respond with <em>6</em> or <em>9</em> to indicate if the number on the cube is a 6 or a 9 from the perspective of the person.</td> </tr> <tr> <td><strong>Level 2: Visual Judgment</strong></td> <td> <img src="/blog/assets/img/2024-11-20-failures_perspectivetaking/letter_ex.jpg" alt="Level 2 Visual: &quot;W&quot; 315°" class="img-fluid rounded shadow-sm" style="max-width: 300px;"/> </td> <td>For the following images respond with <em>M</em> or <em>W</em> to indicate if the letter on the cube is an M or a W from the perspective of the person.</td> </tr> </tbody> </table> </div> <h3 id="chain-of-thought-prompting">Chain of Thought Prompting</h3> <p>To further examine how language might be used to solve spatial tasks, we included chain-of-thought prompting to the Level 2 spatial task with the prompt:</p> <p>“Analyze this image step by step to determine if the cube is to the person’s left or right, from the person’s perspective. First, identify the direction the person is looking relative to the camera. Second, determine if the cube is to the left or right, relative to the camera. Third, if the person is facing the camera, then from their perspective, the cube is to the inverse of the camera’s left or right. If the person is facing away from the camera, then the cube is on the same side as seen from the camera. Respond with whether the cube is to the person’s left or right.”</p> <hr/> <h2 id="results">Results</h2> <h3 id="level-1">Level 1</h3> <p>GPT-4o performed with near-perfect accuracy on 6 out of the 8 image angles as seen below. Its poor performance on 0° images is likely due to an accidental viewpoint where the avatar blocked one of the shapes. However, poor performance on 315° image types is less interpretable, especially in contrast to GPT-4o’s impressive performance on 45° images, which have the same angular perspective.</p> <div class="l-page"> <iframe src="/blog/assets/plotly/2024-11-20-failures_perspectivetaking/infront_behind.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <h3 id="level-2-spatial-and-visual-judgments">Level 2 Spatial and Visual Judgments</h3> <p>As previously mentioned, human response times increase on perspective-taking tasks as the angular difference between the target and observer increases <d-cite key="surtees2013similarities"></d-cite>. We administered the task to a small number of human participants and replicated this effect with both our stimuli types, finding a bell-shaped curve in the relationship between response time and angle. Response times peaked when the target required a full mental rotation (180°), as seen in the green line in the figure below. As expected, GPT-4o struggled with the task when mental rotation was involved, beginning around a 90° angular difference. Interestingly, in both tasks, GPT-4o exhibited a response bias toward either “left” or “6” or “W” when the angular difference of the avatar is 90° or 135° in either direction. This likely reflects uncertainty from an egocentric perspective, and thus, a default to one response over another.</p> <div class="l-page"> <iframe src="/blog/assets/plotly/2024-11-20-failures_perspectivetaking/subplots.html" frameborder="0" scrolling="no" height="550px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <h3 id="chain-of-thought">Chain of Thought</h3> <p>GPT-4o performance significantly improved with chain-of-thought prompting on 180° stimuli. However, this linguistic strategy did not improve the model’s ability to handle intermediate rotations between 90° and 180°. This suggests that while language can convey some level of spatial information, it lacks the precision required for human-level spatial cognition. This demonstration of surface-level perspective-taking abilities can partially explain how multimodal models achieve high performance on certain spatial benchmarks.</p> <div class="l-page"> <iframe src="/blog/assets/plotly/2024-11-20-failures_perspectivetaking/cot.html" frameborder="0" scrolling="no" height="450px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <hr/> <h2 id="conclusion">Conclusion</h2> <p>With this project, we highlight the value of applying cognitive science techniques to explore AI capabilities in spatial cognition.</p> <ol> <li> <p>We investigated GPT-4o’s perspective-taking abilities, finding it fails when there is a large difference between image-based and avatar-based perspectives</p> </li> <li> <p>We developed a targeted set of three tasks to assess multimodal model performance on Level 1 and Level 2 perspective-taking, with spatial and visual judgments</p> <ul> <li> <p>GPT-4o can do Level 1, aligning with the spatial reasoning abilities of a human infant/toddler</p> </li> <li> <p>GPT-4o fails on Level 2 tasks when mental rotation is required—the avatar’s perspective is not aligned with image perspective</p> </li> </ul> </li> <li> <p>We investigated if chain-of-thought prompting could elicit more spatial reasoning through language</p> <ul> <li>This enabled GPT-4o to succeed on 180° tasks, but it continued to fail at intermediate angles, underscoring its limitations in performing true mental rotation</li> </ul> </li> </ol> <p>While GPT-4o’s performance decreases on tasks that humans typically solve using mental rotation, this does not necessarily indicate that GPT-4o struggles with or cannot perform mental rotation. Instead, it suggests that GPT-4o likely employs a fundamentally different strategy to approach these tasks. Rather than engaging in mental rotation, GPT-4o appears to rely primarily on image-based information processing. We found more support for this when testing an open prompt for Level 2 visual images that did not specify which letters or numbers to respond with. GPT-4o often responded with “E” and “0” for images around a 90° angular difference, where from the image view, an M/W would look like an E, and a 9/6 would look like a 0.</p> <p>It could be that current multimodal models aren’t trained on the appropriate data to achieve the reasoning necessary for Level 2 perspective-taking. However, considering the developmental trajectory of humans, it becomes evident that this issue may not be solely data-related. Level 2 perspective-taking typically develops between the ages of 6 and 10 <d-cite key="frick2014picturing"></d-cite><d-cite key="frick2018measuring"></d-cite>, even after children have had exposure to extensive amounts of “data” through experience. This late development suggests that the challenge may be more computational than data-driven. Specifically, this ability likely relies on computations occurring outside of the visual and language networks, perhaps in areas responsible for cognitive processes like mental rotation or spatial transformation or even theory of mind <d-cite key="gunia2021brain"></d-cite><d-cite key="schurz2013common"></d-cite><d-cite key="surtees2013use"></d-cite><d-cite key="surtees2013similarities"></d-cite>. While the argument that better or more focused training data could improve model performance remains valid, it is possible that entirely new computational strategies are needed to mirror the complex, integrative processes that enable Level 2 reasoning in humans.</p> <p>This project demonstrates the potential of cognitive science methods to establish baselines for AI assessment. Using these well-established techniques, we achieve clear, interpretable measures that are less susceptible to bias. Additionally, these measures can be directly compared to human performance and developmental trajectories, providing a robust framework for understanding AI’s strengths and weaknesses in relation to well-researched human cognitive processes.</p>]]></content><author><name>Bridget Leonard</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[An investigation into the spatial reasoning abilities of multimodal LLMs.]]></summary></entry><entry><title type="html">Submission Guidelines</title><link href="https://unireps.org//blog/2024/guidelines/" rel="alternate" type="text/html" title="Submission Guidelines"/><published>2024-10-09T00:00:00+00:00</published><updated>2024-10-09T00:00:00+00:00</updated><id>https://unireps.org//blog/2024/guidelines</id><content type="html" xml:base="https://unireps.org//blog/2024/guidelines/"><![CDATA[<p>Welcome to the <strong>UniReps Blogpost Submission</strong> guide! This page outlines the steps to submit your blogpost to the UniReps Workshop, where you can share your insights, spark discussions, and connect with the community. We’re excited you’re interested in contributing!</p> <p>To manage the blogpost track, we rely on <a href="https://github.com/alshedivat/al-folio">al-folio</a>, a <a href="https://jekyllrb.com">Jekyll</a> theme for academic websites. If this is all new to you, don’t worry. It’s simpler than it sounds and <strong>we are available for any questions you might have</strong>.</p> <h2 id="getting-started">Getting Started</h2> <h3 id="fork-the-repository">Fork the Repository</h3> <p>Start by <strong>forking the repository</strong>. You can do that <a href="https://github.com/UniReps/UniReps">here</a>. Forking creates your own copy of the project where you’ll be able to freely work on your post before submitting it via a pull request (PR).</p> <h3 id="clone-your-fork-locally">Clone Your Fork Locally</h3> <p>Next, clone your new repository to your local machine so you can start editing:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:UniReps/UniReps.git
<span class="nb">cd </span>UniReps
</code></pre></div></div> <h2 id="create-a-new-branch">Create a new branch</h2> <p>Create a new branch specific for each post submission. We recommend this format for the branch name:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git checkout <span class="nt">-b</span> post/&lt;your-post-title&gt;
</code></pre></div></div> <p>You could even use the GitHub web interface to directly write the blogpost, but we recommend cloning the repository for a smoother experience.</p> <h3 id="preview-the-website-locally">Preview the Website Locally</h3> <p>We’ve got a few options depending on your setup. Choose what works best for you:</p> <ul> <li> <p><strong>Option 1: Jekyll</strong><br/> If you have <a href="https://jekyllrb.com/">Jekyll</a> installed, run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle <span class="nb">install
</span>bundle <span class="nb">exec </span>jekyll serve
</code></pre></div> </div> </li> <li> <p><strong>Option 2: Docker</strong><br/> If Docker is your thing, simply run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-compose up
</code></pre></div> </div> </li> <li> <p><strong>Option 3: VS Code</strong><br/> If you are a <strong>VS Code</strong> user, we suggest using the <a href="https://code.visualstudio.com/docs/devcontainers/containers">Dev Container</a> feature, and it will take care of the environment setup for you.</p> </li> </ul> <p>All these options will start a local server, and you’ll be able to preview the website at <a href="http://127.0.0.1:8000/">http://127.0.0.1:8000/</a>.</p> <p>If you are unsure about which option to choose, we recommend starting with <strong>Option 3</strong> as it’s the easiest to set up (assuming you are already using VS Code). However, if you run into any issues, don’t hesitate to reach out to us for help!</p> <hr/> <h2 id="writing-your-post">Writing Your Post</h2> <p>Now to the exciting part—<strong>writing your blogpost</strong>!</p> <h3 id="topics-and-styles-">Topics and Styles 📝</h3> <p>We welcome blogposts of different nature:</p> <ul> <li>New or early-stage research results 🧑‍🔬</li> <li>Tutorial-style summaries of key methods and literature 🔍</li> <li>Opinion pieces on relevant topics in UniReps 🧠</li> <li>And more!</li> </ul> <p>Remember, the goal is to share your insights, spark discussions, and connect with the community. Keep it engaging and accessible to the <a href="https://unireps.org/2025/">broad workshop audience</a>!</p> <h3 id="create-a-new-post">Create a New Post</h3> <p>Head over to the <code class="language-plaintext highlighter-rouge">_posts/</code> directory and create a new file following this format:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>_posts/YYYY-MM-DD-title.md
</code></pre></div></div> <p>A great starting point is the template <code class="language-plaintext highlighter-rouge">/_posts/2024-10-09-template.md</code>, the original one from <a href="https://distill.pub">Distill</a>. Create a <strong>copy</strong> of it (it’s important, don’t directly edit it), rename it, and edit it for your post. It’s already structured for you!</p> <h3 id="add-media-optional">Add Media (Optional)</h3> <p>If your post includes media (we recommend to include them to exploit the blogpost format at its best), here’s how to add them:</p> <ul> <li><strong>Images</strong>: Place images in the folder <code class="language-plaintext highlighter-rouge">assets/img/YYYY-MM-DD-title/</code>.</li> <li><strong>Plotly Interactive Figures</strong>: Drop them into <code class="language-plaintext highlighter-rouge">assets/plotly/YYYY-MM-DD-title/</code>.</li> <li><strong>Citations</strong>: Save them as a BibTeX file (<code class="language-plaintext highlighter-rouge">.bib</code>) in <code class="language-plaintext highlighter-rouge">assets/bibliography/YYYY-MM-DD-title.bib</code>.</li> <li><strong>Other file types</strong>: follow the structure of the <code class="language-plaintext highlighter-rouge">assets/</code> folder, placing them in the appropriate subfolder.</li> </ul> <p>To display them in your post, use the following syntax:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>![Your Media](/blog/assets/complete/media/path/including.extension)
</code></pre></div></div> <p>For example, if you have an image <code class="language-plaintext highlighter-rouge">unireps_banner.jpeg</code> in the folder <code class="language-plaintext highlighter-rouge">assets/img/2024-10-09-guidelines/</code>, you would include it in your post like this:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>![My Image](/blog/assets/img/2024-10-09-guidelines/unireps_banner.jpeg)
</code></pre></div></div> <p>Resulting in: <img src="/blog/assets/img/2024-10-09-guidelines/unireps_banner.jpeg" alt="My Image"/></p> <p>For more examples on how to include media, check the <a href="https://unireps.org/blog/2024-10-09-template">Post Template</a> or the <a href="https://distill.pub/guide/">Distill Guide</a>.</p> <hr/> <h2 id="submitting-your-blogpost">Submitting Your Blogpost</h2> <p>Ready to submit? Follow these steps:</p> <h3 id="push-your-changes">Push Your Changes</h3> <p>Push the changes from your local machine to your forked repository:</p> <p>As an example:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git add <span class="nb">.</span>
git commit <span class="nt">-m</span> <span class="s2">"Blogpost submission: &lt;title&gt;"</span>
git push origin master
</code></pre></div></div> <h3 id="open-a-pull-request">Open a Pull Request</h3> <p>Now, head to your forked repository on GitHub and open a <strong>New Pull Request</strong>. Ensure the title reflects the topic of your blogpost, and double-check the description is clear and concise.</p> <hr/> <h2 id="review-and-publication">Review and Publication</h2> <p>Here’s what happens next:</p> <h3 id="automatic-checks">Automatic Checks</h3> <p>The repository is set up to run automatic checks on your submission so be sure to:</p> <ol> <li>Only create/edit new files, never edit existing ones from the original repository.</li> <li>Create one PR per blogpost submission (i.e., only one new .md file under the <code class="language-plaintext highlighter-rouge">_posts</code> directory).</li> <li>Place all the files in the appropriate directories with the submission pattern mentioned above.</li> </ol> <p>If everything looks good, the PR will be marked as “Ready for Review” and you will also get a preview link to see how your blogpost will look like on the website.</p> <h3 id="review-process">Review Process</h3> <p>Your blogpost will be reviewed based on the live content (ignoring the commit history or previous drafts). The review process will focus on the following aspects:</p> <ul> <li><strong>Content</strong>: Is the content relevant for the community, insightful, and engaging?</li> <li><strong>Structure</strong>: Is the blogpost well-structured and easy to follow?</li> <li><strong>Media</strong>: Are the images, plots, and other media elements well-integrated?</li> <li><strong>Style</strong>: Is the writing clear, concise, and correct?</li> </ul> <h3 id="post-acceptance">Post-Acceptance</h3> <p>Once your blogpost is accepted, it will be merged into the main repository and published on the website. You will be notified via email and your blogpost will be shared on the workshop’s social media channels.</p> <hr/> <h2 id="questions">Questions?</h2> <p>If you need any help or run into any issues, don’t hesitate to reach out to us:</p> <ul> <li>Open an issue on the <a href="https://github.com/UniReps/UniReps">GitHub repository</a></li> <li>Email: unireps-workshop [at] university [dot] org</li> </ul> <p>🔵 We’re here to help! 🔴</p>]]></content><author><name>Valentino Maiorca</name></author><category term="guide,"/><category term="tutorial"/><summary type="html"><![CDATA[How to submit a blogpost to the UniReps Workshop]]></summary></entry><entry><title type="html">Blog Post Template</title><link href="https://unireps.org//blog/2024/template/" rel="alternate" type="text/html" title="Blog Post Template"/><published>2024-10-09T00:00:00+00:00</published><updated>2024-10-09T00:00:00+00:00</updated><id>https://unireps.org//blog/2024/template</id><content type="html" xml:base="https://unireps.org//blog/2024/template/"><![CDATA[<h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <hr/> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags. An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>. For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p> <d-code block="" language="javascript"> var x = 25; function(x) { return x * x; } </d-code> <p><strong>❗️ Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look good in the dark mode. ❗️</p> <p>You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p> <figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
<span class="k">return</span> <span class="nx">x</span> <span class="err">\</span><span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="interactive-plots">Interactive Plots</h2> <p>You can add interative plots using plotly + iframes :framed_picture:</p> <div class="l-page"> <iframe src="/blog/assets/plotly/demo.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <p>The plot must be generated separately and saved into an HTML file. To generate the plot that you see above, you can use the following code snippet:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span>
<span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span>
<span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
<span class="n">df</span><span class="p">,</span>
<span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span>
<span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">assets/plotly/demo.html</span><span class="sh">'</span><span class="p">)</span></code></pre></figure> <hr/> <h2 id="details-boxes">Details boxes</h2> <p>Details boxes are collapsible boxes which hide additional information from the user. They can be added with the <code class="language-plaintext highlighter-rouge">details</code> liquid tag:</p> <details><summary>Click here to know more</summary> <p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p> </details> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p> <ul> <li> <p>Unordered list can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[An example of a Distill-style blog post and main elements]]></summary></entry><entry><title type="html">Learning Embedding Spaces with Metrics via Contrastive Learning |</title><link href="https://unireps.org//blog/2024/learning-embedding-spaces-with-metrics-via-contrastive-learning/" rel="alternate" type="text/html" title="Learning Embedding Spaces with Metrics via Contrastive Learning |"/><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://unireps.org//blog/2024/learning-embedding-spaces-with-metrics-via-contrastive-learning-</id><content type="html" xml:base="https://unireps.org//blog/2024/learning-embedding-spaces-with-metrics-via-contrastive-learning/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[GRaM Workshop]]></summary></entry></feed>